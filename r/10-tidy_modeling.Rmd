# `r if (knitr::is_latex_output()) {"Alternative Technical Approaches in R and Python"} else {"Alternative Technical Approaches in R, Python and Julia"}` {#alt-approaches}

As outlined earlier in this book, all technical implementations of the modeling techniques in previous chapters have relied wherever possible on base R code and specialist packages for specific methodologies---this allowed a focus on the basics of understanding, running and interpreting these models which is the key aim of this book.    For those interested in a wider range of technical options for running inferential statistical models, this chapter illustrates some alternative options and should be considered a starting point for those interested rather than an in-depth exposition.  

First we look at options for generating models in more predictable formats in R.  We have seen in prior chapters that the output of many models in R can be inconsistent.  In many cases we are given more information than we need, and in some cases we have less than we need.  Formats can vary, and we sometimes need to look in different parts of the output to see specific statistics that we seek.  `r if (knitr::is_latex_output()) '\\index{R!packages!tidymodels@\\texttt{tidymodels}}'`The `tidymodels` set of packages tries to bring the principles of tidy data into the realm of statistical modeling and we will illustrate this briefly.

Second, for those whose preference is to use Python, we provide some examples for how inferential regression models can be run in Python.  While Python is particularly well-tooled for running predictive models, it does not have the full range of statistical inference tools that are available in R.  In particular, using predictive modeling or machine learning packages like `r if (knitr::is_latex_output()) '\\index{Python!packages!scikit-learn@\\texttt{scikit-learn}}'``scikit-learn` to conduct regression modeling can often leave the analyst lacking when seeking information about certain model statistics when those statistics are not typically sought after in a predictive modeling workflow.  We briefly illustrate some Python packages which perform modeling with a greater emphasis on inference versus prediction.  

`r if (knitr::is_latex_output()) '<!--'`
Finally, we show some examples of regression models in Julia, which is a new but increasingly popular open source programming language.  Julia is considerably younger as a language than both R and Python, but its uptake is rapid.  Already many of the most common models can be easily implemented in Julia, but more advanced models are still awaiting the development of packages that implement them.  
`r if (knitr::is_latex_output()) '-->'`

## 'Tidier' modeling approaches in R

The `tidymodels` meta-package is a collection of packages which collectively apply the principles of tidy data to the construction of statistical models.  More information and learning resources on `tidymodels` can be found `r if (knitr::is_latex_output()) {"at \\texttt{https://www.tidymodels.org/}"} else {"[here](https://www.tidymodels.org/)"}`.  Within `tidymodels` there are two packages which are particularly useful in controlling the output of models in R:  the `broom` and `parsnip` packages.

### The `broom` package

`r if (knitr::is_latex_output()) '\\index{R!packages!broom@\\texttt{broom}}'`Consistent with how it is named, `broom` aims to tidy up the output of the models into a predictable format.  It works with over 100 different types of models in R.  `r if (knitr::is_latex_output()) '\\index{data sets!salespeople@\\texttt{salespeople}}'`In order to illustrate its use, let's run a model from a previous chapter---specifically our salesperson promotion model in Chapter \@ref(bin-log-reg).

```{r}
# obtain salespeople data
url <- "http://peopleanalytics-regression-book.org/data/salespeople.csv"
salespeople <- read.csv(url)
```

As in Chapter \@ref(bin-log-reg), we convert the `promoted` column to a factor and run a binomial logistic regression model on the `promoted` outcome.

```{r}
# convert promoted to factor
salespeople$promoted <- as.factor(salespeople$promoted)

# build model to predict promotion based on sales and customer_rate
promotion_model <- glm(formula = promoted ~ sales + customer_rate,
                       family = "binomial",
                       data = salespeople)

```

We now have our model sitting in memory.  We can use three key functions in the `broom` package to view a variety of model statistics.  First, the `tidy()` function allows us to see the coefficient statistics of the model.

```{r}
# load tidymodels metapackage
library(tidymodels)

# view coefficient statistics
broom::tidy(promotion_model)

```

The `glance()` function allows us to see a row of overall model statistics:

```{r}
# view model statistics
broom::glance(promotion_model)
```

The `augment()` function augments the observations in the data set with a range of observation-level model statistics such as residuals:

```{r, eval = FALSE}
# view augmented data
head(broom::augment(promotion_model))
```

`r if (knitr::is_latex_output()) '\\small'`
```{r, echo = FALSE}
head(broom::augment(promotion_model))
```
`r if (knitr::is_latex_output()) '\\normalsize'`

`r if (knitr::is_latex_output()) '\\index{data sets!soccer@\\texttt{soccer}}'`These functions are model-agnostic for a very wide range of common models in R.  For example, we can use them on our proportional odds model on soccer discipline from Chapter \@ref(ord-reg), and they will generate the relevant statistics in tidy tables.

```{r}
# get soccer data
url <- "http://peopleanalytics-regression-book.org/data/soccer.csv"
soccer <- read.csv(url)

# convert discipline to ordered factor
soccer$discipline <- ordered(soccer$discipline, 
                             levels = c("None", "Yellow", "Red"))

# run proportional odds model
library(MASS)
soccer_model <- polr(
  formula = discipline ~ n_yellow_25 + n_red_25 + position + 
    country + level + result, 
  data = soccer
)

# view model statistics
broom::glance(soccer_model)
```


`r if (knitr::is_latex_output()) '\\index{R!packages!dplyr@\\texttt{dplyr}}'``broom` functions integrate well into other tidyverse methods, and allow easy running of models over nested subsets of data.  For example, if we want to run our soccer discipline model across the different countries in the data set and see all the model statistics in a neat table, we can use typical tidyverse grammar to do so using `dplyr`.

`r if (knitr::is_latex_output()) '\\newpage'`

```{r, warning = FALSE, message = FALSE}
# load the tidyverse metapackage (includes dplyr)
library(tidyverse)

# define function to run soccer model and glance at results
soccer_model_glance <- function(form, df) {
  model <- polr(formula = form, data = df)
  broom::glance(model)
}

# run it nested by country
soccer %>% 
  dplyr::nest_by(country) %>% 
  dplyr::summarise(
    soccer_model_glance("discipline ~ n_yellow_25 + n_red_25", data)
  )
```

In a similar way, by putting model formulas in a dataframe column, numerous models can be run in a single command and results viewed in a tidy dataframe.

```{r, eval = FALSE}
# create model formula column
formula <- c(
  "discipline ~ n_yellow_25", 
  "discipline ~ n_yellow_25 + n_red_25",
  "discipline ~ n_yellow_25 + n_red_25 + position"
)

# create dataframe
models <- data.frame(formula)

# run models and glance at results
models %>% 
  dplyr::group_by(formula) %>% 
  dplyr::summarise(soccer_model_glance(formula, soccer))
```
`r if (knitr::is_latex_output()) '\\newpage \\scriptsize'`
```{r, echo = FALSE, warning = FALSE, message = FALSE}
# create model formula column
formula <- c(
  "discipline ~ n_yellow_25", 
  "discipline ~ n_yellow_25 + n_red_25",
  "discipline ~ n_yellow_25 + n_red_25 + position"
)

# create dataframe
models <- data.frame(formula)

# run models and glance at results
models %>% 
  dplyr::group_by(formula) %>% 
  dplyr::summarise(soccer_model_glance(formula, soccer))
```
`r if (knitr::is_latex_output()) '\\normalsize'`

### The `parsnip` package

`r if (knitr::is_latex_output()) '\\index{R!packages!parsnip@\\texttt{parsnip}}'`The `parsnip` package aims to create a unified interface to running models, to avoid users needing to understand different model terminology and other minutiae.   It also takes a more hierarchical approach to defining models that is similar in nature to the object-oriented approaches that Python users would be more familiar with.

Again let's use our salesperson promotion model example to illustrate.  We start by defining a model family that we wish to use, in this case logistic regression, and define a specific engine and mode.

```{r}
model <- parsnip::logistic_reg() %>% 
  parsnip::set_engine("glm") %>% 
  parsnip::set_mode("classification")
```

We can use the `translate()` function to see what kind of model we have created:

```{r, eval = FALSE}
model %>% 
  parsnip::translate()
```

`r if (knitr::is_latex_output()) '\\small'`
```{r, echo = FALSE}
model %>% 
  parsnip::translate()
```
`r if (knitr::is_latex_output()) '\\normalsize'`

Now with our model defined, we can fit it using a formula and data and then use `broom` to view the coefficients:

```{r}
model %>% 
  parsnip::fit(formula = promoted ~ sales + customer_rate,
               data = salespeople) %>% 
  broom::tidy()
```

`parsnip` functions are particularly motivated around tooling for machine learning model workflows in a similar way to `scikit-learn` in Python, but they can offer an attractive approach to coding inferential models, particularly where common families of models are used. 

## Inferential statistical modeling in Python

`r if (knitr::is_latex_output()) '\\index{Python!packages!scikit-learn@\\texttt{scikit-learn}}'`In general, the modeling functions contained in `scikit-learn`---which tends to be the go-to modeling package for most Python users---are oriented towards predictive modeling and can be challenging to navigate for those who are primarily interested in inferential modeling.  In this section we will briefly review approaches for running some of the models contained in this book in Python.  `r if (knitr::is_latex_output()) '\\index{Python!packages!statsmodels@\\texttt{statsmodels}}'`The `statsmodels` package is highly recommended as it offers a wide range of models which report similar statistics to those reviewed in this book.  Full `statsmodels` documentation can be found `r if (knitr::is_latex_output()) {"at \\texttt{https://www.statsmodels.org/stable/index.html}"} else {"[here](https://www.statsmodels.org/stable/index.html)"}`.

### Ordinary Least Squares (OLS) linear regression

The OLS linear regression model reviewed in Chapter \@ref(linear-reg-ols) can be generated using the `statsmodels` package, which can report a reasonably thorough set of model statistics.  By using the `statsmodels` formula API, model formulas similar to those used in R can be used.  

```{python, eval = FALSE}
import pandas as pd
import statsmodels.formula.api as smf

# get data
url = "http://peopleanalytics-regression-book.org/data/ugtests.csv"
ugtests = pd.read_csv(url)

# define model
model = smf.ols(formula = "Final ~ Yr3 + Yr2 + Yr1", data = ugtests)

# fit model
ugtests_model = model.fit()

# see results summary
print(ugtests_model.summary())
```

`r if (knitr::is_latex_output()) '\\small'`
```{python, echo = FALSE}
import pandas as pd
import statsmodels.formula.api as smf

# get data
url = "http://peopleanalytics-regression-book.org/data/ugtests.csv"
ugtests = pd.read_csv(url)

# define model
model = smf.ols(formula = "Final ~ Yr3 + Yr2 + Yr1", data = ugtests)

# fit model
ugtests_model = model.fit()

# see results summary
print(ugtests_model.summary())
```
`r if (knitr::is_latex_output()) '\\normalsize'`


### Binomial logistic regression

Binomial logistic regression models can be generated in a similar way to OLS linear regression models using the `statsmodels` formula API, calling the binomial family from the general `statsmodels` API.

```{python, eval = FALSE}
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf

# obtain salespeople data
url = "http://peopleanalytics-regression-book.org/data/salespeople.csv"
salespeople = pd.read_csv(url)

# define model
model = smf.glm(formula = "promoted ~ sales + customer_rate", 
                data = salespeople, 
                family = sm.families.Binomial())


# fit model
promotion_model = model.fit()


# see results summary
print(promotion_model.summary())
```

`r if (knitr::is_latex_output()) '\\small'`
```{python, echo = FALSE}
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf

# obtain salespeople data
url = "http://peopleanalytics-regression-book.org/data/salespeople.csv"
salespeople = pd.read_csv(url)

# define model
model = smf.glm(formula = "promoted ~ sales + customer_rate", 
                data = salespeople, family = sm.families.Binomial())

# fit model
promotion_model = model.fit()

# see results summary
print(promotion_model.summary())
```
`r if (knitr::is_latex_output()) '\\normalsize'`

### Multinomial logistic regression

Multinomial logistic regression is similarly available using the `statsmodels` formula API.  As usual, care must be taken to ensure that the reference category is appropriately defined, dummy input variables need to be explicitly constructed, and a constant term must be added to ensure an intercept is calculated.

`r if (knitr::is_latex_output()) '\\small'`
```{python, eval = FALSE}
import pandas as pd
import statsmodels.api as sm

# load health insurance data
url = "http://peopleanalytics-regression-book.org/data/health_insurance.csv"
health_insurance = pd.read_csv(url)

# convert product to categorical as an outcome variable
y = pd.Categorical(health_insurance['product'])

# create dummies for gender
X1 = pd.get_dummies(health_insurance['gender'], drop_first = True)

# replace back into input variables 
X2 = health_insurance.drop(['product', 'gender'], axis = 1)
X = pd.concat([X1, X2], axis = 1)

# add a constant term to ensure intercept is calculated
Xc = sm.add_constant(X)

# define model
model = sm.MNLogit(y, Xc.astype(float))

# fit model
insurance_model = model.fit()

# see results summary
print(insurance_model.summary())

```

`r if (knitr::is_latex_output()) '\\newpage \\small'`
```{python, echo = FALSE, results = FALSE}
import pandas as pd
import statsmodels.api as sm

# load health insurance data
url = "http://peopleanalytics-regression-book.org/data/health_insurance.csv"
health_insurance = pd.read_csv(url)

# convert product to categorical as an outcome variable
y = pd.Categorical(health_insurance['product'])

# create dummies for gender
X1 = pd.get_dummies(health_insurance['gender'], drop_first = True)

# replace back into input variables 
X2 = health_insurance.drop(['product', 'gender'], axis = 1)
X = pd.concat([X1, X2], axis = 1)

# add a constant term to ensure intercept is calculated
Xc = sm.add_constant(X)

# define model
model = sm.MNLogit(y, Xc.astype(float))

# fit model
insurance_model = model.fit()

# see results summary
print(insurance_model.summary())
```

```{python, echo = FALSE}
# see results summary
print(insurance_model.summary())
```
`r if (knitr::is_latex_output()) '\\normalsize'`

### Ordinal logistic regression

`statsmodels` also offers an ordinal model based on proportional odds logistic regression.  It is important to define the order of your outcome categories before running this model^[Note that intercept coefficients are presented a little differently compared to `MASS::polr()` in R.  The base level intercept is similar, but to get higher level intercepts, their coefficients need to be exponentiated and added to the base level intercept.].

`r if (knitr::is_latex_output()) '\\small'`
```{python, eval = FALSE}
import pandas as pd
from statsmodels.miscmodels.ordinal_model import OrderedModel
from pandas.api.types import CategoricalDtype

# load soccer data
url = "http://peopleanalytics-regression-book.org/data/soccer.csv"
soccer = pd.read_csv(url)

# fill NaN with "None" in discipline column
soccer.discipline = soccer.discipline.fillna("None")

# turn discipline column into ordered category
cat = CategoricalDtype(categories=["None", "Yellow", "Red"], ordered=True)
soccer.discipline = soccer.discipline.astype(cat)

# run proportional odds logistic regression
prop_odds_logit = OrderedModel.from_formula(
  "discipline ~ 1 + n_yellow_25 + n_red_25 + position + result + country + level", 
  soccer, 
  distr='logit'
)
soccer_model = prop_odds_logit.fit(method='bfgs')
print(soccer_model.summary())
```

`r if (knitr::is_latex_output()) '\\newpage \\small'`
```{python, echo = FALSE, results = FALSE}
import pandas as pd
from statsmodels.miscmodels.ordinal_model import OrderedModel
from pandas.api.types import CategoricalDtype

# load soccer data
url = "http://peopleanalytics-regression-book.org/data/soccer.csv"
soccer = pd.read_csv(url)

# fill NaN with "None" in discipline column
soccer.discipline = soccer.discipline.fillna("None")

# turn discipline column into ordered category
cat = CategoricalDtype(categories=["None", "Yellow", "Red"], ordered=True)
soccer.discipline = soccer.discipline.astype(cat)

# run proportional odds logistic regression
prop_odds_logit = OrderedModel.from_formula(
  "discipline ~ 1 + n_yellow_25 + n_red_25 + position + result + country + level", 
  soccer, 
  distr='logit'
)
soccer_model = prop_odds_logit.fit(method='bfgs')
```

```{python, echo = FALSE}
# see results summary
print(soccer_model.summary())
```
`r if (knitr::is_latex_output()) '\\normalsize'`

### Mixed effects models

`r if (knitr::is_latex_output()) '\\index{Python!packages!pymer4@\\texttt{pymer4}}'`The `pymer4` package provides a simple implementation of linear and generalized linear mixed effects models.  The `Lmer()` function is used for all models, and the specific model is defined by the `family` parameter.

`r if (knitr::is_latex_output()) '\\small'`
```{python, eval = FALSE}
from pymer4 import Lmer
import pandas as pd

url = "http://peopleanalytics-regression-book.org/data/speed_dating.csv"
speed_dating = pd.read_csv(url)

model = Lmer("dec ~ agediff + samerace + attr + intel + prob + (1 | iid)",
             data = speed_dating, family = 'binomial')

print(model.fit())
```


`r if (knitr::is_latex_output()) '\\newpage \\small'`
```{python, echo = FALSE}
from pymer4 import Lmer
import pandas as pd

url = "http://peopleanalytics-regression-book.org/data/speed_dating.csv"
speed_dating = pd.read_csv(url)

model = Lmer("dec ~ agediff + samerace + attr + intel + prob + (1 | iid)",
             data = speed_dating, family = 'binomial')

print(model.fit())
```

`r if (knitr::is_latex_output()) '\\normalsize'`


### Structural equation models

`r if (knitr::is_latex_output()) '\\index{Python!packages!semopy@\\texttt{semopy}}'`The `semopy` package is a specialized package for the implementation of Structural Equation Models in Python, and its implementation is very similar to the `lavaan` package in R.  However, its reporting is not as intuitive compared to `lavaan`.  A full tutorial is available `r if (knitr::is_latex_output()) {"at \\texttt{https://semopy.com/tutorial.html}"} else {"[here](https://semopy.com/tutorial.html)"}`.  Here is an example of how to run the same model as that studied in Section \@ref(struc-eq-model) using `semopy`.

`r if (knitr::is_latex_output()) '\\newpage \\small'`
```{python, results = "hide"}
import pandas as pd
from semopy import Model


# get data
url = "http://peopleanalytics-regression-book.org/data/politics_survey.csv"
politics_survey = pd.read_csv(url)


# define full measurement and structural model
measurement_model = """
# measurement model
Pol =~ Pol1 + Pol2
Hab =~ Hab1 + Hab2 + Hab3
Loc =~ Loc2 + Loc3
Env =~ Env1 + Env2
Int =~ Int1 + Int2
Pers =~ Pers2 + Pers3
Nat =~ Nat1 + Nat2
Eco =~ Eco1 + Eco2

# structural model
Overall ~ Pol + Hab + Loc + Env + Int + Pers + Nat + Eco
"""

full_model = Model(measurement_model)


# fit model to data and inspect
full_model.fit(politics_survey)
```
`r if (knitr::is_latex_output()) '\\normalsize'`

Then to inspect the results:

```{python}
# inspect the results of SEM (first few rows)
full_model.inspect().head()
```

### Survival analysis

`r if (knitr::is_latex_output()) '\\index{Python!packages!semopy@\\texttt{lifelines}}'`The `lifelines` package in Python is designed to support survival analysis, with functions to calculate survival estimates, plot survival curves, perform Cox proportional hazard regression and check proportional hazard assumptions.  A full tutorial is available `r if (knitr::is_latex_output()) {"at \\texttt{https://lifelines.readthedocs.io/en/latest/index.html}"} else {"[here](https://lifelines.readthedocs.io/en/latest/index.html)"}`.  

Here is an example of how to plot Kaplan-Meier survival curves in Python using our Chapter \@ref(survival) walkthrough example. The survival curves are displayed in Figure \@ref(fig:survival-python).

`r if (knitr::is_latex_output()) '\\small'`
```{python, eval = FALSE}
import pandas as pd
from lifelines import KaplanMeierFitter
from matplotlib import pyplot as plt

# get data
url = "http://peopleanalytics-regression-book.org/data/job_retention.csv"
job_retention = pd.read_csv(url)

# fit our data to Kaplan-Meier estimates
T = job_retention["month"]
E = job_retention["left"]
kmf = KaplanMeierFitter()
kmf.fit(T, event_observed = E)

# split into high and not high sentiment
highsent = (job_retention["sentiment"] >= 7)


# set up plot
survplot = plt.subplot()

# plot high sentiment survival function
kmf.fit(T[highsent], event_observed = E[highsent], 
label = "High Sentiment")

kmf.plot_survival_function(ax = survplot)

# plot not high sentiment survival function
kmf.fit(T[~highsent], event_observed = E[~highsent], 
label = "Not High Sentiment")

kmf.plot_survival_function(ax = survplot)

# show survival curves by sentiment category
plt.show()
```

`r if (knitr::is_latex_output()) '\\normalsize'`

```{python, echo = FALSE, fig.show='hide'}
import pandas as pd
import matplotlib
from lifelines import KaplanMeierFitter
from matplotlib import pyplot as plt
matplotlib.rcParams['pdf.fonttype'] = 42
matplotlib.rcParams['ps.fonttype'] = 42
# matplotlib.rcParams['text.usetex'] = True
# matplotlib.rcParams.update(matplotlib.rcParamsDefault)

plt.cla()

# get data
url = "http://peopleanalytics-regression-book.org/data/job_retention.csv"
job_retention = pd.read_csv(url)

# fit our data to Kaplan-Meier estimates
T = job_retention["month"]
E = job_retention["left"]
kmf = KaplanMeierFitter()
kmf.fit(T, event_observed = E)

# split into high and not high sentiment
highsent = (job_retention["sentiment"] >= 7)

# set up plot
survplot = plt.subplot()

# plot high sentiment survival function
kmf.fit(T[highsent], event_observed = E[highsent], 
label = "High Sentiment")

kmf.plot_survival_function(ax = survplot)

# plot not high sentiment survival function
kmf.fit(T[~highsent], event_observed = E[~highsent], 
label = "Not High Sentiment")

kmf.plot_survival_function(ax = survplot)

# show survival curves by sentiment category
plt.savefig("www/10/survival-python-1.png")
```

```{r survival-python, fig.align = "center", fig.cap = "Survival curves by sentiment category in the job retention data", out.width = "90%", echo = FALSE}
knitr::include_graphics("www/10/survival-python-1.png")
```

And here is an example of how to fit a Cox Proportional Hazard model similarly to Section \@ref(coxphmodel)^[I am not aware of any way of running frailty models currently in Python.].

```{python, results = "hide"}
from lifelines import CoxPHFitter

# fit Cox PH model to job_retention data
cph = CoxPHFitter()
cph.fit(job_retention, duration_col = 'month', event_col = 'left', 
        formula = "gender + field + level + sentiment")
```

```{python, eval = FALSE}
# view results
cph.print_summary()
```

`r if (knitr::is_latex_output()) '\\newpage \\tiny'`
```{python, echo = FALSE}
# view results
cph.print_summary()
```
`r if (knitr::is_latex_output()) '\\normalsize'`

Proportional Hazard assumptions can be checked using the `check_assumptions()` method^[Schoenfeld residual plots can be seen by setting `show_plots = True` in the parameters.].

```{python, eval = FALSE}
cph.check_assumptions(job_retention, p_value_threshold = 0.05)
```

`r if (knitr::is_latex_output()) '\\newpage \\scriptsize'`
```{python, echo = FALSE}
cph.check_assumptions(job_retention, p_value_threshold = 0.05)
```
`r if (knitr::is_latex_output()) '\\normalsize'`


`r if (knitr::is_latex_output()) '<!--'`
## Inferential statistical modeling in Julia

The `GLM` package in Julia offers functions for a variety of elementary regression models.  This package contains an implementation of linear regression as well as binomial logistic regression.  

### Ordinary Least Squares (OLS) linear regression

Here is how to run our OLS linear model on the `ugtests` data set from Chapter \@ref(linear-reg-ols):

```{julia, message = FALSE, warning = FALSE}
using DataFrames, CSV, GLM;

# get data
url = "http://peopleanalytics-regression-book.org/data/ugtests.csv";
ugtests = CSV.read(download(url), DataFrame);

# run linear model
ols_model = lm(@formula(Final ~ Yr1 + Yr2 + Yr3), ugtests)
```

Summary statistics such as the $R^2$ or adjusted $R^2$ can be obtained using appropriate functions:

```{julia, message = FALSE, warning = FALSE}
# r-squared
r2(ols_model)
# adjusted r-squared
adjr2(ols_model)

```

### Binomial logistic regression

Here is how to run our binomial logistic regression on the `salespeople` data set from Chapter \@ref(bin-log-reg).  Note that categorical inputs will need to be explicitly converted before running the model.  

```{julia, message = FALSE, warning = FALSE}
using DataFrames, CSV, GLM, Missings, CategoricalArrays;

# get salespeople data
url = "http://peopleanalytics-regression-book.org/data/salespeople.csv";
salespeople = CSV.read(download(url), DataFrame, missingstring=["NA"]);

# remove missing value rows from dataset
salespeople = salespeople[completecases(salespeople), :];

# ensure no missing data structures
salespeople = mapcols(col -> disallowmissing(col), salespeople);

# map categorical input columns
salespeople.performance = CategoricalArray(salespeople.performance);

# run binomial logistic regression model
promotion_model = glm(@formula(promoted ~ sales + customer_rate + performance), salespeople, Binomial(), LogitLink())



```

The AIC, log likelihood and other summary statistics of the model can be obtained using the appropriate functions.  For example: 

```{julia, message = FALSE, warning = FALSE}
using StatsBase

# AIC
aic(promotion_model)
# log likelihood
loglikelihood(promotion_model)
```

### Multinomial logistic regression

The `Econometrics` package provides a convenient function for a multinomial logistic regression model, where the levels of the outcome variable can be set as an argument.

```{julia, message = FALSE, warning = FALSE}
using DataFrames, CSV, Econometrics, CategoricalArrays;

# get health_insurance data
url = "http://peopleanalytics-regression-book.org/data/health_insurance.csv";
health_insurance = CSV.read(download(url), DataFrame, missingstring=["NA"]);

# map gender and product to categorical
health_insurance.gender = CategoricalArray(health_insurance.gender);
health_insurance.product = CategoricalArray(health_insurance.product);

# fit model defining reference level
model = fit(EconometricModel, 
            @formula(product ~ age + household + position_level + gender + absent),
            health_insurance,
            contrasts = Dict(:product => DummyCoding(base = "A")))
```

### Proportional odds logistic regression

The `Econometrics` package also provides a convenient function for proportional odds logistic regression.

```{julia, message = FALSE, warning = FALSE}
using DataFrames, CSV, Econometrics, CategoricalArrays;

# get soccer data
url = "http://peopleanalytics-regression-book.org/data/soccer.csv";
soccer = CSV.read(download(url), DataFrame, missingstring=["NA"]);

# map columns not starting with "n" to categorical
transform!(soccer, names(soccer, r"^(?!n)") .=> CategoricalArray, renamecols = false);

# map discipline column to ordinal
soccer.discipline = levels!(categorical(soccer.discipline, ordered = true), ["None", "Yellow", "Red"]);

# run proportional odds model
soccer_model = fit(EconometricModel,
                   @formula(discipline ~ n_yellow_25 + n_red_25 + position + result + country + level),
                   soccer)

```

Note that the $R^2$ results presented correspond to the McFadden variant of the pseudo-$R^2$, although other variants can be obtained by specifying them in the appropriate function.

```{julia, message = FALSE, warning = FALSE}
r2(soccer_model, :Nagelkerke)
```

### Mixed models

The `MixedModels` package allows for the construction of both linear mixed models and mixed GLMs.  Here is an approach to our mixed model on speed dating from Section \@ref(mixed):

```{julia, message = FALSE, warning = FALSE}
using DataFrames, CSV, MixedModels, CategoricalArrays;

# get speed_dating data
url = "http://peopleanalytics-regression-book.org/data/speed_dating.csv";
speed_dating = CSV.read(download(url), DataFrame, missingstring=["NA"]);

# make iid categorical
speed_dating.iid = CategoricalArray(speed_dating.iid);

# run mixed GLM
dating_model = fit(MixedModel, @formula(dec ~ agediff + samerace + attr + intel + prob + (1|iid)),
                   speed_dating, Binomial(), LogitLink())
```

`r if (knitr::is_latex_output()) '-->'`


