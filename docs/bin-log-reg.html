<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>5 Binomial Logistic Regression for Binary Outcomes | Handbook of Regression Modeling in People Analytics: With Examples in R, Python and Julia</title>
<meta name="author" content="Keith McNulty">
<meta name="description" content="In the previous chapter we looked at how to explain outcomes that have continuous scale, such as quantity, money, height or weight. While there are a number of typical outcomes of this type in the...">
<meta name="generator" content="bookdown 0.22.15 with bs4_book()">
<meta property="og:title" content="5 Binomial Logistic Regression for Binary Outcomes | Handbook of Regression Modeling in People Analytics: With Examples in R, Python and Julia">
<meta property="og:type" content="book">
<meta property="og:url" content="https://peopleanalytics-regression-book.org/bin-log-reg.html">
<meta property="og:image" content="https://peopleanalytics-regression-book.org/www/cover/coverpage-og.png">
<meta property="og:description" content="In the previous chapter we looked at how to explain outcomes that have continuous scale, such as quantity, money, height or weight. While there are a number of typical outcomes of this type in the...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="5 Binomial Logistic Regression for Binary Outcomes | Handbook of Regression Modeling in People Analytics: With Examples in R, Python and Julia">
<meta name="twitter:site" content="@dr_keithmcnulty">
<meta name="twitter:description" content="In the previous chapter we looked at how to explain outcomes that have continuous scale, such as quantity, money, height or weight. While there are a number of typical outcomes of this type in the...">
<meta name="twitter:image" content="https://peopleanalytics-regression-book.org/www/cover/coverpage-og.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.9.5/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.5.1/tabs.js"></script><script src="libs/bs3compat-0.2.5.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><meta name="citation_title" content="Handbook of Regression Modeling in People Analytics: With Examples in R and Python">
<meta name="citation_author" content="Keith McNulty">
<meta name="citation_publication_date" content="2021">
<meta name="citation_isbn" content="9781003194156">
<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script><script src="libs/plotly-binding-4.9.2.1/plotly.js"></script><script src="libs/typedarray-0.1/typedarray.min.js"></script><link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet">
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script><link href="libs/plotly-htmlwidgets-css-1.52.2/plotly-htmlwidgets.css" rel="stylesheet">
<script src="libs/plotly-main-1.52.2/plotly-latest.min.js"></script><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-N7JZGMVRZK"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-N7JZGMVRZK');
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="css/style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="With Examples in R, Python and Julia">Handbook of Regression Modeling in People Analytics</a>:
        <small class="text-muted">With Examples in R, Python and Julia</small>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="foreword-by-alexis-fink.html">Foreword by Alexis Fink</a></li>
<li><a class="" href="introduction.html">Introduction</a></li>
<li><a class="" href="inf-model.html"><span class="header-section-number">1</span> The Importance of Regression in People Analytics</a></li>
<li><a class="" href="the-basics-of-the-r-programming-language.html"><span class="header-section-number">2</span> The Basics of the R Programming Language</a></li>
<li><a class="" href="found-stats.html"><span class="header-section-number">3</span> Statistics Foundations</a></li>
<li><a class="" href="linear-reg-ols.html"><span class="header-section-number">4</span> Linear Regression for Continuous Outcomes</a></li>
<li><a class="active" href="bin-log-reg.html"><span class="header-section-number">5</span> Binomial Logistic Regression for Binary Outcomes</a></li>
<li><a class="" href="multinomial-logistic-regression-for-nominal-category-outcomes.html"><span class="header-section-number">6</span> Multinomial Logistic Regression for Nominal Category Outcomes</a></li>
<li><a class="" href="ord-reg.html"><span class="header-section-number">7</span> Proportional Odds Logistic Regression for Ordered Category Outcomes</a></li>
<li><a class="" href="modeling-explicit-and-latent-hierarchy-in-data.html"><span class="header-section-number">8</span> Modeling Explicit and Latent Hierarchy in Data</a></li>
<li><a class="" href="survival.html"><span class="header-section-number">9</span> Survival Analysis for Modeling Singular Events Over Time</a></li>
<li><a class="" href="alt-approaches.html"><span class="header-section-number">10</span> Alternative Technical Approaches in R, Python and Julia</a></li>
<li><a class="" href="power-tests.html"><span class="header-section-number">11</span> Power Analysis to Estimate Required Sample Sizes for Modeling</a></li>
<li><a class="" href="solutions-to-exercises-slide-presentations-videos-and-other-learning-resources.html">Solutions to Exercises, Slide Presentations, Videos and Other Learning Resources</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/keithmcnulty/peopleanalytics-regression-book">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="bin-log-reg" class="section level1" number="5">
<h1>
<span class="header-section-number">5</span> Binomial Logistic Regression for Binary Outcomes<a class="anchor" aria-label="anchor" href="#bin-log-reg"><i class="fas fa-link"></i></a>
</h1>
<p>In the previous chapter we looked at how to explain outcomes that have continuous scale, such as quantity, money, height or weight. While there are a number of typical outcomes of this type in the people analytics domain, they are not the most common form of outcomes that are typically modeled. Much more common are situations where the outcome of interest takes the form of a limited set of classes. Binary (two class) problems are very common. Hiring, promotion and attrition are often modeled as binary outcomes: for example ‘Promoted’ or ‘Not promoted.’ Multi-class outcomes like performance ratings on an ordinal scale, or survey responses on a Likert scale are often converted to binary outcomes by dividing the ratings into two groups, for example ‘High’ and ‘Not High.’</p>
<p>In any situation where our outcome is binary, we are effectively working with likelihoods. These are not generally linear in nature, and so we no longer have the comfort of our inputs being <em>directly</em> linearly related to our outcome. Therefore direct linear regression methods such as Ordinary Least Squares regression are not well suited to outcomes of this type. Instead, linear relationships can be inferred on <em>transformations</em> of the outcome variable, which gives us a path to building interpretable models. Hence, binomial logistic regression is said to be in a class of <em>generalized linear models</em> or <em>GLMs</em>. Understanding logistic regression and using it reliably in practice is not straightforward, but it is an invaluable skill to have in the people analytics domain. The mathematics of this chapter is a little more involved but worth the time investment in order to build a competent understanding of how to interpret these types of models.</p>
<div id="when-to-use-it" class="section level2" number="5.1">
<h2>
<span class="header-section-number">5.1</span> When to use it<a class="anchor" aria-label="anchor" href="#when-to-use-it"><i class="fas fa-link"></i></a>
</h2>
<div id="logistic-origins" class="section level3" number="5.1.1">
<h3>
<span class="header-section-number">5.1.1</span> Origins and intuition of binomial logistic regression<a class="anchor" aria-label="anchor" href="#logistic-origins"><i class="fas fa-link"></i></a>
</h3>
<p>The <em>logistic function</em> was first introduced by the Belgian mathematician Pierre François Verhulst in the mid-1800s as a tool for modeling population growth for humans, animals and certain species of plants and fruits. By this time, it was generally accepted that population growth could not continue exponentially forever, and that there were environmental and resource limits which place a maximum limit on the size of a population. The formula for Verhulst’s function was:</p>
<p><span class="math display">\[
y = \frac{L}{1 + e^{-k(x - x_0)}}
\]</span>
where <span class="math inline">\(e\)</span> is the exponential constant, <span class="math inline">\(x_0\)</span> is the value of <span class="math inline">\(x\)</span> at the midpoint, <span class="math inline">\(L\)</span> is the maximum value of <span class="math inline">\(y\)</span> (known as the ‘carrying capacity’) and <span class="math inline">\(k\)</span> is the maximum gradient of the curve.</p>
<p>The logistic function, as shown in Figure <a href="bin-log-reg.html#fig:logistic-function-verhulst">5.1</a>, was felt to accurately capture the theorized stages of population growth, with slower growth in the initial stage, moving to exponential growth during the intermediate stage and then to slower growth as the population approaches its carrying capacity.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:logistic-function-verhulst"></span>
<img src="www/05/logistic-curve.png" alt="Verhulst's logistic function modeled both the exponential nature and the natural limit of population growth" width="400"><p class="caption">
Figure 5.1: Verhulst’s logistic function modeled both the exponential nature and the natural limit of population growth
</p>
</div>
<p>In the early 20th century, starting with applications in economics and in chemistry, the logistic function was adopted in a wide array of fields as a useful tool for modeling phenomena. In statistics, it was observed that the logistic function has a similar S-shape (or <em>sigmoid</em>) to a cumulative normal distribution of probability, as depicted in Figure <a href="bin-log-reg.html#fig:norm-log-curves">5.2</a><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;The logistic function plotted in Figure &lt;a href="bin-log-reg.html#fig:norm-log-curves"&gt;5.2&lt;/a&gt; takes the simple form &lt;span class="math inline"&gt;\(y = \frac{1}{1 + e^{-x}}\)&lt;/span&gt;.&lt;/p&gt;'><sup>24</sup></a>, where the <span class="math inline">\(x\)</span> scale represents standard deviations around a mean. As we will learn, the logistic function gives rise to a mathematical model where the coefficients are easily interpreted in terms of likelihood of the outcome. Unsurprisingly, therefore, the logistic model soon became a common approach to modeling probabilistic phenomena.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:norm-log-curves"></span>
<img src="_main_files/figure-html/norm-log-curves-1.png" alt="The logistic function (blue dashed line) is very similar to a cumulative normal distribution (red solid line) but easier to interpret" width="672"><p class="caption">
Figure 5.2: The logistic function (blue dashed line) is very similar to a cumulative normal distribution (red solid line) but easier to interpret
</p>
</div>
</div>
<div id="use-cases-for-binomial-logistic-regression" class="section level3" number="5.1.2">
<h3>
<span class="header-section-number">5.1.2</span> Use cases for binomial logistic regression<a class="anchor" aria-label="anchor" href="#use-cases-for-binomial-logistic-regression"><i class="fas fa-link"></i></a>
</h3>
<p>Binomial logistic regression can be used when the outcome of interest is binary or dichotomous in nature. That is, it takes one of two values. For example, one or zero, true or false, yes or no. These classes are commonly described as ‘positive’ and ‘negative’ classes. There is an underlying assumption that the cumulative probability of the outcome takes a shape similar to a cumulative normal distribution.</p>
<p>Here are some example questions that could be approached using binomial logistic regression:</p>
<ul>
<li>Given a set of data about sales managers in an organization, including performance against targets, team size, tenure in the organization and other factors, what influence do these factors have on the likelihood of the individual receiving a high performance rating?</li>
<li>Given a set of demographic, income and location data, what influence does each have on the likelihood of an individual voting in an election?</li>
<li>Given a set of statistics about the in-game activity of soccer players, what relationship does each statistic have with the likelihood of a player scoring a goal?</li>
</ul>
</div>
<div id="walkthrough-logit" class="section level3" number="5.1.3">
<h3>
<span class="header-section-number">5.1.3</span> Walkthrough example<a class="anchor" aria-label="anchor" href="#walkthrough-logit"><i class="fas fa-link"></i></a>
</h3>
<p>You are an analyst for a large company consisting of regional sales teams across the country. Twice every year, this company promotes some of its salespeople. Promotion is at the discretion of the head of each regional sales team, taking into consideration financial performance, customer satisfaction ratings, recent performance ratings and personal judgment.</p>
<p>You are asked by the management of the company to conduct an analysis to determine how the factors of financial performance, customer ratings and performance ratings influence the likelihood of a given salesperson being promoted. You are provided with a data set containing data for the last three years of salespeople considered for promotion. The <code>salespeople</code> data set contains the following fields:</p>
<ul>
<li>
<code>promoted</code>: A binary value indicating 1 if the individual was promoted and 0 if not</li>
<li>
<code>sales</code>: the sales (in thousands of dollars) attributed to the individual in the period of the promotion</li>
<li>
<code>customer_rate</code>: the average satisfaction rating from a survey of the individual’s customers during the promotion period</li>
<li>
<code>performance</code>: the most recent performance rating prior to promotion, from 1 (lowest) to 4 (highest)</li>
</ul>
<p>Let’s take a quick look at the data.</p>
<div class="sourceCode" id="cb292"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># if needed, download salespeople data</span>
<span class="va">url</span> <span class="op">&lt;-</span> <span class="st">"http://peopleanalytics-regression-book.org/data/salespeople.csv"</span>
<span class="va">salespeople</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.csv</a></span><span class="op">(</span><span class="va">url</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb293"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># look at the first few rows of data</span>
<span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">salespeople</span><span class="op">)</span></code></pre></div>
<pre><code>##   promoted sales customer_rate performance
## 1        0   594          3.94           2
## 2        0   446          4.06           3
## 3        1   674          3.83           4
## 4        0   525          3.62           2
## 5        1   657          4.40           3
## 6        1   918          4.54           2</code></pre>
<p>The data looks as expected. Let’s get a summary of the data.</p>
<div class="sourceCode" id="cb295"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">salespeople</span><span class="op">)</span></code></pre></div>
<pre><code>##     promoted          sales       customer_rate    performance 
##  Min.   :0.0000   Min.   :151.0   Min.   :1.000   Min.   :1.0  
##  1st Qu.:0.0000   1st Qu.:389.2   1st Qu.:3.000   1st Qu.:2.0  
##  Median :0.0000   Median :475.0   Median :3.620   Median :3.0  
##  Mean   :0.3219   Mean   :527.0   Mean   :3.608   Mean   :2.5  
##  3rd Qu.:1.0000   3rd Qu.:667.2   3rd Qu.:4.290   3rd Qu.:3.0  
##  Max.   :1.0000   Max.   :945.0   Max.   :5.000   Max.   :4.0  
##                   NA's   :1       NA's   :1       NA's   :1</code></pre>
<p>First we see a small number of missing values, and we should remove those observations. We see that about a third of individuals were promoted, that sales ranged from $151k to $945k, that as expected the average satisfaction ratings range from 1 to 5, and finally we see four performance ratings, although the performance categories are numeric when they should be an ordered factor, and <code>promoted</code> is numeric when it should be categorical. Let’s convert these, and then let’s do a pairplot to get a quick view on some possible underlying relationships, as in Figure <a href="bin-log-reg.html#fig:log-pairplot">5.3</a>.</p>
<div class="sourceCode" id="cb297"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggobi.github.io/ggally">GGally</a></span><span class="op">)</span>

<span class="co"># remove NAs</span>
<span class="va">salespeople</span> <span class="op">&lt;-</span> <span class="va">salespeople</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/stats/complete.cases.html">complete.cases</a></span><span class="op">(</span><span class="va">salespeople</span><span class="op">)</span>, <span class="op">]</span>

<span class="co"># convert performance to ordered factor and promoted to categorical</span>
<span class="va">salespeople</span><span class="op">$</span><span class="va">performance</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">ordered</a></span><span class="op">(</span><span class="va">salespeople</span><span class="op">$</span><span class="va">performance</span>, 
                                   levels <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">)</span>
<span class="va">salespeople</span><span class="op">$</span><span class="va">promoted</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="va">salespeople</span><span class="op">$</span><span class="va">promoted</span><span class="op">)</span>

<span class="co"># generate pairplot</span>
<span class="fu">GGally</span><span class="fu">::</span><span class="fu"><a href="https://ggobi.github.io/ggally/reference/ggpairs.html">ggpairs</a></span><span class="op">(</span><span class="va">salespeople</span><span class="op">)</span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:log-pairplot"></span>
<img src="_main_files/figure-html/log-pairplot-1.png" alt="Pairplot for the `salespeople` data set" width="672"><p class="caption">
Figure 5.3: Pairplot for the <code>salespeople</code> data set
</p>
</div>
<p>We can see from this pairplot that there are clearly higher sales for those who are promoted versus those who are not. We also see a moderate relationship between customer rating and sales, which is intuitive (if the customer doesn’t think much of you, sales wouldn’t likely be very high).</p>
<p>So we can see that some relationships with our outcome may exist here, but it’s not clear how to tease them out and quantify them relative to each other. Let’s explore how binomial logistic regression can help us do this.</p>
</div>
</div>
<div id="mod-prob" class="section level2" number="5.2">
<h2>
<span class="header-section-number">5.2</span> Modeling probabilistic outcomes using a logistic function<a class="anchor" aria-label="anchor" href="#mod-prob"><i class="fas fa-link"></i></a>
</h2>
<p>Imagine that you have an outcome event <span class="math inline">\(y\)</span> which either occurs or does not occur. The probability of <span class="math inline">\(y\)</span> occurring, or <span class="math inline">\(P(y = 1)\)</span>, obviously takes a value between 0 and 1. Now imagine that some input variable <span class="math inline">\(x\)</span> has a positive effect on the probability of the event occurring. Then you would naturally expect <span class="math inline">\(P(y = 1)\)</span> to increase as <span class="math inline">\(x\)</span> increases.</p>
<p>In our <code>salespeople</code> data set, let’s plot our <code>promotion</code> outcome against the <code>sales</code> input. This can be seen in Figure <a href="bin-log-reg.html#fig:prom-sales-plot">5.4</a>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:prom-sales-plot"></span>
<img src="_main_files/figure-html/prom-sales-plot-1.png" alt="Plot of promotion against sales in the `salespeople` data set" width="672"><p class="caption">
Figure 5.4: Plot of promotion against sales in the <code>salespeople</code> data set
</p>
</div>
<p>It’s clear that promotion is more likely with higher sales levels. As we move along the <span class="math inline">\(x\)</span> axis from left to right and gradually include more and more individuals with higher sales, we know that the probability of promotion is gradually increasing overall. We could try to model this probability using our logistic function, which we learned about in Section <a href="bin-log-reg.html#logistic-origins">5.1.1</a>. For example, let’s plot the logistic function
<span class="math display">\[
P(y = 1) = \frac{1}{1 + e^{-k(x - x_{0})}}
\]</span></p>
<p>on this data, where we set <span class="math inline">\(x_0\)</span> to the mean of <code>sales</code> and <span class="math inline">\(k\)</span> to be some maximum gradient value. In Figure <a href="bin-log-reg.html#fig:prom-with-logistic">5.5</a> we can see these logistic functions for different values of <span class="math inline">\(k\)</span>. All of these seem to reflect the pattern we are observing to some extent, but how do we determine the best-fitting logistic function?</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:prom-with-logistic"></span>
<img src="_main_files/figure-html/prom-with-logistic-1.png" alt="Overlaying logistic functions with various gradients onto previous plot" width="672"><p class="caption">
Figure 5.5: Overlaying logistic functions with various gradients onto previous plot
</p>
</div>
<div id="deriving-the-concept-of-log-odds" class="section level3" number="5.2.1">
<h3>
<span class="header-section-number">5.2.1</span> Deriving the concept of log odds<a class="anchor" aria-label="anchor" href="#deriving-the-concept-of-log-odds"><i class="fas fa-link"></i></a>
</h3>
<p>Let’s look more carefully at the index of the exponential constant <span class="math inline">\(e\)</span> in the denominator of our logistic function. Note that, because <span class="math inline">\(x_{0}\)</span> is a constant, we have:</p>
<p><span class="math display">\[
-k(x - x_{0}) = -(-kx_{0} + kx) = -(\beta_{0} + \beta_1x)
\]</span>
where <span class="math inline">\(\beta_0 = -kx_0\)</span> and <span class="math inline">\(\beta_{1} = k\)</span>. Therefore,</p>
<p><span class="math display">\[
P(y = 1) = \frac{1}{1 + e^{-(\beta_0 + \beta_1x)}}
\]</span></p>
<p>This equation makes intuitive sense. As the value of <span class="math inline">\(x\)</span> increases, the value <span class="math inline">\(e^{-(\beta_0 + \beta_1x)}\)</span> gets smaller and smaller towards zero, and thus <span class="math inline">\(P(y = 1)\)</span> approaches its theoretical maximum value of 1. As the value of <span class="math inline">\(x\)</span> decreases towards zero, we see that the value of <span class="math inline">\(P(y = 1)\)</span> approaches a minimum value of <span class="math inline">\(\frac{1}{1 + e^{-\beta_0}}\)</span>. Referring back to our salespeople example, we can thus see that <span class="math inline">\(\beta_0\)</span> helps determine the baseline probability of promotion assuming no sales at all. If <span class="math inline">\(\beta_0\)</span> has an extremely negative value, this baseline probability will approach its theoretical minimum of zero.</p>
<p>Let’s formalize the role of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> in the likelihood of a positive outcome. We know that for any binary event <span class="math inline">\(y\)</span>, <span class="math inline">\(P(y = 0)\)</span> is equal to <span class="math inline">\(1 - P(y = 1)\)</span>, so</p>
<p><span class="math display">\[
\begin{aligned}
P(y = 0) &amp;= 1 - \frac{1}{1 + e^{-(\beta_0 + \beta_1x)}} \\
&amp;= \frac{1 + e^{-(\beta_0 + \beta_1x)} - 1}{1 + e^{-(\beta_0 + \beta_1x)}} \\
&amp;= \frac{e^{-(\beta_0 + \beta_1x)}}{1 + e^{-(\beta_0 + \beta_1x)}}
\end{aligned}
\]</span></p>
<p>Putting these together, we find that</p>
<p><span class="math display">\[
\begin{aligned}
\frac{P(y = 1)}{P(y = 0)} &amp;= \frac{\frac{1}{1 + e^{-(\beta_0 + \beta_1x)}}}{\frac{e^{-(\beta_0 + \beta_1x)}}{1 + e^{-(\beta_0 + \beta_1x)}}} \\
&amp;= \frac{1}{e^{-(\beta_0 + \beta_1x)}} \\
&amp;= e^{\beta_0 + \beta_1x}
\end{aligned}
\]</span></p>
<p>or alternatively, if we apply the natural logarithm to both sides</p>
<p><span class="math display">\[
\ln\left(\frac{P(y = 1)}{P(y = 0)}\right) = \beta_0 + \beta_1x
\]</span></p>
<p>The right-hand side should look familiar from the previous chapter on linear regression, meaning there is something here we can model linearly. But what is the left-hand side?</p>
<p><span class="math inline">\(P(y = 1)\)</span> is the probability that the event will occur, while <span class="math inline">\(P(y = 0)\)</span> is the probability that the event will not occur. You may be familiar from sports like horse racing or other gambling situations that the ratio of these two represents the <em>odds</em> of an event. For example, if a given horse has odds of 1:4, this means that there is a 20% probability they will win and an 80% probability they will not<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Often in sports the odds are expressed in the reverse order, but the concept is the same.&lt;/p&gt;"><sup>25</sup></a>.</p>
<p>Therefore we can conclude that the natural logarithm of the odds of <span class="math inline">\(y\)</span>—usually termed the <em>log odds</em> of <span class="math inline">\(y\)</span>—is linear in <span class="math inline">\(x\)</span>, and therefore we can model the log odds of <span class="math inline">\(y\)</span> using similar linear regression methods to those studied in Chapter <a href="linear-reg-ols.html#linear-reg-ols">4</a><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;In this case, a more general form of the Ordinary Least Squares procedure is used to fit the model, known as &lt;em&gt;maximum likelihood estimation&lt;/em&gt;.&lt;/p&gt;"><sup>26</sup></a>.</p>
</div>
<div id="modeling-the-log-odds-and-interpreting-the-coefficients" class="section level3" number="5.2.2">
<h3>
<span class="header-section-number">5.2.2</span> Modeling the log odds and interpreting the coefficients<a class="anchor" aria-label="anchor" href="#modeling-the-log-odds-and-interpreting-the-coefficients"><i class="fas fa-link"></i></a>
</h3>
<p>Let’s take our simple case of regressing the <code>promoted</code> outcome against <code>sales</code>. We use a standard binomial GLM function and our standard formula notation which we learned in the previous chapter.</p>
<div class="sourceCode" id="cb298"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># run a binomial model </span>
<span class="va">sales_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span>formula <span class="op">=</span> <span class="va">promoted</span> <span class="op">~</span> <span class="va">sales</span>, 
                   data <span class="op">=</span> <span class="va">salespeople</span>, family <span class="op">=</span> <span class="st">"binomial"</span><span class="op">)</span>

<span class="co"># view the coefficients</span>
<span class="va">sales_model</span><span class="op">$</span><span class="va">coefficients</span></code></pre></div>
<pre><code>##  (Intercept)        sales 
## -21.77642020   0.03675848</code></pre>
<p>We can interpret the coefficients as follows:</p>
<ol style="list-style-type: decimal">
<li><p>The <code>(Intercept)</code> coefficient is the value of the log odds with zero input value of <span class="math inline">\(x\)</span>—it is the log odds of promotion if you made no sales.</p></li>
<li><p>The <code>sales</code> coefficient represents the increase in the log odds of promotion associated with each unit increase in sales.</p></li>
</ol>
<p>We can convert these coefficients from log odds to odds by applying the exponent function, to return to the identity we had previously</p>
<p><span class="math display">\[
\frac{P(y = 1)}{P(y = 0)} = e^{\beta_0 + \beta_1x} = e^{\beta_0}(e^{\beta_1})^x
\]</span></p>
<p>From this, we can interpret that <span class="math inline">\(e^{\beta_0}\)</span> represents the base odds of promotion assuming no sales, and that for every additional unit sales, those base odds are multiplied by <span class="math inline">\(e^{\beta_1}\)</span>. Given this multiplicative effect that <span class="math inline">\(e^{\beta_1}\)</span> has on the odds, it is known as an <em>odds ratio</em>.</p>
<div class="sourceCode" id="cb300"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># convert log odds to base odds and odds ratio</span>
<span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="va">sales_model</span><span class="op">$</span><span class="va">coefficients</span><span class="op">)</span></code></pre></div>
<pre><code>##  (Intercept)        sales 
## 3.488357e-10 1.037442e+00</code></pre>
<p>So we can see that the base odds of promotion with zero sales is very close to zero, which makes sense. Note that odds can only be precisely zero in a situation where it is impossible to be in the positive class (that is, nobody gets promoted). We can also see that each unit (that is, every $1000) of sales multiplies the base odds by approximately 1.04—in other words, it increases the odds of promotion by 4%.</p>
</div>
<div id="odds-versus-probability" class="section level3" number="5.2.3">
<h3>
<span class="header-section-number">5.2.3</span> Odds versus probability<a class="anchor" aria-label="anchor" href="#odds-versus-probability"><i class="fas fa-link"></i></a>
</h3>
<p>It is worth spending a little time understanding the concept of odds and how it relates to probability. It is extremely common for these two terms to be used synonymously, and this can lead to serious misunderstandings when interpreting a logistic regression model.</p>
<p>If a certain event has a probability of 0.1, then this means that its odds are 1:9, or 0.111. If the probability is 0.5, then the odds are 1, if the probability is 0.9, then the odds are 9, and if the probability is 0.99, the odds are 99. As we approach a probability of 1, the odds become exponentially large, as illustrated in Figure <a href="bin-log-reg.html#fig:odds-prob">5.6</a>:</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:odds-prob"></span>
<img src="_main_files/figure-html/odds-prob-1.png" alt="Odds plotted against probability" width="672"><p class="caption">
Figure 5.6: Odds plotted against probability
</p>
</div>
<p>The consequence of this is that a given increase in odds can have a different effect on probability depending on what the original probability was in the first place. If the probability was already quite low, for example 0.1, then a 4% increase in odds translates to odds of 0.116, which translates to a new probability of 0.103586, representing an increase in probability of 3.59%, which is very close to the increase in odds. If the probability was already high, say 0.9, then a 4% increase in odds translates to odds of 9.36, which translates to a new probability of 0.903475 representing an increase in probability of 0.39%, which is very different from the increase in odds. Figure <a href="bin-log-reg.html#fig:pcoddsplot">5.7</a> shows the impact of a 4% increase in odds according to the original probability of the event.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:pcoddsplot"></span>
<img src="_main_files/figure-html/pcoddsplot-1.png" alt="Effect of 4\% increase in odds plotted against original probability" width="672"><p class="caption">
Figure 5.7: Effect of 4% increase in odds plotted against original probability
</p>
</div>
<p>We can see that the closer the base probability is to zero, the similar the effect of the increase on both odds and on probability. However, the higher the probability of the event, the less impact the increase in odds has. In any case, it’s useful to remember the formulas for converting odds to probability and vice versa. If <span class="math inline">\(O\)</span> represents odds and <span class="math inline">\(P\)</span> represents probability then we have:</p>
<p><span class="math display">\[
\begin{aligned}
O &amp;= \frac{P}{1 - P} \\
P &amp;= \frac{O}{1 + O}
\end{aligned}
\]</span></p>
</div>
</div>
<div id="running-a-multivariate-binomial-logistic-regression-model" class="section level2" number="5.3">
<h2>
<span class="header-section-number">5.3</span> Running a multivariate binomial logistic regression model<a class="anchor" aria-label="anchor" href="#running-a-multivariate-binomial-logistic-regression-model"><i class="fas fa-link"></i></a>
</h2>
<p>The derivations in the previous section extend to multivariate data. Let <span class="math inline">\(y\)</span> be a dichotomous outcome, and let <span class="math inline">\(x_1, x_2, \dots, x_p\)</span> be our input variables. Then</p>
<p><span class="math display">\[
\ln\left(\frac{P(y = 1)}{P(y = 0)}\right) = \beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_px_p
\]</span>
for coefficients <span class="math inline">\(\beta_0, \beta_1,\dots, \beta_p\)</span>. As before:</p>
<ul>
<li>
<span class="math inline">\(\beta_0\)</span> represents the log odds of our outcome when all inputs are zero</li>
<li>Each <span class="math inline">\(\beta_i\)</span> represents the increase in the log odds of our outcome associated with a unit change in <span class="math inline">\(x_i\)</span>, assuming no change in other inputs.</li>
</ul>
<p>Applying an exponent as before, we have</p>
<p><span class="math display">\[
\begin{aligned}
\frac{P(y = 1)}{P(y = 0)} &amp;= e^{\beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_px_p} \\
&amp;= e^{\beta_0}(e^{\beta_1})^{x_1}(e^{\beta_2})^{x_2}\dots(e^{\beta_p})^{x_p} 
\end{aligned}
\]</span></p>
<p>Therefore we can conclude that:</p>
<ul>
<li>
<span class="math inline">\(e^{\beta_0}\)</span> represents the odds of the outcome when all inputs are zero.</li>
<li>Each <span class="math inline">\(e^{\beta_i}\)</span> represents the <em>odds ratio</em> associated with a unit increase in <span class="math inline">\(x_i\)</span> assuming no change in the other inputs (that is, a unit increase in <span class="math inline">\(x_i\)</span> multiplies the odds of our outcome by <span class="math inline">\(e^{\beta_i}\)</span>).</li>
</ul>
<p>Let’s put this into practice.</p>
<div id="running-and-interpreting-a-multivariate-binomial-logistic-regression-model" class="section level3" number="5.3.1">
<h3>
<span class="header-section-number">5.3.1</span> Running and interpreting a multivariate binomial logistic regression model<a class="anchor" aria-label="anchor" href="#running-and-interpreting-a-multivariate-binomial-logistic-regression-model"><i class="fas fa-link"></i></a>
</h3>
<p>Let’s use a binomial logistic regression model to understand how each of the three inputs in our <code>salespeople</code> data set influence the likelihood of promotion.</p>
<p>First, as we learned previously, it is good practice to convert the categorical <code>performance</code> variable to a dummy variable<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Note that most standard modeling functions have a built-in capability to deal with categorical variables, meaning that it’s often not necessary to explicitly construct dummies. However, it is shown here for completion sake. You may wish to try running the subsequent code without explicitly constructing dummies, but note that constructing your own dummies gives you greater control over how they are labeled in any modeling output.&lt;/p&gt;"><sup>27</sup></a>.</p>
<div class="sourceCode" id="cb302"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.decisionpatterns.com">dummies</a></span><span class="op">)</span>

<span class="co"># convert performance to dummy</span>
<span class="va">perf_dummies</span> <span class="op">&lt;-</span> <span class="fu">dummies</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/dummies/man/dummy.html">dummy</a></span><span class="op">(</span><span class="st">"performance"</span>, data <span class="op">=</span> <span class="va">salespeople</span><span class="op">)</span>


<span class="co"># replace in salespeople dataframe</span>
<span class="va">salespeople_dummies</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span>
  <span class="va">salespeople</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"promoted"</span>, <span class="st">"sales"</span>, <span class="st">"customer_rate"</span><span class="op">)</span><span class="op">]</span>, 
  <span class="va">perf_dummies</span>
<span class="op">)</span>

<span class="co"># check it worked</span>
<span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">salespeople_dummies</span><span class="op">)</span></code></pre></div>
<pre><code>##   promoted sales customer_rate performance1 performance2 performance3 performance4
## 1        0   594          3.94            0            1            0            0
## 2        0   446          4.06            0            0            1            0
## 3        1   674          3.83            0            0            0            1
## 4        0   525          3.62            0            1            0            0
## 5        1   657          4.40            0            0            1            0
## 6        1   918          4.54            0            1            0            0</code></pre>
<p>Now we can run our model (using the formula <code>promoted ~ .</code> to mean regressing <code>promoted</code> against everything else) and view our coefficients.</p>
<div class="sourceCode" id="cb304"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># run binomial glm</span>
<span class="va">full_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span>formula <span class="op">=</span> <span class="st">"promoted ~ ."</span>,
                  family <span class="op">=</span> <span class="st">"binomial"</span>,
                  data <span class="op">=</span> <span class="va">salespeople_dummies</span><span class="op">)</span>

<span class="co"># get coefficient summary </span>
<span class="op">(</span><span class="va">coefs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">full_model</span><span class="op">)</span><span class="op">$</span><span class="va">coefficients</span><span class="op">)</span></code></pre></div>
<pre><code>##                   Estimate  Std. Error     z value     Pr(&gt;|z|)
## (Intercept)   -19.12443855 3.501115197 -5.46238483 4.697803e-08
## sales           0.04012425 0.006576429  6.10122119 1.052611e-09
## customer_rate  -1.11213130 0.482681585 -2.30406822 2.121881e-02
## performance1   -0.73449340 1.071963758 -0.68518492 4.932272e-01
## performance2   -0.47149387 0.933503552 -0.50507989 6.135027e-01
## performance3   -0.04953888 0.911614825 -0.05434189 9.566628e-01</code></pre>
<p>Note how only three of the <code>performance</code> dummies have displayed. This is because everyone is in one of the four performance categories, so the model is using <code>performance4</code> as the reference case. We can interpret each performance coefficient as the effect of a move to that performance category from <code>performance4</code>.</p>
<p>We can already see from the last column of our coefficient summary—the coefficient p-values—that only <code>sales</code> and <code>customer_rate</code> meet the significance threshold of less than 0.05. Interestingly, it appears from the <code>Estimate</code> column that <code>customer_rate</code> has a negative effect on the log odds of promotion. For convenience, we can add an extra column to our coefficient summary to create the exponents of our estimated coefficients so that we can see the odds ratios. We can also remove columns that are less useful to us if we wish.</p>
<div class="sourceCode" id="cb306"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># create coefficient table with estimates, p-values and odds ratios</span>
<span class="op">(</span><span class="va">full_coefs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">coefs</span><span class="op">[</span> ,<span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Estimate"</span>, <span class="st">"Pr(&gt;|z|)"</span><span class="op">)</span><span class="op">]</span>, 
                     odds_ratio <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="va">full_model</span><span class="op">$</span><span class="va">coefficients</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> </code></pre></div>
<pre><code>##                   Estimate     Pr(&gt;|z|)   odds_ratio
## (Intercept)   -19.12443855 4.697803e-08 4.947227e-09
## sales           0.04012425 1.052611e-09 1.040940e+00
## customer_rate  -1.11213130 2.121881e-02 3.288573e-01
## performance1   -0.73449340 4.932272e-01 4.797484e-01
## performance2   -0.47149387 6.135027e-01 6.240693e-01
## performance3   -0.04953888 9.566628e-01 9.516682e-01</code></pre>
<p>Now we can interpret our model as follows:</p>
<ul>
<li>All else being equal, sales have a significant positive effect on the likelihood of promotion, with each additional thousand dollars of sales increasing the odds of promotion by 4%</li>
<li>All else being equal, customer ratings have a significant negative effect on the likelihood of promotion, with one full rating higher associated with 67% lower odds of promotion</li>
<li>All else being equal, performance ratings have no significant effect on the likelihood of promotion</li>
</ul>
<p>The second conclusion may appear counter-intuitive, but remember from our pairplot in Section <a href="bin-log-reg.html#walkthrough-logit">5.1.3</a> that there is already moderate correlation between sales and customer ratings, and this model will be controlling for that relationship. Recall that our odds ratios act <em>assuming all other variables are the same</em>. Therefore, if two individuals have the same sales and performance ratings, the one with the lower customer rating is more likely to have been promoted. Similarly, if two individuals have the same level of sales and the same customer rating, their performance rating will have no significant bearing on the likelihood of promotion.</p>
<p>Many analysts will feel uncomfortable with stating these conclusions with too much precision, and therefore exponent confidence intervals can be calculated to provide a range for the odds ratios.</p>
<div class="sourceCode" id="cb308"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/confint.html">confint</a></span><span class="op">(</span><span class="va">full_model</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code>##                      2.5 %       97.5 %
## (Intercept)   1.505306e-12 1.750716e-06
## sales         1.029762e+00 1.057214e+00
## customer_rate 1.141645e-01 7.793018e-01
## performance1  5.345231e-02 3.824309e+00
## performance2  9.675452e-02 3.958066e+00
## performance3  1.591405e-01 5.976988e+00
## performance4            NA           NA</code></pre>
<p>Therefore we can say that—all else being equal—every additional unit of sales increases the odds of promotion by between 3.0% and 5.7%, and every additional point in customer rating decreases the odds of promotion by between 22% and 89%.</p>
<p>Similar to other regression models, the unit scale needs to be taken into consideration during interpretation. On first sight, a decrease of up to 89% in odds seems a lot more important than an increase of up to 5.7% in odds. However, the increase of up to 5.7% is for one unit ($1000) in many thousands of sales units, and over 10 or 100 additional units can have a substantial compound effect on odds of promotion. The decrease of up to 89% is on a full customer rating point on a scale of only 4 full points.</p>
</div>
<div id="logistic-gof" class="section level3" number="5.3.2">
<h3>
<span class="header-section-number">5.3.2</span> Understanding the fit and goodness-of-fit of a binomial logistic regression model<a class="anchor" aria-label="anchor" href="#logistic-gof"><i class="fas fa-link"></i></a>
</h3>
<p>Understanding the fit of a binomial logistic regression model is not straightforward and is sometimes controversial. Before we discuss this, let’s simplify our model based on our learning that the performance data has no significant effect on the outcome.</p>
<div class="sourceCode" id="cb310"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># simplify model</span>
<span class="va">simpler_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span>formula <span class="op">=</span> <span class="va">promoted</span> <span class="op">~</span> <span class="va">sales</span> <span class="op">+</span> <span class="va">customer_rate</span>,
                     family <span class="op">=</span> <span class="st">"binomial"</span>,
                     data <span class="op">=</span> <span class="va">salespeople</span><span class="op">)</span></code></pre></div>
<p>As in the previous chapter, again we have the luxury of a three-dimensional model, so we can visualize it in Interactive Figure <a href="bin-log-reg.html#fig:log-reg-3d">5.8</a>, revealing a 3D sigmoid curve which ‘twists’ to reflect the relative influence of <code>sales</code> and <code>customer_rate</code> on the outcome.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:log-reg-3d"></span>
<div id="htmlwidget-1b9b75ccce2be8b2321e" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-1b9b75ccce2be8b2321e">{"x":{"visdat":{"5e2f592b3675":["function () ","plotlyVisDat"]},"cur_data":"5e2f592b3675","attrs":{"5e2f592b3675":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"mode":"markers","type":"scatter3d","marker":{"size":5,"color":"blue","symbol":104},"name":"Observations","inherit":true},"5e2f592b3675.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":[0.513043787470359,0.00232903663328908,0.967913256750304,0.0850551708097334,0.888995085423327,0.999996140019344,3.94158464693613e-05,3.35263706194057e-05,5.01060177266513e-05,0.000707188247832798,0.276972293777689,0.997142089228203,0.276937782290319,0.00707098121407314,0.00012184293115255,0.00014545030098594,6.84915273186555e-06,0.000143211579530735,0.00152892829256721,0.9999969975465,0.992416730662413,0.00480633124196493,0.0509105644536528,5.86285387060062e-05,0.999809822413457,0.996943317290957,0.000118615465376282,2.2450005899479e-06,2.1214280004875e-05,0.997755806949912,0.0160362485836803,0.0172087806009677,3.25172961244736e-06,0.999957992739096,0.000659644995444577,0.998993729278265,0.998006324427762,0.00163509087030353,0.999998798150486,0.142827193798469,0.000523614576483736,1.02804069437619e-05,0.999998997898883,0.0555559926345098,7.95572439847695e-05,0.00078402305155006,0.0157209181560215,0.00123316245855046,0.00915518434736615,0.964635086181134,0.298991498178672,0.00172169111871495,0.00731149415558982,0.00299267062629279,8.13486277352262e-05,0.0009919685890872,0.999849046334855,0.000169404711508035,0.99881729861371,0.000878745611777349,0.999405643490919,0.00194270040136403,0.994956412156812,0.969409271404243,0.000983045026702069,0.0441228624337416,0.0366721437432225,0.0118950568704761,0.0973087746178125,5.93208210886117e-06,5.18213033962158e-05,0.000122682802644693,0.130380489644764,0.0012194896705953,0.0860989360858597,0.654257531860114,0.999811091710991,0.00208252587854619,0.000574009405357797,0.0339125416269992,0.999740829250325,0.188419968797621,0.00555773532145034,0.985338268945737,0.915730894027182,0.00085938512230808,0.00399383426060736,0.999118267829343,3.91734674643955e-06,0.00109726041757671,0.999482941282944,0.632630647012708,0.999998751657777,0.190203738191751,0.000222840756953284,0.00180852289496773,0.0326826319192853,0.00150508224144465,0.875981581307738,0.000667121649622688,0.000384207446612768,0.000867236915514207,0.000266544672797829,0.00447525670063509,0.023412393006146,0.900628860957561,0.00082007828253893,0.00469963611389774,5.25291784366936e-05,0.000456582883875495,0.998307838314995,0.762230497902839,0.000583112445221324,0.014266535715782,0.000516517219795822,0.000427859101273705,0.000366552783603423,0.00961029895244763,0.999997736956588,0.997333072816588,0.235084145549547,0.996480775935701,0.22953554089936,0.00176029224675975,0.0100038504855864,0.00143895377467194,0.998959255826982,0.999987657848766,0.00574625393012701,0.999999326398943,6.34501829264573e-06,0.00154231280936458,0.000326190798410533,0.00621464544507926,3.90598119936624e-05,0.999996744953141,5.73891575051991e-07,1.55289519923123e-05,0.999676378660561,0.00288103881820208,0.000183690909802646,0.936349307443568,0.00618659867184173,0.999999026957478,0.001101975810758,0.000158771891172238,0.000291612404873436,0.0237744003735563,0.00445476983993911,0.00137262353986326,0.00909048367885119,6.36011351721503e-06,0.99973845989494,2.45528218049459e-05,0.0217326511556995,0.999821820242306,0.999843548335401,0.997444360668698,8.99894446639757e-05,3.77781855569176e-06,0.999975535144643,0.000677836509799099,0.000411918822778071,0.00504967855302496,0.999999336920644,0.0125462326909835,0.0141036046706846,0.000434707796178076,0.999999329592644,0.998359309181947,0.00255829347595589,0.127099331520617,0.99676094350263,0.983333732522409,6.92357788813096e-05,0.932780957179355,0.0488994925723189,0.915726460856288,0.262312705733867,0.0120798125799123,0.000331422155550535,0.409159254733535,0.872561766823381,0.99934408759309,0.00119741205504615,0.000796293984344839,0.891187613202626,0.944153999722608,0.99609126241652,6.69325770237538e-05,0.000829110047541921,0.00288136889784001,0.00178460304627864,0.000765039230798448,2.87341679526324e-06,3.89659904413517e-05,0.958234623862356,0.504065835592993,0.99973967717072,0.999972203869007,0.00104921212035199,0.971593802993312,0.948063780841036,0.00829883464852333,0.00427906522281583,0.340924115299724,0.000126259041340483,0.000284409226334902,0.99997909217582,0.000479806115653712,0.00180047984765014,0.999995826774106,0.740933131031879,0.0309538523500136,0.0147460768105556,0.000333572067183507,0.00965454007601623,0.995356440624207,0.992519228710186,0.961268412902984,0.0131116410047973,9.92236475060711e-07,0.999997274345737,0.00123312708195465,0.00109013125315413,0.0221187522128585,3.58703961056395e-05,0.999473582638834,0.00132489962203097,0.999999769575772,0.999153140599956,0.00742538062757464,0.000495046906656143,0.0399741920393675,0.028009469676191,0.0458492072779167,0.0908155821376306,0.997135309500949,0.0087740321902466,0.999999683804294,0.996894964614887,0.000288367833980847,0.00177235522052612,0.457063903985327,0.578590129340158,0.00145545924330552,0.0026457962736067,0.995539430548577,0.00182028939482876,0.000591124331979311,0.000362505929608337,3.0790540845672e-05,0.999998062284237,8.96400573157646e-06,0.999997879812384,0.999641213089921,0.999916920484269,0.000816263547081754,0.013552896352098,6.4886446899742e-06,0.000263692498092281,1.11305841836127e-07,0.00426081294163332,0.00115023341429871,1.82625853985749e-06,0.988786482675747,0.866145126912557,7.01553181649738e-05,0.00126060737761916,3.03156990654424e-05,0.272426780898702,0.177394359435125,0.99998355251201,0.000284409226334902,0.993031647172845,5.09241607681648e-06,0.000193894837854818,0.0290001944219321,0.999855020791343,0.00051669519286774,0.99995516374663,3.71812867301718e-05,0.127866710849074,0.00270012638169066,0.000615402024466842,0.109144176687199,0.000198254547568351,0.0267548922535356,1.31897314081956e-05,0.995241808835247,0.884286166185453,0.9999568347781,0.29803239897586,0.0269846498868655,0.00190037928182268,0.00248514746809089,0.00413859902178308,6.13285253006733e-05,0.999946228569285,2.89221960859789e-05,0.0450778529370493,0.00012265814096524,0.999983592019718,8.01056607525442e-05,0.0110348086350056,0.998284811133502,0.99939895815811,2.99218887914604e-06,0.427194845578936,0.00674775275480331,0.261018691852725,0.00360284830258554,0.999998339795156,0.00056503217303787,5.83942300144117e-06,0.0131740265571074,0.190177194536309,0.000955059386512636,0.189833583998875,0.0859414705368557,0.997626404972969,0.0184643952845435,0.00118698304721022,0.975029541105096,0.00797431740816249,2.03418748597587e-06,0.0024304808904848,6.7529079488795e-05,0.999721560856028,0.999571643115692,0.0157874843665773,0.997709638990727,0.203263346467612,7.13021153112378e-06,0.0024201638965908,0.999840743000542,1.83890140271786e-06,0.0234077962518407,0.000205898663575221,2.42149864338555e-05,0.00537337201022365,0.999997340164109,0.37231494492211,0.999108122648289,0.77655885885669,0.015618494795571,0.580243362219714,0.00595721992487906,0.0179436359933955,0.00067319868260558],"x":{},"y":{},"type":"mesh3d","name":"Fitted values","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"sales"},"yaxis":{"title":"customer_rate","nticks":5},"camera":{"eye":{"x":-0.5,"y":2,"z":0}},"zaxis":{"title":"promoted"},"aspectmode":"cube"},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[594,446,674,525,657,918,318,364,342,387,527,716,557,450,344,372,258,338,410,937,702,469,535,342,819,736,330,274,341,717,478,487,239,825,400,728,773,425,943,510,389,270,945,497,329,389,475,383,432,619,578,411,445,440,359,419,840,393,754,441,803,444,753,688,431,511,464,473,532,280,342,320,531,373,547,611,825,431,401,517,803,586,444,693,659,416,423,756,245,419,757,617,909,516,317,425,528,416,645,390,393,394,387,450,487,607,369,489,324,417,694,651,395,442,422,404,381,501,944,753,591,735,538,451,477,436,738,902,464,944,285,453,382,414,335,935,203,348,800,436,360,674,425,901,453,350,362,486,471,459,506,262,825,291,464,802,818,736,364,308,862,349,375,423,938,456,517,373,898,777,470,545,699,697,300,677,497,669,596,492,346,590,592,780,432,418,662,678,716,330,414,416,403,362,284,363,655,597,794,818,409,681,606,489,475,590,396,420,857,371,421,828,594,533,462,392,475,752,659,650,496,211,898,388,383,455,319,756,377,940,757,469,394,484,491,547,519,739,479,943,742,357,432,584,595,401,460,753,466,362,361,338,882,293,922,793,787,400,516,295,307,151,441,406,270,680,662,347,453,309,592,540,886,420,718,284,323,513,841,362,842,321,516,428,383,521,358,489,252,720,610,871,594,522,379,454,450,317,835,297,516,355,858,305,410,707,798,265,576,448,590,456,930,412,286,440,546,385,544,505,732,506,394,674,458,251,429,348,789,795,509,754,580,289,390,787,241,522,412,359,489,940,592,796,653,459,586,401,500,373],"y":[3.94,4.06,3.83,3.62,4.4,4.54,3.09,4.89,3.74,3,2.43,3.16,3.51,3.21,3.02,3.87,2.49,2.66,3.14,5,3.53,4.24,4.47,3.6,4.45,3.94,2.54,4.06,4.47,2.98,3.48,3.74,2.47,3.32,3.53,2.66,4.89,3.62,4.4,2.56,3.34,2.56,4.31,3.02,2.86,2.98,3.39,2.36,2.33,1.94,4.17,3.07,3,3.62,3.92,3.85,5,4.49,3.74,4.75,4.89,4.15,5,4.29,4.29,3.74,2.22,3.57,3.74,3.41,3.71,2.15,3.41,2.01,4.4,4.03,4.66,3.62,3.69,4.2,4.15,5,3.21,3.8,4.2,3.87,2.75,3.55,2.52,3.76,3.11,4.33,3.21,2.47,1.51,3.53,4.63,3.37,4.08,3.16,3.76,3.07,3.87,3.62,3.46,2.49,2.22,4.98,3.05,4.47,1.9,5,3.46,2.29,4.54,4.06,3.37,4.77,5,4.43,4.93,4.03,3.05,4.49,3.87,4.13,3.05,5,3.9,3.92,3.53,4.68,3.51,2.03,3.71,5,2.72,5,4.24,3.51,3.23,4.47,2.43,2.7,4.98,3,2.89,3.41,4.38,5,5,2.7,4.95,2.54,2.7,3.78,4.24,3.78,4.01,4.82,4.17,1.67,3.05,2.54,3.69,2.91,5,2.93,2.26,4.86,4.84,3.94,2.66,4.06,1.94,4.63,3.14,4.56,4.98,4.24,2.2,4.17,2.2,4.15,4.15,4.01,4.56,4.49,3.44,3.05,3.83,2.79,2.75,2.03,4.2,4.72,3.39,4.08,3.83,2.7,3.44,3.97,1.83,4.47,4.56,4.43,4.86,5,3.85,2.77,3.39,1.37,3.05,4.86,2.98,3.85,3.83,4.89,1.97,3.14,4.31,2.52,3.51,2.54,2.47,2.36,3.21,3.09,2.08,2.82,3.55,3.85,3.57,2.86,3.44,5,3.34,3.99,4.06,3.21,4.17,2.72,3.8,3.78,3.74,2.86,4.45,4.89,5,2.26,2.66,4.03,2.63,3.51,4.15,4.08,2.56,3.34,5,3.87,1,2.31,3.34,3.25,4.1,3.09,4.77,3.62,4.86,3,4.79,3.41,4.68,5,4.03,3.69,1.85,4.2,5,2.38,3.99,3.25,2.89,3.28,2.98,3.23,3.09,3.41,1.69,3.76,2.75,5,4.75,4.59,1.83,4.29,3.69,2.66,3.9,2.61,3.9,3.41,3.67,1.99,1.37,2.38,4.72,3.48,3.6,3.18,4.77,4.03,4.22,4.1,3.64,2.29,3.55,2.66,3.48,2.89,3.57,4.36,2.79,3.6,3.39,3.32,3.41,3.69,3.71,4.31,4.61,4.33,4.7,3.57,2.01,3.14,3.05,4.72,5,5,4.86,5,4.38,5,5,2.82,3.41,1.6,4.17,2.54],"z":["0","0","1","0","1","1","0","0","0","0","0","1","0","0","0","0","0","0","0","1","1","0","0","0","1","1","0","0","0","1","0","0","0","1","0","1","1","0","1","0","0","0","1","0","0","0","0","0","1","1","1","0","0","0","0","0","1","0","1","0","1","0","1","1","0","0","0","0","0","0","0","0","0","0","0","1","1","0","0","0","1","0","0","1","1","0","0","1","0","0","1","1","1","0","0","0","0","0","1","0","0","0","0","0","0","1","0","0","0","0","1","1","0","0","0","0","0","0","1","1","0","1","1","0","0","0","1","1","0","1","0","0","0","0","0","1","0","0","1","0","0","1","0","1","0","0","0","0","0","0","0","0","1","0","1","1","1","1","0","0","1","0","0","0","1","0","0","0","1","1","0","0","1","1","0","1","0","1","1","0","0","1","0","1","0","0","1","1","1","0","0","0","0","0","0","0","1","0","1","1","0","1","1","0","0","0","0","0","1","0","0","1","0","0","0","0","0","1","1","1","0","0","1","0","0","0","0","1","0","1","1","0","0","0","0","0","0","1","0","1","1","0","0","0","1","0","0","1","0","0","0","0","1","0","1","1","1","0","0","0","0","0","0","0","0","1","1","0","0","0","0","0","1","0","1","0","0","0","1","0","1","0","0","0","0","1","0","0","0","1","1","1","0","0","0","0","0","0","1","0","0","0","1","0","0","1","1","0","1","0","0","0","1","0","0","0","0","0","0","0","1","0","0","1","0","0","0","0","1","1","0","1","0","0","0","1","0","0","0","0","0","1","0","1","1","0","0","0","0","0"],"mode":"markers","type":"scatter3d","marker":{"color":"blue","size":5,"symbol":104,"line":{"color":"rgba(31,119,180,1)"}},"name":"Observations","error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"promoted","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":true,"z":[0.513043787470359,0.00232903663328908,0.967913256750304,0.0850551708097334,0.888995085423327,0.999996140019344,3.94158464693613e-05,3.35263706194057e-05,5.01060177266513e-05,0.000707188247832798,0.276972293777689,0.997142089228203,0.276937782290319,0.00707098121407314,0.00012184293115255,0.00014545030098594,6.84915273186555e-06,0.000143211579530735,0.00152892829256721,0.9999969975465,0.992416730662413,0.00480633124196493,0.0509105644536528,5.86285387060062e-05,0.999809822413457,0.996943317290957,0.000118615465376282,2.2450005899479e-06,2.1214280004875e-05,0.997755806949912,0.0160362485836803,0.0172087806009677,3.25172961244736e-06,0.999957992739096,0.000659644995444577,0.998993729278265,0.998006324427762,0.00163509087030353,0.999998798150486,0.142827193798469,0.000523614576483736,1.02804069437619e-05,0.999998997898883,0.0555559926345098,7.95572439847695e-05,0.00078402305155006,0.0157209181560215,0.00123316245855046,0.00915518434736615,0.964635086181134,0.298991498178672,0.00172169111871495,0.00731149415558982,0.00299267062629279,8.13486277352262e-05,0.0009919685890872,0.999849046334855,0.000169404711508035,0.99881729861371,0.000878745611777349,0.999405643490919,0.00194270040136403,0.994956412156812,0.969409271404243,0.000983045026702069,0.0441228624337416,0.0366721437432225,0.0118950568704761,0.0973087746178125,5.93208210886117e-06,5.18213033962158e-05,0.000122682802644693,0.130380489644764,0.0012194896705953,0.0860989360858597,0.654257531860114,0.999811091710991,0.00208252587854619,0.000574009405357797,0.0339125416269992,0.999740829250325,0.188419968797621,0.00555773532145034,0.985338268945737,0.915730894027182,0.00085938512230808,0.00399383426060736,0.999118267829343,3.91734674643955e-06,0.00109726041757671,0.999482941282944,0.632630647012708,0.999998751657777,0.190203738191751,0.000222840756953284,0.00180852289496773,0.0326826319192853,0.00150508224144465,0.875981581307738,0.000667121649622688,0.000384207446612768,0.000867236915514207,0.000266544672797829,0.00447525670063509,0.023412393006146,0.900628860957561,0.00082007828253893,0.00469963611389774,5.25291784366936e-05,0.000456582883875495,0.998307838314995,0.762230497902839,0.000583112445221324,0.014266535715782,0.000516517219795822,0.000427859101273705,0.000366552783603423,0.00961029895244763,0.999997736956588,0.997333072816588,0.235084145549547,0.996480775935701,0.22953554089936,0.00176029224675975,0.0100038504855864,0.00143895377467194,0.998959255826982,0.999987657848766,0.00574625393012701,0.999999326398943,6.34501829264573e-06,0.00154231280936458,0.000326190798410533,0.00621464544507926,3.90598119936624e-05,0.999996744953141,5.73891575051991e-07,1.55289519923123e-05,0.999676378660561,0.00288103881820208,0.000183690909802646,0.936349307443568,0.00618659867184173,0.999999026957478,0.001101975810758,0.000158771891172238,0.000291612404873436,0.0237744003735563,0.00445476983993911,0.00137262353986326,0.00909048367885119,6.36011351721503e-06,0.99973845989494,2.45528218049459e-05,0.0217326511556995,0.999821820242306,0.999843548335401,0.997444360668698,8.99894446639757e-05,3.77781855569176e-06,0.999975535144643,0.000677836509799099,0.000411918822778071,0.00504967855302496,0.999999336920644,0.0125462326909835,0.0141036046706846,0.000434707796178076,0.999999329592644,0.998359309181947,0.00255829347595589,0.127099331520617,0.99676094350263,0.983333732522409,6.92357788813096e-05,0.932780957179355,0.0488994925723189,0.915726460856288,0.262312705733867,0.0120798125799123,0.000331422155550535,0.409159254733535,0.872561766823381,0.99934408759309,0.00119741205504615,0.000796293984344839,0.891187613202626,0.944153999722608,0.99609126241652,6.69325770237538e-05,0.000829110047541921,0.00288136889784001,0.00178460304627864,0.000765039230798448,2.87341679526324e-06,3.89659904413517e-05,0.958234623862356,0.504065835592993,0.99973967717072,0.999972203869007,0.00104921212035199,0.971593802993312,0.948063780841036,0.00829883464852333,0.00427906522281583,0.340924115299724,0.000126259041340483,0.000284409226334902,0.99997909217582,0.000479806115653712,0.00180047984765014,0.999995826774106,0.740933131031879,0.0309538523500136,0.0147460768105556,0.000333572067183507,0.00965454007601623,0.995356440624207,0.992519228710186,0.961268412902984,0.0131116410047973,9.92236475060711e-07,0.999997274345737,0.00123312708195465,0.00109013125315413,0.0221187522128585,3.58703961056395e-05,0.999473582638834,0.00132489962203097,0.999999769575772,0.999153140599956,0.00742538062757464,0.000495046906656143,0.0399741920393675,0.028009469676191,0.0458492072779167,0.0908155821376306,0.997135309500949,0.0087740321902466,0.999999683804294,0.996894964614887,0.000288367833980847,0.00177235522052612,0.457063903985327,0.578590129340158,0.00145545924330552,0.0026457962736067,0.995539430548577,0.00182028939482876,0.000591124331979311,0.000362505929608337,3.0790540845672e-05,0.999998062284237,8.96400573157646e-06,0.999997879812384,0.999641213089921,0.999916920484269,0.000816263547081754,0.013552896352098,6.4886446899742e-06,0.000263692498092281,1.11305841836127e-07,0.00426081294163332,0.00115023341429871,1.82625853985749e-06,0.988786482675747,0.866145126912557,7.01553181649738e-05,0.00126060737761916,3.03156990654424e-05,0.272426780898702,0.177394359435125,0.99998355251201,0.000284409226334902,0.993031647172845,5.09241607681648e-06,0.000193894837854818,0.0290001944219321,0.999855020791343,0.00051669519286774,0.99995516374663,3.71812867301718e-05,0.127866710849074,0.00270012638169066,0.000615402024466842,0.109144176687199,0.000198254547568351,0.0267548922535356,1.31897314081956e-05,0.995241808835247,0.884286166185453,0.9999568347781,0.29803239897586,0.0269846498868655,0.00190037928182268,0.00248514746809089,0.00413859902178308,6.13285253006733e-05,0.999946228569285,2.89221960859789e-05,0.0450778529370493,0.00012265814096524,0.999983592019718,8.01056607525442e-05,0.0110348086350056,0.998284811133502,0.99939895815811,2.99218887914604e-06,0.427194845578936,0.00674775275480331,0.261018691852725,0.00360284830258554,0.999998339795156,0.00056503217303787,5.83942300144117e-06,0.0131740265571074,0.190177194536309,0.000955059386512636,0.189833583998875,0.0859414705368557,0.997626404972969,0.0184643952845435,0.00118698304721022,0.975029541105096,0.00797431740816249,2.03418748597587e-06,0.0024304808904848,6.7529079488795e-05,0.999721560856028,0.999571643115692,0.0157874843665773,0.997709638990727,0.203263346467612,7.13021153112378e-06,0.0024201638965908,0.999840743000542,1.83890140271786e-06,0.0234077962518407,0.000205898663575221,2.42149864338555e-05,0.00537337201022365,0.999997340164109,0.37231494492211,0.999108122648289,0.77655885885669,0.015618494795571,0.580243362219714,0.00595721992487906,0.0179436359933955,0.00067319868260558],"x":[594,446,674,525,657,918,318,364,342,387,527,716,557,450,344,372,258,338,410,937,702,469,535,342,819,736,330,274,341,717,478,487,239,825,400,728,773,425,943,510,389,270,945,497,329,389,475,383,432,619,578,411,445,440,359,419,840,393,754,441,803,444,753,688,431,511,464,473,532,280,342,320,531,373,547,611,825,431,401,517,803,586,444,693,659,416,423,756,245,419,757,617,909,516,317,425,528,416,645,390,393,394,387,450,487,607,369,489,324,417,694,651,395,442,422,404,381,501,944,753,591,735,538,451,477,436,738,902,464,944,285,453,382,414,335,935,203,348,800,436,360,674,425,901,453,350,362,486,471,459,506,262,825,291,464,802,818,736,364,308,862,349,375,423,938,456,517,373,898,777,470,545,699,697,300,677,497,669,596,492,346,590,592,780,432,418,662,678,716,330,414,416,403,362,284,363,655,597,794,818,409,681,606,489,475,590,396,420,857,371,421,828,594,533,462,392,475,752,659,650,496,211,898,388,383,455,319,756,377,940,757,469,394,484,491,547,519,739,479,943,742,357,432,584,595,401,460,753,466,362,361,338,882,293,922,793,787,400,516,295,307,151,441,406,270,680,662,347,453,309,592,540,886,420,718,284,323,513,841,362,842,321,516,428,383,521,358,489,252,720,610,871,594,522,379,454,450,317,835,297,516,355,858,305,410,707,798,265,576,448,590,456,930,412,286,440,546,385,544,505,732,506,394,674,458,251,429,348,789,795,509,754,580,289,390,787,241,522,412,359,489,940,592,796,653,459,586,401,500,373],"y":[3.94,4.06,3.83,3.62,4.4,4.54,3.09,4.89,3.74,3,2.43,3.16,3.51,3.21,3.02,3.87,2.49,2.66,3.14,5,3.53,4.24,4.47,3.6,4.45,3.94,2.54,4.06,4.47,2.98,3.48,3.74,2.47,3.32,3.53,2.66,4.89,3.62,4.4,2.56,3.34,2.56,4.31,3.02,2.86,2.98,3.39,2.36,2.33,1.94,4.17,3.07,3,3.62,3.92,3.85,5,4.49,3.74,4.75,4.89,4.15,5,4.29,4.29,3.74,2.22,3.57,3.74,3.41,3.71,2.15,3.41,2.01,4.4,4.03,4.66,3.62,3.69,4.2,4.15,5,3.21,3.8,4.2,3.87,2.75,3.55,2.52,3.76,3.11,4.33,3.21,2.47,1.51,3.53,4.63,3.37,4.08,3.16,3.76,3.07,3.87,3.62,3.46,2.49,2.22,4.98,3.05,4.47,1.9,5,3.46,2.29,4.54,4.06,3.37,4.77,5,4.43,4.93,4.03,3.05,4.49,3.87,4.13,3.05,5,3.9,3.92,3.53,4.68,3.51,2.03,3.71,5,2.72,5,4.24,3.51,3.23,4.47,2.43,2.7,4.98,3,2.89,3.41,4.38,5,5,2.7,4.95,2.54,2.7,3.78,4.24,3.78,4.01,4.82,4.17,1.67,3.05,2.54,3.69,2.91,5,2.93,2.26,4.86,4.84,3.94,2.66,4.06,1.94,4.63,3.14,4.56,4.98,4.24,2.2,4.17,2.2,4.15,4.15,4.01,4.56,4.49,3.44,3.05,3.83,2.79,2.75,2.03,4.2,4.72,3.39,4.08,3.83,2.7,3.44,3.97,1.83,4.47,4.56,4.43,4.86,5,3.85,2.77,3.39,1.37,3.05,4.86,2.98,3.85,3.83,4.89,1.97,3.14,4.31,2.52,3.51,2.54,2.47,2.36,3.21,3.09,2.08,2.82,3.55,3.85,3.57,2.86,3.44,5,3.34,3.99,4.06,3.21,4.17,2.72,3.8,3.78,3.74,2.86,4.45,4.89,5,2.26,2.66,4.03,2.63,3.51,4.15,4.08,2.56,3.34,5,3.87,1,2.31,3.34,3.25,4.1,3.09,4.77,3.62,4.86,3,4.79,3.41,4.68,5,4.03,3.69,1.85,4.2,5,2.38,3.99,3.25,2.89,3.28,2.98,3.23,3.09,3.41,1.69,3.76,2.75,5,4.75,4.59,1.83,4.29,3.69,2.66,3.9,2.61,3.9,3.41,3.67,1.99,1.37,2.38,4.72,3.48,3.6,3.18,4.77,4.03,4.22,4.1,3.64,2.29,3.55,2.66,3.48,2.89,3.57,4.36,2.79,3.6,3.39,3.32,3.41,3.69,3.71,4.31,4.61,4.33,4.7,3.57,2.01,3.14,3.05,4.72,5,5,4.86,5,4.38,5,5,2.82,3.41,1.6,4.17,2.54],"type":"mesh3d","name":"Fitted values","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script><p class="caption">
Figure 5.8: 3D visualization of the fitted <code>simpler_model</code> against the <code>salespeople</code> data
</p>
</div>
<p>Now let’s look at the summary of our <code>simpler_model</code>.</p>
<div class="sourceCode" id="cb311"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">simpler_model</span><span class="op">)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = promoted ~ sales + customer_rate, family = "binomial", 
##     data = salespeople)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.02984  -0.09256  -0.02070   0.00874   3.06380  
## 
## Coefficients:
##                 Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   -19.517689   3.346762  -5.832 5.48e-09 ***
## sales           0.040389   0.006525   6.190 6.03e-10 ***
## customer_rate  -1.122064   0.466958  -2.403   0.0163 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 440.303  on 349  degrees of freedom
## Residual deviance:  65.131  on 347  degrees of freedom
## AIC: 71.131
## 
## Number of Fisher Scoring iterations: 8</code></pre>
<p>Note that, unlike what we saw for linear regression in Section <a href="linear-reg-ols.html#lin-good-fit">4.3.3</a>, our summary does not provide a statistic on overall model fit or goodness-of-fit. The main reason for this is that there is no clear unified point of view in the statistics community on a single appropriate measure for model fit in the case of logistic regression. Nevertheless, a number of options are available to analysts for estimating fit and goodness-of-fit for these models.</p>
<p><em>Pseudo-<span class="math inline">\(R^2\)</span></em> measures are attempts to estimate the amount of variance in the outcome that is explained by the fitted model, analogous to the <span class="math inline">\(R^2\)</span> in linear regression. There are numerous variants of pseudo-<span class="math inline">\(R^2\)</span> with some of the most common listed here:</p>
<ul>
<li>McFadden’s <span class="math inline">\(R^2\)</span> works by comparing the likelihood function of the fitted model with that of a random model and using this to estimate the explained variance in the outcome.</li>
<li>Cox and Snell’s <span class="math inline">\(R^2\)</span> works by applying a ‘sum of squares’ analogy to the likelihood functions to align more closely with the precise methodology for calculating <span class="math inline">\(R^2\)</span> in linear regression. However, this usually means that the maximum value is less than 1 and in certain circumstances substantially less than 1, which can be problematic and unintuitive for an <span class="math inline">\(R^2\)</span>.</li>
<li>Nagelkerke’s <span class="math inline">\(R^2\)</span> resolves the issue with the upper bound for Cox and Snell by dividing Cox and Snell’s <span class="math inline">\(R^2\)</span> by its upper bound. This restores an intuitive scale with a maximum of 1, but is considered somewhat arbitrary with limited theoretical foundation.</li>
<li>Tjur’s <span class="math inline">\(R^2\)</span> is a more recent and simpler concept. It is defined as simply the absolute difference between the predicted probabilities of the positive observations and those of the negative observations.</li>
</ul>
<p>Standard modeling functions generally do not offer the calculation of pseudo-<span class="math inline">\(R^2\)</span> as standard, but numerous methods are available for their calculation. For example:</p>
<div class="sourceCode" id="cb313"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://andrisignorell.github.io/DescTools/">DescTools</a></span><span class="op">)</span>
<span class="fu">DescTools</span><span class="fu">::</span><span class="fu"><a href="https://andrisignorell.github.io/DescTools/reference/PseudoR2.html">PseudoR2</a></span><span class="op">(</span>
  <span class="va">simpler_model</span>, 
  which <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"McFadden"</span>, <span class="st">"CoxSnell"</span>, <span class="st">"Nagelkerke"</span>, <span class="st">"Tjur"</span><span class="op">)</span>
<span class="op">)</span></code></pre></div>
<pre><code>##   McFadden   CoxSnell Nagelkerke       Tjur 
##  0.8520759  0.6576490  0.9187858  0.8784834</code></pre>
<p>We see that the Cox and Snell variant is notably lower than the other estimates, which is consistent with the known issues with its upper bound. However, the other estimates are reasonably aligned and suggest a strong fit.</p>
<p>Goodness-of-fit tests for logistic regression models compare the predictions to the observed outcome and test the null hypothesis that they are similar. This means that, unlike in linear regression, a low p-value indicates a poor fit. One commonly used method is the Hosmer-Lemeshow test, which divides the observations into a number of groups (usually 10) according to their fitted probabilities, calculates the proportion of each group that is positive and then compares this to the expected proportions based on the model prediction using a Chi-squared test. However, this method has limitations. It is particularly problematic for situations where there is a low sample size and can return highly varied results based on the number of groups used. It is therefore recommended to use a range of goodness-of-fit tests, and not rely entirely on any one specific approach.</p>
<p>In R, the <code>LogisticDx</code> package offers a range of diagnostic tools for logistic regression models, and is recommended for exploration. Here is an example using the <code><a href="https://rdrr.io/pkg/LogisticDx/man/gof.html">gof()</a></code> function for assessing goodness-of-fit.</p>
<div class="sourceCode" id="cb315"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">LogisticDx</span><span class="op">)</span>

<span class="co"># get range of goodness-of-fit diagnostics</span>
<span class="va">simpler_model_diagnostics</span> <span class="op">&lt;-</span> <span class="fu">LogisticDx</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/LogisticDx/man/gof.html">gof</a></span><span class="op">(</span><span class="va">simpler_model</span>, 
                                             plotROC <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span>

<span class="co"># returns a list</span>
<span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">simpler_model_diagnostics</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] "ct"    "chiSq" "ctHL"  "gof"   "R2"    "auc"</code></pre>
<p>The <code>gof</code> object in this list provides a range of variants of goodness-of-fit statistics.</p>
<div class="sourceCode" id="cb317"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># in our case we are interested in goodness-of-fit statistics</span>
<span class="va">simpler_model_diagnostics</span><span class="op">$</span><span class="va">gof</span></code></pre></div>
<pre><code>##          test  stat         val df        pVal
## 1:         HL chiSq  3.44576058  8 0.903358158
## 2:        mHL     F  2.74709957  8 0.005971045
## 3:       OsRo     Z -0.02415249 NA 0.980730971
## 4: SstPgeq0.5     Z  0.88656856 NA 0.375311227
## 5:   SstPl0.5     Z  0.96819352 NA 0.332947728
## 6:    SstBoth chiSq  1.72340251  2 0.422442787
## 7: SllPgeq0.5 chiSq  1.85473814  1 0.173233325
## 8:   SllPl0.5 chiSq  0.68570870  1 0.407627859
## 9:    SllBoth chiSq  1.86640617  2 0.393291943</code></pre>
<p>This confirms that almost all tests, including the Hosmer-Lemeshow test, which is the first in the list, suggest a fit for our model.</p>
<p>Various measures of predictive accuracy can also be used to assess a binomial logistic regression model in a predictive context, such as precision, recall and ROC-curve analysis. These are particularly suited for implementations of logistic regression models as predictive classifiers in a Machine Learning context, a topic which is outside the scope of this book. However, a recommended source for a deeper treatment of goodness-of-fit tests for logistic regression models is <span class="citation"><a href="references.html#ref-hosmer-logistic" role="doc-biblioref">Hosmer, Lemeshow, and Sturdivant</a> (<a href="references.html#ref-hosmer-logistic" role="doc-biblioref">2013</a>)</span>.</p>
</div>
<div id="model-parsimony" class="section level3" number="5.3.3">
<h3>
<span class="header-section-number">5.3.3</span> Model parsimony<a class="anchor" aria-label="anchor" href="#model-parsimony"><i class="fas fa-link"></i></a>
</h3>
<p>We saw that in both our linear regression and our logistic regression approach, we decided to drop variables from our model when we determined that they had no significant effect on the outcome. The principle of <em>Occam’s Razor</em> states that—all else being equal—the simplest explanation is the best. In this sense, a model that contains information that does not contribute to its primary inference objective is more complex than it needs to be. Such a model increases the communication burden in explaining its results to others, with no notable analytic benefit in return.</p>
<p><em>Parsimony</em> describes the concept of being careful with resources or with information. A model could be described as more parsimonious if it can achieve the same (or very close to the same) fit with a smaller number of inputs. The <em>Akaike Information Criterion</em> or <em>AIC</em> is a measure of model parsimony that is computed for log-likelihood models like logistic regression models, with a lower AIC indicating a more parsimonious model. AIC is often calculated as standard in summary reports of logistic regression models but can also be calculated independently. Let’s compare the different iterations of our model in this chapter using AIC.</p>
<div class="sourceCode" id="cb319"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># sales only model</span>
<span class="fu"><a href="https://rdrr.io/r/stats/AIC.html">AIC</a></span><span class="op">(</span><span class="va">sales_model</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] 76.49508</code></pre>
<div class="sourceCode" id="cb321"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># sales and customer rating model</span>
<span class="fu"><a href="https://rdrr.io/r/stats/AIC.html">AIC</a></span><span class="op">(</span><span class="va">simpler_model</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] 71.13145</code></pre>
<div class="sourceCode" id="cb323"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># model with all inputs</span>
<span class="fu"><a href="https://rdrr.io/r/stats/AIC.html">AIC</a></span><span class="op">(</span><span class="va">full_model</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] 76.37433</code></pre>
<p>We can see that the model which is limited to our two significant inputs—sales and customer rating—is determined to be the most parsimonious model according to the AIC. Note that the AIC should not be used to interpret model quality or confidence—it is possible that the lowest AIC might still be a very poor fit.</p>
<p>Model parsimony becomes a substantial concern when there is a large number of input variables. As a general rule, the more input variables there are in a model the greater the chance that the model will be difficult to interpret clearly, and the greater the risk of measurement problems, such as multicollinearity. Analysts who are eager to please their customers, clients, professors or bosses can easily be tempted to think up new potential inputs to their model, often derived mathematically from measures that are already inputs in the model. Before long the model is too complex, and in extreme cases there are more inputs than there are observations. The primary way to manage model complexity is to exercise caution in selecting model inputs. When large numbers of inputs are unavoidable, coefficient regularization methods such as LASSO regression can help with model parsimony.</p>
</div>
</div>
<div id="other-considerations-in-binomial-logistic-regression" class="section level2" number="5.4">
<h2>
<span class="header-section-number">5.4</span> Other considerations in binomial logistic regression<a class="anchor" aria-label="anchor" href="#other-considerations-in-binomial-logistic-regression"><i class="fas fa-link"></i></a>
</h2>
<p>To predict from new data, just use the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function as in the previous chapter. This function recognizes the type of model being used—in this case a generalized linear model—and adjusts its prediction approach accordingly. In particular, if you want to return the probability of the new observations being promoted, you need to use <code>type = "response"</code> as an argument.</p>
<div class="sourceCode" id="cb325"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># define new observations</span>
<span class="op">(</span><span class="va">new_data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>sales <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">420</span>, <span class="fl">510</span>, <span class="fl">710</span><span class="op">)</span>, 
                        customer_rate <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3.4</span>, <span class="fl">2.3</span>, <span class="fl">4.2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code>##   sales customer_rate
## 1   420           3.4
## 2   510           2.3
## 3   710           4.2</code></pre>
<div class="sourceCode" id="cb327"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># predict probability of promotion</span>
<span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">simpler_model</span>, <span class="va">new_data</span>, type <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span></code></pre></div>
<pre><code>##          1          2          3 
## 0.00171007 0.18238565 0.98840506</code></pre>
<p>Many of the principles covered in the previous chapter on linear regression are equally important in logistic regression. For example, input variables should be managed in a similar way. Collinearity and multicollinearity should be of concern. Interaction of input variables can be modeled. For the most part, analysts should be aware of the fundamental mathematical transformations which take place in a logistic regression model when they consider some of these issues (another reason to ensure that the mathematics covered earlier in this chapter is well understood). For example, while coefficients in linear regression have a direct additive impact on <span class="math inline">\(y\)</span>, in logistic regression they have a direct additive impact on the log odds of <span class="math inline">\(y\)</span>, or alternatively their exponents have a direct multiplicative impact on the odds of <span class="math inline">\(y\)</span>. Therefore coefficient overestimation such as that which can occur when collinearity is not managed can result in inferences that could substantially overstate the importance or effect of input variables.</p>
<p>Because of the binary nature of our outcome variable, the residuals of a logistic regression model have limited direct application to the problem being studied. In practical contexts the residuals of logistic regression models are rarely examined, but they can be useful in identifying outliers or particularly influential observations and in assessing goodness-of-fit. When residuals are examined, they need to be transformed in order to be analyzed appropriately. For example, the <em>Pearson residual</em> is a standardized form of residual from logistic regression which can be expected to have a normal distribution over large-enough samples. We can see in Figure <a href="bin-log-reg.html#fig:pearson-resids">5.9</a> that this is the case for our <code>simpler_model</code>, but that there are a small number of substantial underestimates in our model. A good source of further learning on diagnostics of logistic regression models is <span class="citation"><a href="references.html#ref-menard" role="doc-biblioref">Menard</a> (<a href="references.html#ref-menard" role="doc-biblioref">2010</a>)</span>.</p>
<div class="sourceCode" id="cb329"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">d</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/density.html">density</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html">residuals</a></span><span class="op">(</span><span class="va">simpler_model</span>, <span class="st">"pearson"</span><span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">d</span>, main<span class="op">=</span> <span class="st">""</span><span class="op">)</span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:pearson-resids"></span>
<img src="_main_files/figure-html/pearson-resids-1.png" alt="Distribution of Pearson residuals in `simpler_model`" width="672"><p class="caption">
Figure 5.9: Distribution of Pearson residuals in <code>simpler_model</code>
</p>
</div>
</div>
<div id="learning-exercises-3" class="section level2" number="5.5">
<h2>
<span class="header-section-number">5.5</span> Learning exercises<a class="anchor" aria-label="anchor" href="#learning-exercises-3"><i class="fas fa-link"></i></a>
</h2>
<div id="discussion-questions-3" class="section level3" number="5.5.1">
<h3>
<span class="header-section-number">5.5.1</span> Discussion questions<a class="anchor" aria-label="anchor" href="#discussion-questions-3"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li><p>Draw the shape of a logistic function. Describe the three population growth phases it was originally intended to model.</p></li>
<li><p>Explain why the logistic function is useful to statisticians in modeling.</p></li>
<li><p>In the formula for the logistic function in Section <a href="bin-log-reg.html#logistic-origins">5.1.1</a>, what might be a common value for <span class="math inline">\(L\)</span> in probabilistic applications? Why?</p></li>
<li><p>What types of problems are suitable for logistic regression modeling?</p></li>
<li><p>Can you think of some modeling scenarios in your work or studies that could use a logistic regression approach?</p></li>
<li><p>Explain the concept of odds. How do odds differ from probability? How do odds change as probability increases?</p></li>
<li><p>Complete the following:</p></li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>If an event has a 1% probability of occurring, a 10% increase in odds results in an almost __% increase in probability.</li>
<li>If an event has a 99% probability of occurring, a 10% increase in odds results in an almost __% increase in probability.</li>
</ol>
<ol start="8" style="list-style-type: decimal">
<li><p>Describe how the coefficients of a logistic regression model affect the fitted outcome. If <span class="math inline">\(\beta\)</span> is a coefficient estimate, how is the odds ratio associated with <span class="math inline">\(\beta\)</span> calculated and what does it mean?</p></li>
<li><p>What are some of the options for determining the fit of a binomial logistic regression model?</p></li>
<li><p>Describe the concept of model parsimony. What measure is commonly used to determine the most parsimonious logistic regression model?</p></li>
</ol>
</div>
<div id="data-exercises-3" class="section level3" number="5.5.2">
<h3>
<span class="header-section-number">5.5.2</span> Data exercises<a class="anchor" aria-label="anchor" href="#data-exercises-3"><i class="fas fa-link"></i></a>
</h3>
<p>A nature preservation charity has asked you to analyze some data to help them understand the features of those members of the public who donated in a given month. Load the <code>charity_donation</code> data set via the <code>peopleanalyticsdata</code> package or download it from the internet<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;&lt;a href="http://peopleanalytics-regression-book.org/data/charity_donation.csv" class="uri"&gt;http://peopleanalytics-regression-book.org/data/charity_donation.csv&lt;/a&gt;&lt;/p&gt;'><sup>28</sup></a>. It contains the following data:</p>
<ul>
<li>
<code>n_donations</code>: The total number of times the individual donated previous to the month being studied.</li>
<li>
<code>total_donations</code>: The total amount of money donated by the individual previous to the month being studied</li>
<li>
<code>time_donating</code>: The number of months between the first donation and the month being studied</li>
<li>
<code>recent_donation</code>: Whether or not the individual donated in the month being studied</li>
<li>
<code>last_donation</code>: The number of months between the most recent previous donation and the month being studied</li>
<li>
<code>gender</code>: The gender of the individual</li>
<li>
<code>reside</code>: Whether the person resides in an Urban or Rural Domestic location or Overseas</li>
<li>
<code>age</code>: The age of the individual</li>
</ul>
<ol style="list-style-type: decimal">
<li>View the data and obtain statistical summaries. Ensure data types are appropriate and there is no missing data. Determine the outcome and input variables.</li>
<li>Using a pairplot or by plotting or correlating selected fields, try to hypothesize which variables may be significant in explaining who recently donated.</li>
<li>Run a binomial logistic regression model using all input fields. Determine which input variables have a significant effect on the outcome and the direction of that effect.</li>
<li>Calculate the odds ratios for the significant variables and explain their impact on the outcome.</li>
<li>Check for collinearity or multicollinearity in your model using methods from previous chapters.</li>
<li>Experiment with model parsimony by reducing input variables that do not have a significant impact on the outcome. Decide on the most parsimonious model.</li>
<li>Calculate a variety of Pseudo-<span class="math inline">\(R^2\)</span> variants for your model. How would you explain these to someone with no statistics expertise?</li>
<li>Report the conclusions of your modeling exercise to the charity by writing a simple explanation that assumes no knowledge of statistics.</li>
<li>
<strong>Extension:</strong> Using a variety of methods of your choice, test the hypothesis that your model fits the data. How conclusive are your tests?</li>
</ol>
</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="linear-reg-ols.html"><span class="header-section-number">4</span> Linear Regression for Continuous Outcomes</a></div>
<div class="next"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html"><span class="header-section-number">6</span> Multinomial Logistic Regression for Nominal Category Outcomes</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#bin-log-reg"><span class="header-section-number">5</span> Binomial Logistic Regression for Binary Outcomes</a></li>
<li>
<a class="nav-link" href="#when-to-use-it"><span class="header-section-number">5.1</span> When to use it</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#logistic-origins"><span class="header-section-number">5.1.1</span> Origins and intuition of binomial logistic regression</a></li>
<li><a class="nav-link" href="#use-cases-for-binomial-logistic-regression"><span class="header-section-number">5.1.2</span> Use cases for binomial logistic regression</a></li>
<li><a class="nav-link" href="#walkthrough-logit"><span class="header-section-number">5.1.3</span> Walkthrough example</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#mod-prob"><span class="header-section-number">5.2</span> Modeling probabilistic outcomes using a logistic function</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#deriving-the-concept-of-log-odds"><span class="header-section-number">5.2.1</span> Deriving the concept of log odds</a></li>
<li><a class="nav-link" href="#modeling-the-log-odds-and-interpreting-the-coefficients"><span class="header-section-number">5.2.2</span> Modeling the log odds and interpreting the coefficients</a></li>
<li><a class="nav-link" href="#odds-versus-probability"><span class="header-section-number">5.2.3</span> Odds versus probability</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#running-a-multivariate-binomial-logistic-regression-model"><span class="header-section-number">5.3</span> Running a multivariate binomial logistic regression model</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#running-and-interpreting-a-multivariate-binomial-logistic-regression-model"><span class="header-section-number">5.3.1</span> Running and interpreting a multivariate binomial logistic regression model</a></li>
<li><a class="nav-link" href="#logistic-gof"><span class="header-section-number">5.3.2</span> Understanding the fit and goodness-of-fit of a binomial logistic regression model</a></li>
<li><a class="nav-link" href="#model-parsimony"><span class="header-section-number">5.3.3</span> Model parsimony</a></li>
</ul>
</li>
<li><a class="nav-link" href="#other-considerations-in-binomial-logistic-regression"><span class="header-section-number">5.4</span> Other considerations in binomial logistic regression</a></li>
<li>
<a class="nav-link" href="#learning-exercises-3"><span class="header-section-number">5.5</span> Learning exercises</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#discussion-questions-3"><span class="header-section-number">5.5.1</span> Discussion questions</a></li>
<li><a class="nav-link" href="#data-exercises-3"><span class="header-section-number">5.5.2</span> Data exercises</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/keithmcnulty/peopleanalytics-regression-book/blob/master/r/05-binomial_logistic_regression.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/keithmcnulty/peopleanalytics-regression-book/edit/master/r/05-binomial_logistic_regression.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Handbook of Regression Modeling in People Analytics</strong>: With Examples in R, Python and Julia" was written by Keith McNulty. </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
