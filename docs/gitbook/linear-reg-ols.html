<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Linear Regression for Continuous Outcomes | Handbook of Regression Modeling in People Analytics: With Examples in R, Python and Julia</title>
  <meta name="description" content="A technical manual of inferential statistics and regression modeling in the people and social sciences" />
  <meta name="generator" content="bookdown 0.23 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Linear Regression for Continuous Outcomes | Handbook of Regression Modeling in People Analytics: With Examples in R, Python and Julia" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://peopleanalytics-regression-book.org" />
  <meta property="og:image" content="https://peopleanalytics-regression-book.org/www/cover/coverpage-og.png" />
  <meta property="og:description" content="A technical manual of inferential statistics and regression modeling in the people and social sciences" />
  <meta name="github-repo" content="keithmcnulty/peopleanalytics-regression-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Linear Regression for Continuous Outcomes | Handbook of Regression Modeling in People Analytics: With Examples in R, Python and Julia" />
  <meta name="twitter:site" content="@dr_keithmcnulty" />
  <meta name="twitter:description" content="A technical manual of inferential statistics and regression modeling in the people and social sciences" />
  <meta name="twitter:image" content="https://peopleanalytics-regression-book.org/www/cover/coverpage-og.png" />

<meta name="author" content="Keith McNulty" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="found-stats.html"/>
<link rel="next" href="bin-log-reg.html"/>
<script src="libs/header-attrs-2.9.5/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<meta name="citation_title" content="Handbook of Regression Modeling in People Analytics: With Examples in R and Python"/>
<meta name="citation_author" content="Keith McNulty"/>
<meta name="citation_publication_date" content="2021"/>
<meta name="citation_isbn" content="9781003194156"/>
<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.2.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.52.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.52.2/plotly-latest.min.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-N7JZGMVRZK"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-N7JZGMVRZK');
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Handbook of Regression Modeling in People Analytics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#notes-on-data-used-in-this-book"><i class="fa fa-check"></i>Notes on data used in this book</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="foreword-by-alexis-fink.html"><a href="foreword-by-alexis-fink.html"><i class="fa fa-check"></i>Foreword by Alexis Fink</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="inf-model.html"><a href="inf-model.html"><i class="fa fa-check"></i><b>1</b> The Importance of Regression in People Analytics</a>
<ul>
<li class="chapter" data-level="1.1" data-path="inf-model.html"><a href="inf-model.html#why-is-regression-modeling-so-important-in-people-analytics"><i class="fa fa-check"></i><b>1.1</b> Why is regression modeling so important in people analytics?</a></li>
<li class="chapter" data-level="1.2" data-path="inf-model.html"><a href="inf-model.html#theory-modeling"><i class="fa fa-check"></i><b>1.2</b> What do we mean by ‘modeling’‍?</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="inf-model.html"><a href="inf-model.html#the-theory-of-inferential-modeling"><i class="fa fa-check"></i><b>1.2.1</b> The theory of inferential modeling</a></li>
<li class="chapter" data-level="1.2.2" data-path="inf-model.html"><a href="inf-model.html#the-process-of-inferential-modeling"><i class="fa fa-check"></i><b>1.2.2</b> The process of inferential modeling</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="inf-model.html"><a href="inf-model.html#the-structure-system-and-organization-of-this-book"><i class="fa fa-check"></i><b>1.3</b> The structure, system and organization of this book</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html"><i class="fa fa-check"></i><b>2</b> The Basics of the R Programming Language</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#what-is-r"><i class="fa fa-check"></i><b>2.1</b> What is R?</a></li>
<li class="chapter" data-level="2.2" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#how-to-start-using-r"><i class="fa fa-check"></i><b>2.2</b> How to start using R</a></li>
<li class="chapter" data-level="2.3" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#data-in-r"><i class="fa fa-check"></i><b>2.3</b> Data in R</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#data-types"><i class="fa fa-check"></i><b>2.3.1</b> Data types</a></li>
<li class="chapter" data-level="2.3.2" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#homogeneous-data-structures"><i class="fa fa-check"></i><b>2.3.2</b> Homogeneous data structures</a></li>
<li class="chapter" data-level="2.3.3" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#heterogeneous-data-structures"><i class="fa fa-check"></i><b>2.3.3</b> Heterogeneous data structures</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#working-with-dataframes"><i class="fa fa-check"></i><b>2.4</b> Working with dataframes</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#loading-and-tidying-data-in-dataframes"><i class="fa fa-check"></i><b>2.4.1</b> Loading and tidying data in dataframes</a></li>
<li class="chapter" data-level="2.4.2" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#manipulating-dataframes"><i class="fa fa-check"></i><b>2.4.2</b> Manipulating dataframes</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#functions-packages-and-libraries"><i class="fa fa-check"></i><b>2.5</b> Functions, packages and libraries</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#using-functions"><i class="fa fa-check"></i><b>2.5.1</b> Using functions</a></li>
<li class="chapter" data-level="2.5.2" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#help-with-functions"><i class="fa fa-check"></i><b>2.5.2</b> Help with functions</a></li>
<li class="chapter" data-level="2.5.3" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#writing-your-own-functions"><i class="fa fa-check"></i><b>2.5.3</b> Writing your own functions</a></li>
<li class="chapter" data-level="2.5.4" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#installing-packages"><i class="fa fa-check"></i><b>2.5.4</b> Installing packages</a></li>
<li class="chapter" data-level="2.5.5" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#using-packages"><i class="fa fa-check"></i><b>2.5.5</b> Using packages</a></li>
<li class="chapter" data-level="2.5.6" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#the-pipe-operator"><i class="fa fa-check"></i><b>2.5.6</b> The pipe operator</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#errors-warnings-and-messages"><i class="fa fa-check"></i><b>2.6</b> Errors, warnings and messages</a></li>
<li class="chapter" data-level="2.7" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#plotting-and-graphing"><i class="fa fa-check"></i><b>2.7</b> Plotting and graphing</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#plotting-in-base-r"><i class="fa fa-check"></i><b>2.7.1</b> Plotting in base R</a></li>
<li class="chapter" data-level="2.7.2" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#specialist-plotting-and-graphing-packages"><i class="fa fa-check"></i><b>2.7.2</b> Specialist plotting and graphing packages</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#documenting-your-work-using-r-markdown"><i class="fa fa-check"></i><b>2.8</b> Documenting your work using R Markdown</a></li>
<li class="chapter" data-level="2.9" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#learning-exercises"><i class="fa fa-check"></i><b>2.9</b> Learning exercises</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#discussion-questions"><i class="fa fa-check"></i><b>2.9.1</b> Discussion questions</a></li>
<li class="chapter" data-level="2.9.2" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#data-exercises"><i class="fa fa-check"></i><b>2.9.2</b> Data exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="found-stats.html"><a href="found-stats.html"><i class="fa fa-check"></i><b>3</b> Statistics Foundations</a>
<ul>
<li class="chapter" data-level="3.1" data-path="found-stats.html"><a href="found-stats.html#elementary-descriptive-statistics-of-populations-and-samples"><i class="fa fa-check"></i><b>3.1</b> Elementary descriptive statistics of populations and samples</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="found-stats.html"><a href="found-stats.html#mean-var-sd"><i class="fa fa-check"></i><b>3.1.1</b> Mean, variance and standard deviation</a></li>
<li class="chapter" data-level="3.1.2" data-path="found-stats.html"><a href="found-stats.html#covariance-and-correlation"><i class="fa fa-check"></i><b>3.1.2</b> Covariance and correlation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="found-stats.html"><a href="found-stats.html#distribution-of-random-variables"><i class="fa fa-check"></i><b>3.2</b> Distribution of random variables</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="found-stats.html"><a href="found-stats.html#sampling-of-random-variables"><i class="fa fa-check"></i><b>3.2.1</b> Sampling of random variables</a></li>
<li class="chapter" data-level="3.2.2" data-path="found-stats.html"><a href="found-stats.html#standard-errors-the-t-distribution-and-confidence-intervals"><i class="fa fa-check"></i><b>3.2.2</b> Standard errors, the <span class="math inline">\(t\)</span>-distribution and confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="found-stats.html"><a href="found-stats.html#hyp-tests"><i class="fa fa-check"></i><b>3.3</b> Hypothesis testing</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="found-stats.html"><a href="found-stats.html#means-sig"><i class="fa fa-check"></i><b>3.3.1</b> Testing for a difference in means (Welch’s <span class="math inline">\(t\)</span>-test)</a></li>
<li class="chapter" data-level="3.3.2" data-path="found-stats.html"><a href="found-stats.html#t-test-cor"><i class="fa fa-check"></i><b>3.3.2</b> Testing for a non-zero correlation between two variables (<span class="math inline">\(t\)</span>-test for correlation)</a></li>
<li class="chapter" data-level="3.3.3" data-path="found-stats.html"><a href="found-stats.html#testing-for-a-difference-in-frequency-distribution-between-different-categories-in-a-data-set-chi-square-test"><i class="fa fa-check"></i><b>3.3.3</b> Testing for a difference in frequency distribution between different categories in a data set (Chi-square test)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="found-stats.html"><a href="found-stats.html#foundational-statistics-in-python"><i class="fa fa-check"></i><b>3.4</b> Foundational statistics in Python</a></li>
<li class="chapter" data-level="3.5" data-path="found-stats.html"><a href="found-stats.html#foundational-statistics-in-julia"><i class="fa fa-check"></i><b>3.5</b> Foundational statistics in Julia</a></li>
<li class="chapter" data-level="3.6" data-path="found-stats.html"><a href="found-stats.html#learning-exercises-1"><i class="fa fa-check"></i><b>3.6</b> Learning exercises</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="found-stats.html"><a href="found-stats.html#discussion-questions-1"><i class="fa fa-check"></i><b>3.6.1</b> Discussion questions</a></li>
<li class="chapter" data-level="3.6.2" data-path="found-stats.html"><a href="found-stats.html#data-exercises-1"><i class="fa fa-check"></i><b>3.6.2</b> Data exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html"><i class="fa fa-check"></i><b>4</b> Linear Regression for Continuous Outcomes</a>
<ul>
<li class="chapter" data-level="4.1" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#when-ols"><i class="fa fa-check"></i><b>4.1</b> When to use it</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#origins-ols"><i class="fa fa-check"></i><b>4.1.1</b> Origins and intuition of linear regression</a></li>
<li class="chapter" data-level="4.1.2" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#use-cases-ols"><i class="fa fa-check"></i><b>4.1.2</b> Use cases for linear regression</a></li>
<li class="chapter" data-level="4.1.3" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#walkthrough-ols"><i class="fa fa-check"></i><b>4.1.3</b> Walkthrough example</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#simple-ols"><i class="fa fa-check"></i><b>4.2</b> Simple linear regression</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#linear-single"><i class="fa fa-check"></i><b>4.2.1</b> Linear relationship between a single input and an outcome</a></li>
<li class="chapter" data-level="4.2.2" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#minimising-error-ols"><i class="fa fa-check"></i><b>4.2.2</b> Minimising the error</a></li>
<li class="chapter" data-level="4.2.3" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#best-fit-simple-ols"><i class="fa fa-check"></i><b>4.2.3</b> Determining the best fit</a></li>
<li class="chapter" data-level="4.2.4" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#measuring-the-fit-of-the-model"><i class="fa fa-check"></i><b>4.2.4</b> Measuring the fit of the model</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#multiple-linear-regression"><i class="fa fa-check"></i><b>4.3</b> Multiple linear regression</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#running-a-multiple-linear-regression-model-and-interpreting-its-coefficients"><i class="fa fa-check"></i><b>4.3.1</b> Running a multiple linear regression model and interpreting its coefficients</a></li>
<li class="chapter" data-level="4.3.2" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#coefficient-confidence"><i class="fa fa-check"></i><b>4.3.2</b> Coefficient confidence</a></li>
<li class="chapter" data-level="4.3.3" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#lin-good-fit"><i class="fa fa-check"></i><b>4.3.3</b> Model ‘goodness-of-fit’</a></li>
<li class="chapter" data-level="4.3.4" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#making-predictions-from-your-model"><i class="fa fa-check"></i><b>4.3.4</b> Making predictions from your model</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#managing-inputs-in-linear-regression"><i class="fa fa-check"></i><b>4.4</b> Managing inputs in linear regression</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#relevance-of-input-variables"><i class="fa fa-check"></i><b>4.4.1</b> Relevance of input variables</a></li>
<li class="chapter" data-level="4.4.2" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#sparseness-missingness-of-data"><i class="fa fa-check"></i><b>4.4.2</b> Sparseness (‘missingness’) of data</a></li>
<li class="chapter" data-level="4.4.3" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#transforming-categorical-inputs-to-dummy-variables"><i class="fa fa-check"></i><b>4.4.3</b> Transforming categorical inputs to dummy variables</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#testing-your-model-assumptions"><i class="fa fa-check"></i><b>4.5</b> Testing your model assumptions</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#assumption-of-linearity-and-additivity"><i class="fa fa-check"></i><b>4.5.1</b> Assumption of linearity and additivity</a></li>
<li class="chapter" data-level="4.5.2" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#lin-reg-const-var"><i class="fa fa-check"></i><b>4.5.2</b> Assumption of constant error variance</a></li>
<li class="chapter" data-level="4.5.3" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#norm-dist-assum"><i class="fa fa-check"></i><b>4.5.3</b> Assumption of normally distributed errors</a></li>
<li class="chapter" data-level="4.5.4" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#collinearity"><i class="fa fa-check"></i><b>4.5.4</b> Avoiding high collinearity and multicollinearity between input variables</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#extending-multiple-linear-regression"><i class="fa fa-check"></i><b>4.6</b> Extending multiple linear regression</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#interactions-between-input-variables"><i class="fa fa-check"></i><b>4.6.1</b> Interactions between input variables</a></li>
<li class="chapter" data-level="4.6.2" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#quadratic-and-higher-order-polynomial-terms"><i class="fa fa-check"></i><b>4.6.2</b> Quadratic and higher-order polynomial terms</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#learning-exercises-2"><i class="fa fa-check"></i><b>4.7</b> Learning exercises</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#discussion-questions-2"><i class="fa fa-check"></i><b>4.7.1</b> Discussion questions</a></li>
<li class="chapter" data-level="4.7.2" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#data-exercises-2"><i class="fa fa-check"></i><b>4.7.2</b> Data exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="bin-log-reg.html"><a href="bin-log-reg.html"><i class="fa fa-check"></i><b>5</b> Binomial Logistic Regression for Binary Outcomes</a>
<ul>
<li class="chapter" data-level="5.1" data-path="bin-log-reg.html"><a href="bin-log-reg.html#when-to-use-it"><i class="fa fa-check"></i><b>5.1</b> When to use it</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="bin-log-reg.html"><a href="bin-log-reg.html#logistic-origins"><i class="fa fa-check"></i><b>5.1.1</b> Origins and intuition of binomial logistic regression</a></li>
<li class="chapter" data-level="5.1.2" data-path="bin-log-reg.html"><a href="bin-log-reg.html#use-cases-for-binomial-logistic-regression"><i class="fa fa-check"></i><b>5.1.2</b> Use cases for binomial logistic regression</a></li>
<li class="chapter" data-level="5.1.3" data-path="bin-log-reg.html"><a href="bin-log-reg.html#walkthrough-logit"><i class="fa fa-check"></i><b>5.1.3</b> Walkthrough example</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="bin-log-reg.html"><a href="bin-log-reg.html#mod-prob"><i class="fa fa-check"></i><b>5.2</b> Modeling probabilistic outcomes using a logistic function</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="bin-log-reg.html"><a href="bin-log-reg.html#deriving-the-concept-of-log-odds"><i class="fa fa-check"></i><b>5.2.1</b> Deriving the concept of log odds</a></li>
<li class="chapter" data-level="5.2.2" data-path="bin-log-reg.html"><a href="bin-log-reg.html#modeling-the-log-odds-and-interpreting-the-coefficients"><i class="fa fa-check"></i><b>5.2.2</b> Modeling the log odds and interpreting the coefficients</a></li>
<li class="chapter" data-level="5.2.3" data-path="bin-log-reg.html"><a href="bin-log-reg.html#odds-versus-probability"><i class="fa fa-check"></i><b>5.2.3</b> Odds versus probability</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="bin-log-reg.html"><a href="bin-log-reg.html#running-a-multivariate-binomial-logistic-regression-model"><i class="fa fa-check"></i><b>5.3</b> Running a multivariate binomial logistic regression model</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="bin-log-reg.html"><a href="bin-log-reg.html#running-and-interpreting-a-multivariate-binomial-logistic-regression-model"><i class="fa fa-check"></i><b>5.3.1</b> Running and interpreting a multivariate binomial logistic regression model</a></li>
<li class="chapter" data-level="5.3.2" data-path="bin-log-reg.html"><a href="bin-log-reg.html#logistic-gof"><i class="fa fa-check"></i><b>5.3.2</b> Understanding the fit and goodness-of-fit of a binomial logistic regression model</a></li>
<li class="chapter" data-level="5.3.3" data-path="bin-log-reg.html"><a href="bin-log-reg.html#model-parsimony"><i class="fa fa-check"></i><b>5.3.3</b> Model parsimony</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="bin-log-reg.html"><a href="bin-log-reg.html#other-considerations-in-binomial-logistic-regression"><i class="fa fa-check"></i><b>5.4</b> Other considerations in binomial logistic regression</a></li>
<li class="chapter" data-level="5.5" data-path="bin-log-reg.html"><a href="bin-log-reg.html#learning-exercises-3"><i class="fa fa-check"></i><b>5.5</b> Learning exercises</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="bin-log-reg.html"><a href="bin-log-reg.html#discussion-questions-3"><i class="fa fa-check"></i><b>5.5.1</b> Discussion questions</a></li>
<li class="chapter" data-level="5.5.2" data-path="bin-log-reg.html"><a href="bin-log-reg.html#data-exercises-3"><i class="fa fa-check"></i><b>5.5.2</b> Data exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html"><i class="fa fa-check"></i><b>6</b> Multinomial Logistic Regression for Nominal Category Outcomes</a>
<ul>
<li class="chapter" data-level="6.1" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#when-to-use-it-1"><i class="fa fa-check"></i><b>6.1</b> When to use it</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#intuition-for-multinomial-logistic-regression"><i class="fa fa-check"></i><b>6.1.1</b> Intuition for multinomial logistic regression</a></li>
<li class="chapter" data-level="6.1.2" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#use-cases-for-multinomial-logistic-regression"><i class="fa fa-check"></i><b>6.1.2</b> Use cases for multinomial logistic regression</a></li>
<li class="chapter" data-level="6.1.3" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#walkthrough-example"><i class="fa fa-check"></i><b>6.1.3</b> Walkthrough example</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#stratified"><i class="fa fa-check"></i><b>6.2</b> Running stratified binomial models</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#modeling-the-choice-of-product-a-versus-other-products"><i class="fa fa-check"></i><b>6.2.1</b> Modeling the choice of Product A versus other products</a></li>
<li class="chapter" data-level="6.2.2" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#modeling-other-choices"><i class="fa fa-check"></i><b>6.2.2</b> Modeling other choices</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#running-a-multinomial-regression-model"><i class="fa fa-check"></i><b>6.3</b> Running a multinomial regression model</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#def-ref"><i class="fa fa-check"></i><b>6.3.1</b> Defining a reference level and running the model</a></li>
<li class="chapter" data-level="6.3.2" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#interpreting-the-model"><i class="fa fa-check"></i><b>6.3.2</b> Interpreting the model</a></li>
<li class="chapter" data-level="6.3.3" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#changing-ref"><i class="fa fa-check"></i><b>6.3.3</b> Changing the reference</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#model-simplification-fit-and-goodness-of-fit-for-multinomial-logistic-regression-models"><i class="fa fa-check"></i><b>6.4</b> Model simplification, fit and goodness-of-fit for multinomial logistic regression models</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#elim"><i class="fa fa-check"></i><b>6.4.1</b> Gradual safe elimination of variables</a></li>
<li class="chapter" data-level="6.4.2" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#model-fit-and-goodness-of-fit"><i class="fa fa-check"></i><b>6.4.2</b> Model fit and goodness-of-fit</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#learning-exercises-4"><i class="fa fa-check"></i><b>6.5</b> Learning exercises</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#discussion-questions-4"><i class="fa fa-check"></i><b>6.5.1</b> Discussion questions</a></li>
<li class="chapter" data-level="6.5.2" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#data-exercises-4"><i class="fa fa-check"></i><b>6.5.2</b> Data exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ord-reg.html"><a href="ord-reg.html"><i class="fa fa-check"></i><b>7</b> Proportional Odds Logistic Regression for Ordered Category Outcomes</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ord-reg.html"><a href="ord-reg.html#when-to-use-it-2"><i class="fa fa-check"></i><b>7.1</b> When to use it</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="ord-reg.html"><a href="ord-reg.html#ord-intuit"><i class="fa fa-check"></i><b>7.1.1</b> Intuition for proportional odds logistic regression</a></li>
<li class="chapter" data-level="7.1.2" data-path="ord-reg.html"><a href="ord-reg.html#use-cases-for-proportional-odds-logistic-regression"><i class="fa fa-check"></i><b>7.1.2</b> Use cases for proportional odds logistic regression</a></li>
<li class="chapter" data-level="7.1.3" data-path="ord-reg.html"><a href="ord-reg.html#ord-walkthrough"><i class="fa fa-check"></i><b>7.1.3</b> Walkthrough example</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="ord-reg.html"><a href="ord-reg.html#modeling-ordinal-outcomes-under-the-assumption-of-proportional-odds"><i class="fa fa-check"></i><b>7.2</b> Modeling ordinal outcomes under the assumption of proportional odds</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="ord-reg.html"><a href="ord-reg.html#mod-lin-reg"><i class="fa fa-check"></i><b>7.2.1</b> Using a latent continuous outcome variable to derive a proportional odds model</a></li>
<li class="chapter" data-level="7.2.2" data-path="ord-reg.html"><a href="ord-reg.html#running-a-proportional-odds-logistic-regression-model"><i class="fa fa-check"></i><b>7.2.2</b> Running a proportional odds logistic regression model</a></li>
<li class="chapter" data-level="7.2.3" data-path="ord-reg.html"><a href="ord-reg.html#calculating-the-likelihood-of-an-observation-being-in-a-specific-ordinal-category"><i class="fa fa-check"></i><b>7.2.3</b> Calculating the likelihood of an observation being in a specific ordinal category</a></li>
<li class="chapter" data-level="7.2.4" data-path="ord-reg.html"><a href="ord-reg.html#model-diagnostics"><i class="fa fa-check"></i><b>7.2.4</b> Model diagnostics</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ord-reg.html"><a href="ord-reg.html#testing-the-proportional-odds-assumption"><i class="fa fa-check"></i><b>7.3</b> Testing the proportional odds assumption</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="ord-reg.html"><a href="ord-reg.html#sighting-the-coefficients-of-stratified-binomial-models"><i class="fa fa-check"></i><b>7.3.1</b> Sighting the coefficients of stratified binomial models</a></li>
<li class="chapter" data-level="7.3.2" data-path="ord-reg.html"><a href="ord-reg.html#wald"><i class="fa fa-check"></i><b>7.3.2</b> The Brant-Wald test</a></li>
<li class="chapter" data-level="7.3.3" data-path="ord-reg.html"><a href="ord-reg.html#alternatives-to-proportional-odds-models"><i class="fa fa-check"></i><b>7.3.3</b> Alternatives to proportional odds models</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="ord-reg.html"><a href="ord-reg.html#learning-exercises-5"><i class="fa fa-check"></i><b>7.4</b> Learning exercises</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="ord-reg.html"><a href="ord-reg.html#discussion-questions-5"><i class="fa fa-check"></i><b>7.4.1</b> Discussion questions</a></li>
<li class="chapter" data-level="7.4.2" data-path="ord-reg.html"><a href="ord-reg.html#data-exercises-5"><i class="fa fa-check"></i><b>7.4.2</b> Data exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="modeling-explicit-and-latent-hierarchy-in-data.html"><a href="modeling-explicit-and-latent-hierarchy-in-data.html"><i class="fa fa-check"></i><b>8</b> Modeling Explicit and Latent Hierarchy in Data</a>
<ul>
<li class="chapter" data-level="8.1" data-path="modeling-explicit-and-latent-hierarchy-in-data.html"><a href="modeling-explicit-and-latent-hierarchy-in-data.html#mixed"><i class="fa fa-check"></i><b>8.1</b> Mixed models for explicit hierarchy in data</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="modeling-explicit-and-latent-hierarchy-in-data.html"><a href="modeling-explicit-and-latent-hierarchy-in-data.html#fixed-and-random-effects"><i class="fa fa-check"></i><b>8.1.1</b> Fixed and random effects</a></li>
<li class="chapter" data-level="8.1.2" data-path="modeling-explicit-and-latent-hierarchy-in-data.html"><a href="modeling-explicit-and-latent-hierarchy-in-data.html#running-a-mixed-model"><i class="fa fa-check"></i><b>8.1.2</b> Running a mixed model</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="modeling-explicit-and-latent-hierarchy-in-data.html"><a href="modeling-explicit-and-latent-hierarchy-in-data.html#struc-eq-model"><i class="fa fa-check"></i><b>8.2</b> Structural equation models for latent hierarchy in data</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="modeling-explicit-and-latent-hierarchy-in-data.html"><a href="modeling-explicit-and-latent-hierarchy-in-data.html#running-and-assessing-the-measurement-model"><i class="fa fa-check"></i><b>8.2.1</b> Running and assessing the measurement model</a></li>
<li class="chapter" data-level="8.2.2" data-path="modeling-explicit-and-latent-hierarchy-in-data.html"><a href="modeling-explicit-and-latent-hierarchy-in-data.html#running-and-interpreting-the-structural-model"><i class="fa fa-check"></i><b>8.2.2</b> Running and interpreting the structural model</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="modeling-explicit-and-latent-hierarchy-in-data.html"><a href="modeling-explicit-and-latent-hierarchy-in-data.html#learning-exercises-6"><i class="fa fa-check"></i><b>8.3</b> Learning exercises</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="modeling-explicit-and-latent-hierarchy-in-data.html"><a href="modeling-explicit-and-latent-hierarchy-in-data.html#discussion-questions-6"><i class="fa fa-check"></i><b>8.3.1</b> Discussion questions</a></li>
<li class="chapter" data-level="8.3.2" data-path="modeling-explicit-and-latent-hierarchy-in-data.html"><a href="modeling-explicit-and-latent-hierarchy-in-data.html#data-exercises-6"><i class="fa fa-check"></i><b>8.3.2</b> Data exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="survival.html"><a href="survival.html"><i class="fa fa-check"></i><b>9</b> Survival Analysis for Modeling Singular Events Over Time</a>
<ul>
<li class="chapter" data-level="9.1" data-path="survival.html"><a href="survival.html#tracking-and-illustrating-survival-rates-over-the-study-period"><i class="fa fa-check"></i><b>9.1</b> Tracking and illustrating survival rates over the study period</a></li>
<li class="chapter" data-level="9.2" data-path="survival.html"><a href="survival.html#coxphmodel"><i class="fa fa-check"></i><b>9.2</b> Cox proportional hazard regression models</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="survival.html"><a href="survival.html#running-a-cox-proportional-hazard-regression-model"><i class="fa fa-check"></i><b>9.2.1</b> Running a Cox proportional hazard regression model</a></li>
<li class="chapter" data-level="9.2.2" data-path="survival.html"><a href="survival.html#checking-the-proportional-hazard-assumption"><i class="fa fa-check"></i><b>9.2.2</b> Checking the proportional hazard assumption</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="survival.html"><a href="survival.html#frailty-models"><i class="fa fa-check"></i><b>9.3</b> Frailty models</a></li>
<li class="chapter" data-level="9.4" data-path="survival.html"><a href="survival.html#learning-exercises-7"><i class="fa fa-check"></i><b>9.4</b> Learning exercises</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="survival.html"><a href="survival.html#discussion-questions-7"><i class="fa fa-check"></i><b>9.4.1</b> Discussion questions</a></li>
<li class="chapter" data-level="9.4.2" data-path="survival.html"><a href="survival.html#data-exercises-7"><i class="fa fa-check"></i><b>9.4.2</b> Data exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="alt-approaches.html"><a href="alt-approaches.html"><i class="fa fa-check"></i><b>10</b> Alternative Technical Approaches in R, Python and Julia</a>
<ul>
<li class="chapter" data-level="10.1" data-path="alt-approaches.html"><a href="alt-approaches.html#tidier-modeling-approaches-in-r"><i class="fa fa-check"></i><b>10.1</b> ‘Tidier’ modeling approaches in R</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="alt-approaches.html"><a href="alt-approaches.html#the-broom-package"><i class="fa fa-check"></i><b>10.1.1</b> The <code>broom</code> package</a></li>
<li class="chapter" data-level="10.1.2" data-path="alt-approaches.html"><a href="alt-approaches.html#the-parsnip-package"><i class="fa fa-check"></i><b>10.1.2</b> The <code>parsnip</code> package</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="alt-approaches.html"><a href="alt-approaches.html#inferential-statistical-modeling-in-python"><i class="fa fa-check"></i><b>10.2</b> Inferential statistical modeling in Python</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="alt-approaches.html"><a href="alt-approaches.html#ordinary-least-squares-ols-linear-regression"><i class="fa fa-check"></i><b>10.2.1</b> Ordinary Least Squares (OLS) linear regression</a></li>
<li class="chapter" data-level="10.2.2" data-path="alt-approaches.html"><a href="alt-approaches.html#binomial-logistic-regression"><i class="fa fa-check"></i><b>10.2.2</b> Binomial logistic regression</a></li>
<li class="chapter" data-level="10.2.3" data-path="alt-approaches.html"><a href="alt-approaches.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>10.2.3</b> Multinomial logistic regression</a></li>
<li class="chapter" data-level="10.2.4" data-path="alt-approaches.html"><a href="alt-approaches.html#structural-equation-models"><i class="fa fa-check"></i><b>10.2.4</b> Structural equation models</a></li>
<li class="chapter" data-level="10.2.5" data-path="alt-approaches.html"><a href="alt-approaches.html#survival-analysis"><i class="fa fa-check"></i><b>10.2.5</b> Survival analysis</a></li>
<li class="chapter" data-level="10.2.6" data-path="alt-approaches.html"><a href="alt-approaches.html#other-model-variants"><i class="fa fa-check"></i><b>10.2.6</b> Other model variants</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="alt-approaches.html"><a href="alt-approaches.html#inferential-statistical-modeling-in-julia"><i class="fa fa-check"></i><b>10.3</b> Inferential statistical modeling in Julia</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="alt-approaches.html"><a href="alt-approaches.html#ordinary-least-squares-ols-linear-regression-1"><i class="fa fa-check"></i><b>10.3.1</b> Ordinary Least Squares (OLS) linear regression</a></li>
<li class="chapter" data-level="10.3.2" data-path="alt-approaches.html"><a href="alt-approaches.html#binomial-logistic-regression-1"><i class="fa fa-check"></i><b>10.3.2</b> Binomial logistic regression</a></li>
<li class="chapter" data-level="10.3.3" data-path="alt-approaches.html"><a href="alt-approaches.html#multinomial-logistic-regression-1"><i class="fa fa-check"></i><b>10.3.3</b> Multinomial logistic regression</a></li>
<li class="chapter" data-level="10.3.4" data-path="alt-approaches.html"><a href="alt-approaches.html#proportional-odds-logistic-regression"><i class="fa fa-check"></i><b>10.3.4</b> Proportional odds logistic regression</a></li>
<li class="chapter" data-level="10.3.5" data-path="alt-approaches.html"><a href="alt-approaches.html#mixed-models"><i class="fa fa-check"></i><b>10.3.5</b> Mixed models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="power-tests.html"><a href="power-tests.html"><i class="fa fa-check"></i><b>11</b> Power Analysis to Estimate Required Sample Sizes for Modeling</a>
<ul>
<li class="chapter" data-level="11.1" data-path="power-tests.html"><a href="power-tests.html#errors-effect-sizes-and-statistical-power"><i class="fa fa-check"></i><b>11.1</b> Errors, effect sizes and statistical power</a></li>
<li class="chapter" data-level="11.2" data-path="power-tests.html"><a href="power-tests.html#simple-stats"><i class="fa fa-check"></i><b>11.2</b> Power analysis for simple hypothesis tests</a></li>
<li class="chapter" data-level="11.3" data-path="power-tests.html"><a href="power-tests.html#power-analysis-for-linear-regression-models"><i class="fa fa-check"></i><b>11.3</b> Power analysis for linear regression models</a></li>
<li class="chapter" data-level="11.4" data-path="power-tests.html"><a href="power-tests.html#power-analysis-for-log-likelihood-regression-models"><i class="fa fa-check"></i><b>11.4</b> Power analysis for log-likelihood regression models</a></li>
<li class="chapter" data-level="11.5" data-path="power-tests.html"><a href="power-tests.html#power-analysis-for-hierarchical-regression-models"><i class="fa fa-check"></i><b>11.5</b> Power analysis for hierarchical regression models</a></li>
<li class="chapter" data-level="11.6" data-path="power-tests.html"><a href="power-tests.html#power-analysis-using-python"><i class="fa fa-check"></i><b>11.6</b> Power analysis using Python</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="solutions-to-exercises-slide-presentations-videos-and-other-learning-resources.html"><a href="solutions-to-exercises-slide-presentations-videos-and-other-learning-resources.html"><i class="fa fa-check"></i>Solutions to Exercises, Slide Presentations, Videos and Other Learning Resources</a>
<ul>
<li class="chapter" data-level="" data-path="solutions-to-exercises-slide-presentations-videos-and-other-learning-resources.html"><a href="solutions-to-exercises-slide-presentations-videos-and-other-learning-resources.html#solutions-to-exercises"><i class="fa fa-check"></i>Solutions to exercises</a></li>
<li class="chapter" data-level="" data-path="solutions-to-exercises-slide-presentations-videos-and-other-learning-resources.html"><a href="solutions-to-exercises-slide-presentations-videos-and-other-learning-resources.html#learning-resources"><i class="fa fa-check"></i>Learning resources</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Handbook of Regression Modeling in People Analytics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-reg-ols" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Linear Regression for Continuous Outcomes</h1>
<p>In this chapter, we will introduce and explore linear regression, one of the first learning methods to be developed by statisticians and one of the easiest to interpret. Despite its simplicity—indeed <em>because</em> of its simplicity—it can be a very powerful tool in many situations. Linear regression will often be the first methodology to be trialed on a given problem, and will give an immediate benchmark with which to judge the efficacy of other, more complex, modeling techniques. Given the ease of interpretation, many analysts will select a linear regression model over more complex approaches even if those approaches produce a slightly better fit. This chapter will also introduce many critical concepts that will apply to other modeling approaches as we proceed through this book. Therefore for inexperienced modelers this should be considered a foundational chapter which should not be skipped.</p>
<div id="when-ols" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> When to use it</h2>
<div id="origins-ols" class="section level3" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Origins and intuition of linear regression</h3>
<p>Linear regression, also known as <em>Ordinary Least Squares linear regression</em> or <em>OLS regression</em> for short, was developed independently by the mathematicians Gauss and Legendre at or around the first decade of the 19th century, and there remains today some controversy about who should take credit for its discovery. However, at the time of its discovery it was not actually known as ‘regression’‍. This term became more popular following the work of Francis Galton—a British intellectual jack-of-all-trades and a cousin of Charles Darwin. In the late 1800s, Galton had researched the relationship between the heights of a population of almost 1000 children and the average height of their parents (mid-parent height). He was surprised to discover that there was not a perfect relationship between the height of a child and the average height of its parents, and that in general children’s heights were more likely to be in a range that was closer to the mean for the total population. He described this statistical phenomenon as a ‘regression towards mediocrity’ (‘regression’ comes from a Latin term approximately meaning ‘go back’).</p>
<p>Figure <a href="linear-reg-ols.html#fig:galton-fig">4.1</a> is a scatter plot of Galton’s data with the black solid line showing what a perfect relationship would look like, the black dot-dashed line indicating the mean child height and the red dashed line showing the actual relationship determined by Galton<a href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a>. You can regard the red dashed line as ‘going back’ from the perfect relationship (symbolized by the black line). This might give you an intuition that will help you understand later sections of this chapter. In an arbitrary data set, the red dashed line can lie anywhere between the black dot-dashed line (no relationship) and the black solid line (a perfect relationship). Linear regression is about finding the red dashed line in your data and using it to explain the degree to which your input data (the <span class="math inline">\(x\)</span> axis) explains your outcome data (the <span class="math inline">\(y\)</span> axis).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:galton-fig"></span>
<img src="_main_files/figure-html/galton-fig-1.png" alt="Galton's study of the height of children introduced the term 'regression'" width="672" />
<p class="caption">
Figure 4.1: Galton’s study of the height of children introduced the term ‘regression’
</p>
</div>
</div>
<div id="use-cases-ols" class="section level3" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Use cases for linear regression</h3>
<p>Linear regression is particularly suited to a problem where the outcome of interest is on some sort of continuous scale (for example, quantity, money, height, weight). For outcomes of this type, it can be a first port of call before trying more complex modeling approaches. It is simple and easy to explain, and analysts will often accept a somewhat poorer fit using linear regression in order to avoid having to interpret a more complex model.</p>
<p>Here are some illustratory examples of questions that could be tackled with a linear regression approach:</p>
<ul>
<li><p>Given a data set of demographic data, job data and current salary data, to what extent can current salary be explained by the rest of the data?</p></li>
<li><p>Given annual test scores for a set of students over a four-year period, what is the relationship between the final test score and earlier test scores?</p></li>
<li><p>Given a set of GPA data, SAT data and data on the percentile score on an aptitude test for a set of job applicants, to what extent can the GPA and SAT data explain the aptitude test score?</p></li>
</ul>
</div>
<div id="walkthrough-ols" class="section level3" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> Walkthrough example</h3>
<p>You are working as an analyst for the biology department of a large academic institution which offers a four-year undergraduate degree program. The academic leaders of the department are interested in understanding how student performance in the final-year examination of the degree program relates to performance in the prior three years.</p>
<p>To help with this, you have been provided with data for 975 individuals graduating in the past three years, and you have been asked to create a model to explain each individual’s final examination score based on their examination scores for the first three years of their program. The Year 1 examination scores are awarded on a scale of 0–100, Years 2 and 3 on a scale of 0–200, and the Final year is awarded on a scale of 0–300.</p>
<p>We will load the <code>ugtests</code> data set into our session and take a brief look at it.</p>
<div class="sourceCode" id="cb235"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb235-1"><a href="linear-reg-ols.html#cb235-1" aria-hidden="true" tabindex="-1"></a><span class="co"># if needed, download ugtests data</span></span>
<span id="cb235-2"><a href="linear-reg-ols.html#cb235-2" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="st">&quot;http://peopleanalytics-regression-book.org/data/ugtests.csv&quot;</span></span>
<span id="cb235-3"><a href="linear-reg-ols.html#cb235-3" aria-hidden="true" tabindex="-1"></a>ugtests <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(url)</span></code></pre></div>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb236-1"><a href="linear-reg-ols.html#cb236-1" aria-hidden="true" tabindex="-1"></a><span class="co"># look at the first few rows of data</span></span>
<span id="cb236-2"><a href="linear-reg-ols.html#cb236-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(ugtests)</span></code></pre></div>
<pre><code>##   Yr1 Yr2 Yr3 Final
## 1  27  50  52    93
## 2  70 104 126   207
## 3  27  36 148   175
## 4  26  75 115   125
## 5  46  77  75   114
## 6  86 122 119   159</code></pre>
<p>The data looks as expected, with test scores for four years all read in as numeric data types, but of course this is only a few rows. We need a quick statistical and structural overview of the data.</p>
<div class="sourceCode" id="cb238"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb238-1"><a href="linear-reg-ols.html#cb238-1" aria-hidden="true" tabindex="-1"></a><span class="co"># view structure</span></span>
<span id="cb238-2"><a href="linear-reg-ols.html#cb238-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(ugtests)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    975 obs. of  4 variables:
##  $ Yr1  : int  27 70 27 26 46 86 40 60 49 80 ...
##  $ Yr2  : int  50 104 36 75 77 122 100 92 98 127 ...
##  $ Yr3  : int  52 126 148 115 75 119 125 78 119 67 ...
##  $ Final: int  93 207 175 125 114 159 153 84 147 80 ...</code></pre>
<div class="sourceCode" id="cb240"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb240-1"><a href="linear-reg-ols.html#cb240-1" aria-hidden="true" tabindex="-1"></a><span class="co"># view statistical summary</span></span>
<span id="cb240-2"><a href="linear-reg-ols.html#cb240-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ugtests)</span></code></pre></div>
<pre><code>##       Yr1             Yr2             Yr3            Final    
##  Min.   : 3.00   Min.   :  6.0   Min.   :  8.0   Min.   :  8  
##  1st Qu.:42.00   1st Qu.: 73.0   1st Qu.: 81.0   1st Qu.:118  
##  Median :53.00   Median : 94.0   Median :105.0   Median :147  
##  Mean   :52.15   Mean   : 92.4   Mean   :105.1   Mean   :149  
##  3rd Qu.:62.00   3rd Qu.:112.0   3rd Qu.:130.0   3rd Qu.:175  
##  Max.   :99.00   Max.   :188.0   Max.   :198.0   Max.   :295</code></pre>
<p>We can see that the results do seem to have different scales in the different years as we have been informed, and judging by the means, students seem to have found Year 2 exams more challenging. We can also be assured that there is no missing data, as these would have been displayed as <code>NA</code> counts in our summary if they existed.</p>
<p>We can also plot our four years of test scores pairwise to see any initial relationships of interest, as displayed in Figure <a href="linear-reg-ols.html#fig:pairplot-ugtests">4.2</a>.</p>
<div class="sourceCode" id="cb242"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb242-1"><a href="linear-reg-ols.html#cb242-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(GGally)</span>
<span id="cb242-2"><a href="linear-reg-ols.html#cb242-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb242-3"><a href="linear-reg-ols.html#cb242-3" aria-hidden="true" tabindex="-1"></a><span class="co"># display a pairplot of all four columns of data</span></span>
<span id="cb242-4"><a href="linear-reg-ols.html#cb242-4" aria-hidden="true" tabindex="-1"></a>GGally<span class="sc">::</span><span class="fu">ggpairs</span>(ugtests)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pairplot-ugtests"></span>
<img src="_main_files/figure-html/pairplot-ugtests-1.png" alt="Pairplot of the `ugtests` data set" width="672" />
<p class="caption">
Figure 4.2: Pairplot of the <code>ugtests</code> data set
</p>
</div>
<p>In the diagonal, we can see the distributions of the data in each column. We observe relatively normal-looking distributions in each year. We can see scatter plots and pairwise correlation statistics off the diagonal. For example, we see a particularly strong correlation between <code>Yr3</code> and <code>Final</code> test scores, a moderate correlation between <code>Yr2</code> and <code>Final</code> and relative independence elsewhere.</p>
</div>
</div>
<div id="simple-ols" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Simple linear regression</h2>
<p>In order to visualize our approach and improve our intuition, we will start with <em>simple</em> linear regression, which is the case where there is only a single input variable and outcome variable.</p>
<div id="linear-single" class="section level3" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Linear relationship between a single input and an outcome</h3>
<p>Let our input variable be <span class="math inline">\(x\)</span> and our outcome variable be <span class="math inline">\(y\)</span>. Recalling the equation of a straight line, because we assume that the relationship is linear, we expect the relationship to be of the form:</p>
<p><span class="math display">\[y = mx + c\]</span>
where <span class="math inline">\(m\)</span> represents the slope or gradient of the line, and <span class="math inline">\(c\)</span> represents the point at which the line intercepts the <span class="math inline">\(y\)</span> axis. When using a straight line to model a relationship in the data, we call <span class="math inline">\(c\)</span> and <span class="math inline">\(m\)</span> the <em>coefficients</em> of the model.</p>
<p>Now let’s assume that we have a sample of 10 observations with which to estimate our linear relationship. Let’s take the first 10 values of <code>Yr3</code> and <code>Final</code> in our <code>ugtests</code> data set:</p>
<div class="sourceCode" id="cb243"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb243-1"><a href="linear-reg-ols.html#cb243-1" aria-hidden="true" tabindex="-1"></a>(d <span class="ot">&lt;-</span> <span class="fu">head</span>(ugtests[ , <span class="fu">c</span>(<span class="st">&quot;Yr3&quot;</span>, <span class="st">&quot;Final&quot;</span>)], <span class="dv">10</span>))</span></code></pre></div>
<pre><code>##    Yr3 Final
## 1   52    93
## 2  126   207
## 3  148   175
## 4  115   125
## 5   75   114
## 6  119   159
## 7  125   153
## 8   78    84
## 9  119   147
## 10  67    80</code></pre>
<p>We can do a simple plot of these observations as in Figure <a href="linear-reg-ols.html#fig:sample-plot">4.3</a>. Intuitively, we can imagine a line passing through these points that ‘fits’ the general pattern. For example, taking <span class="math inline">\(m = 1.2\)</span> and <span class="math inline">\(c = 5\)</span>, the resulting line <span class="math inline">\(y = 1.2x + 5\)</span> could fit between the points we are given, as displayed in Figure <a href="linear-reg-ols.html#fig:sample-plot-fit">4.4</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:sample-plot"></span>
<img src="_main_files/figure-html/sample-plot-1.png" alt="Basic scatter plot of 10 observations" width="672" />
<p class="caption">
Figure 4.3: Basic scatter plot of 10 observations
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:sample-plot-fit"></span>
<img src="_main_files/figure-html/sample-plot-fit-1.png" alt="Fitting $y=1.2x + 5$ to our 10 observations" width="672" />
<p class="caption">
Figure 4.4: Fitting <span class="math inline">\(y=1.2x + 5\)</span> to our 10 observations
</p>
</div>
<p>This looks like an approximation of the relationship, but how do we know that it is the best approximation?</p>
</div>
<div id="minimising-error-ols" class="section level3" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Minimising the error</h3>
<p>For each of our observations, we can determine an error in the fitted model by calculating the difference between the real value of <span class="math inline">\(y\)</span> and the one predicted by our model. For example, at <span class="math inline">\(x = 52\)</span>, our modeled value of y is 67.4, but the real value is 93, producing an error of 25.6. These errors are known as the <em>residuals</em> of our model. The residuals for the 10 points in our data set are illustrated by the solid red line segments in Figure <a href="linear-reg-ols.html#fig:sample-plot-residuals">4.5</a>. It looks like at least one of our residuals is pretty large.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:sample-plot-residuals"></span>
<img src="_main_files/figure-html/sample-plot-residuals-1.png" alt="Residuals of $y=1.2x + 5$ for our 10 observations" width="672" />
<p class="caption">
Figure 4.5: Residuals of <span class="math inline">\(y=1.2x + 5\)</span> for our 10 observations
</p>
</div>
<p>The error of our model—which we want to minimize—could be defined in a number of ways:</p>
<ol style="list-style-type: decimal">
<li>The average of our residuals</li>
<li>The average of the <em>absolute values</em> of our residuals (so that negative values are converted to positive values)</li>
<li>The average of the squares of our residuals (note that all squares are positive)</li>
</ol>
<p>For a number of reasons (not least the fact that at the time this method was developed it was one of the easiest to derive), the most common approach is number 3, which is why we call our regression model <em>Ordinary Least Squares</em> regression. Some algebra and calculus can help us determine the equation of the line that generates the least-squared residual error. For more of the theory behind this, consult <span class="citation"><a href="#ref-linear" role="doc-biblioref">Montgomery, Peck, and Vining</a> (<a href="#ref-linear" role="doc-biblioref">2012</a>)</span>, but let’s look at how this works in practice.</p>
</div>
<div id="best-fit-simple-ols" class="section level3" number="4.2.3">
<h3><span class="header-section-number">4.2.3</span> Determining the best fit</h3>
<p>We can run a fairly simple function in R to calculate the best fit linear model for our data. Once we have run that function, the model and all the details will be saved in our session for further investigation or use.</p>
<p>First we need to express the model we are looking to calculate as a formula. In this simple case, we want to regress the outcome <span class="math inline">\(y =\)</span> <code>Final</code> against the input <span class="math inline">\(x =\)</span> <code>Yr3</code>, and therefore we would use the simple formula notation <code>Final ~ Yr3</code>. Now we can use the <code>lm()</code> function to calculate the linear model based on our data set and our formula.</p>
<div class="sourceCode" id="cb245"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb245-1"><a href="linear-reg-ols.html#cb245-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate model</span></span>
<span id="cb245-2"><a href="linear-reg-ols.html#cb245-2" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">formula =</span> Final <span class="sc">~</span> Yr3, <span class="at">data =</span> d)</span></code></pre></div>
<p>The <code>model</code> object that we have created is a list of a number of different pieces of information, which we can see by looking at the names of the objects in the list.</p>
<div class="sourceCode" id="cb246"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb246-1"><a href="linear-reg-ols.html#cb246-1" aria-hidden="true" tabindex="-1"></a><span class="co"># view the names of the objects in the model</span></span>
<span id="cb246-2"><a href="linear-reg-ols.html#cb246-2" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(model) </span></code></pre></div>
<pre><code>##  [1] &quot;coefficients&quot;  &quot;residuals&quot;     &quot;effects&quot;       &quot;rank&quot;          &quot;fitted.values&quot; &quot;assign&quot;        &quot;qr&quot;           
##  [8] &quot;df.residual&quot;   &quot;xlevels&quot;       &quot;call&quot;          &quot;terms&quot;         &quot;model&quot;</code></pre>
<p>So we can already see some terms we are familiar with. For example, we can look at the coefficients.</p>
<div class="sourceCode" id="cb248"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb248-1"><a href="linear-reg-ols.html#cb248-1" aria-hidden="true" tabindex="-1"></a>model<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>## (Intercept)         Yr3 
##   16.630452    1.143257</code></pre>
<p>This tells us that that our best fit model—the one that minimizes the average squares of the residuals—is <span class="math inline">\(y = 1.14x + 16.63\)</span>. In other words, our <code>Final</code> test score can be expected to take a value of 16.63 even with zero score in the <code>Yr3</code> input, and every additional point scored in <code>Yr3</code> will increase the <code>Final</code> score by 1.14.</p>
</div>
<div id="measuring-the-fit-of-the-model" class="section level3" number="4.2.4">
<h3><span class="header-section-number">4.2.4</span> Measuring the fit of the model</h3>
<p>We have calculated a model which minimizes the average squared residual error for the sample of data that we have, but we don’t really have a sense of how ‘good’ the model is. How do we tell how well our model uses the input data to explain the outcome? This is an important question to answer because you would not want to propose a model that does not do a good job of explaining your outcome, and you also may need to compare your model to other alternatives, which will require some sort of benchmark metric.</p>
<p>One natural way to benchmark how good a job your model does of explaining the outcome is to compare it to a situation where you have no input and no model at all. In this situation, all you have is your outcome values, which can be considered a random variable with a mean and a variance. In the case of our 10 observations, we have 10 values of <code>Final</code> with a mean of 133.7. We can consider the horizontal line representing the mean of <span class="math inline">\(y\)</span> as our ‘random model’‍, and we can calculate the residuals around the mean. This can be seen in Figure <a href="linear-reg-ols.html#fig:model-mean">4.6</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:model-mean"></span>
<img src="_main_files/figure-html/model-mean-1.png" alt="Residuals of our 10 observations around their mean value" width="672" />
<p class="caption">
Figure 4.6: Residuals of our 10 observations around their mean value
</p>
</div>
<p>Recall from Section <a href="found-stats.html#mean-var-sd">3.1.1</a> the definition of the population variance of <span class="math inline">\(y\)</span>, notated as <span class="math inline">\(\mathrm{Var}(y)\)</span>. Note that it is defined as the average of the squares of the residuals around the mean of <span class="math inline">\(y\)</span>. Therefore <span class="math inline">\(\mathrm{Var}(y)\)</span> represents the average squared residual error of a random model. This calculates in this case to 1574.21. Let’s overlay our fitted model onto this random model in Figure <a href="linear-reg-ols.html#fig:model-overlay">4.7</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:model-overlay"></span>
<img src="_main_files/figure-html/model-overlay-1.png" alt="Comparison of residuals of fitted model (red) against random variable (blue)" width="672" />
<p class="caption">
Figure 4.7: Comparison of residuals of fitted model (red) against random variable (blue)
</p>
</div>
<p>So for most of our observations (though not all) we seem to have reduced the ‘distance’ from the random model by fitting our new model. If we average the square of our residuals for the fitted model, we obtain the average squared residual error of our fitted model, which calculates to 398.35.</p>
<p>Therefore, before we fit our model, we have an error of 1574.21, and after we fit it, we have an error of 398.35. So we have reduced the error of our model by 1175.86 or, expressed as a proportion, by 0.75. In other words, we can say that our model explains 0.75 (or 75%) of the variance of our outcome.</p>
<p>This metric is known as the <span class="math inline">\(R^2\)</span> of our model and is the primary metric used in measuring the fit of a linear regression model<a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a>.</p>
</div>
</div>
<div id="multiple-linear-regression" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Multiple linear regression</h2>
<p>In reality, regression problems rarely involve one single input variable, but rather multiple variables. The methodology for multiple linear regression is similar in nature to simple linear regression, but obviously more difficult to visualize because of its increased dimensionality.</p>
<p>In this case, our inputs are a set of <span class="math inline">\(p\)</span> variables <span class="math inline">\(x_1, x_2, \dots, x_p\)</span>. Extending the linear equation in Section <a href="linear-reg-ols.html#linear-single">4.2.1</a>, we seek to develop an equation of the form:</p>
<p><span class="math display">\[y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_px_p\]</span>
so that our average squared residual error is minimized.</p>
<div id="running-a-multiple-linear-regression-model-and-interpreting-its-coefficients" class="section level3" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Running a multiple linear regression model and interpreting its coefficients</h3>
<p>A multiple linear regression model is run in a similar way to a simple linear regression model, with your formula notation determining what outcome and input variables you wish to have in your model. Let’s now perform a multiple linear regression on our entire <code>ugtests</code> data set and regress our <code>Final</code> test score against all prior test scores using the formula <code>Final ~ Yr3 + Yr2 + Yr1</code> and determine our coefficients as before.</p>
<div class="sourceCode" id="cb250"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb250-1"><a href="linear-reg-ols.html#cb250-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">data =</span> ugtests, <span class="at">formula =</span> Final <span class="sc">~</span> Yr3 <span class="sc">+</span> Yr2 <span class="sc">+</span> Yr1)</span>
<span id="cb250-2"><a href="linear-reg-ols.html#cb250-2" aria-hidden="true" tabindex="-1"></a>model<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>## (Intercept)         Yr3         Yr2         Yr1 
## 14.14598945  0.86568123  0.43128539  0.07602621</code></pre>
<p>Referring to our formula in Section <a href="linear-reg-ols.html#multiple-linear-regression">4.3</a>, let’s understand what each coefficient <span class="math inline">\(\beta_0, \beta_1, \dots, \beta_p\)</span> means. <span class="math inline">\(\beta_0\)</span>, the <em>intercept</em> of the model, represents the value of <span class="math inline">\(y\)</span> assuming that all the inputs were zero. You can imagine that your output can be expected to have a base value even without any inputs—a student who completely flunked the first three years can still redeem themselves to some extent in the Final year.</p>
<p>Now looking at the other coefficients, let’s consider what happens if our first input <span class="math inline">\(x_1\)</span> increased by a single unit, assuming nothing else changed. We would then expect our value of y to increase by <span class="math inline">\(\beta_1\)</span>. Similarly for any input <span class="math inline">\(x_k\)</span>, a unit increase would result in an increase in <span class="math inline">\(y\)</span> of <span class="math inline">\(\beta_k\)</span>, assuming no other changes in the inputs.</p>
<p>In the case of our <code>ugtests</code> data set, we can say the following:</p>
<ul>
<li><p>The intercept of the model is 14.146. This is the value that a student could be expected to score on their final exam even if they had scored zero in all previous exams.</p></li>
<li><p>The <code>Yr3</code> coefficient is 0.866. Assuming no change in other inputs, this is the increase in the Final exam score that could be expected from an extra point in the Year 3 score.</p></li>
<li><p>The <code>Yr2</code> coefficient is 0.431. Assuming no change in other inputs, this is the increase in the Final exam score that could be expected from an extra point in the Year 2 score.</p></li>
<li><p>The <code>Yr1</code> coefficient is 0.076. Assuming no change in other inputs, this is the increase in the Final exam score that could be expected from an extra point in the Year 1 score.</p></li>
</ul>
</div>
<div id="coefficient-confidence" class="section level3" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Coefficient confidence</h3>
<p>Intuitively, these coefficients appear too precise for comfort. After all, we are attempting to estimate a relationship based on a limited set of data. In particular, looking at the <code>Yr1</code> coefficient, it seems to be very close to zero, implying that there is a possibility that the Year 1 examination score has no impact on the final examination score. Like in any statistical estimation, the coefficients calculated for our model have a margin of error. Typically, in any such situation, we seek to know a 95% confidence interval to set a standard of certainty around the values we are interpreting.<br />
The <code>summary()</code> function is a useful way to gather critical information in your model, including important statistics on your coefficients:</p>
<div class="sourceCode" id="cb252"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb252-1"><a href="linear-reg-ols.html#cb252-1" aria-hidden="true" tabindex="-1"></a>model_summary <span class="ot">&lt;-</span> <span class="fu">summary</span>(model)</span>
<span id="cb252-2"><a href="linear-reg-ols.html#cb252-2" aria-hidden="true" tabindex="-1"></a>model_summary<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>##                Estimate Std. Error   t value      Pr(&gt;|t|)
## (Intercept) 14.14598945 5.48005618  2.581358  9.986880e-03
## Yr3          0.86568123 0.02913754 29.710169 1.703293e-138
## Yr2          0.43128539 0.03250783 13.267124  4.860109e-37
## Yr1          0.07602621 0.06538163  1.162807  2.451936e-01</code></pre>
<p>The 95% confidence interval corresponds to approximately two standard errors above or below the estimated value. For a given coefficient, if this confidence interval includes zero, you cannot reject the hypothesis that the variable has no relationship with the outcome. Another indicator of this is the <code>Pr(&gt;|t|)</code> column of the coefficient summary, which represents the <em>p-value</em> of the null hypothesis that the input variable has no relationship with the outcome. If this value is less than a certain threshold (usually 0.05), you can conclude that this variable has a statistically significant relationship with the outcome. To see the precise confidence intervals for your model coefficients, you can use the <code>confint()</code> function.</p>
<div class="sourceCode" id="cb254"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb254-1"><a href="linear-reg-ols.html#cb254-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(model)</span></code></pre></div>
<pre><code>##                   2.5 %     97.5 %
## (Intercept)  3.39187185 24.9001071
## Yr3          0.80850142  0.9228610
## Yr2          0.36749170  0.4950791
## Yr1         -0.05227936  0.2043318</code></pre>
<p>In this case, we can conclude that the examinations in Years 2 and 3 have a significant relationship with the Final examination score, but we cannot conclude this for Year 1. Effectively, this means that we can drop <code>Yr1</code> from our model with no substantial loss of fit. In general, simpler models are easier to manage and interpret, so let’s remove the non-significant variable now.</p>
<div class="sourceCode" id="cb256"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb256-1"><a href="linear-reg-ols.html#cb256-1" aria-hidden="true" tabindex="-1"></a>newmodel <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">data =</span> ugtests, <span class="at">formula =</span> Final <span class="sc">~</span> Yr3 <span class="sc">+</span> Yr2)</span></code></pre></div>
<p>Given that our new model only has three dimensions, we have the luxury of visualizing it. Interactive Figure <a href="linear-reg-ols.html#fig:lin-reg-3d">4.8</a> shows the data and the fitted plane of our model.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lin-reg-3d"></span>
<div id="htmlwidget-69b72f580275164a3c7a" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-69b72f580275164a3c7a">{"x":{"visdat":{"7610301ea57e":["function () ","plotlyVisDat"]},"cur_data":"7610301ea57e","attrs":{"7610301ea57e":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"mode":"markers","type":"scatter3d","marker":{"size":5,"color":"blue","symbol":104},"name":"Observations","inherit":true},"7610301ea57e.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":[84.6829397872682,172.037323163715,161.665981520901,149.984349274745,116.250704032985,173.765079359715,169.442926301292,125.330972939804,163.388452803379,130.94900320106,128.785764661772,170.746010204377,166.412806872234,177.229480015353,162.094977833963,183.27626636633,143.497035890301,176.797360798848,147.388030625586,182.848471169976,156.036420539241,120.141218321585,165.552652236033,177.230921355404,192.787213149604,167.287615132282,200.58265512731,179.822195314387,183.286836193367,104.141757037159,148.685829615153,156.898016515493,137.877324065653,156.032817189114,147.386829508877,187.1735069085,147.824714085584,191.067864770568,165.554814246109,207.067806501678,159.497458068095,146.087829402601,127.495893042483,186.739946351944,223.932467112483,191.499743763732,104.1391145804,188.471305898067,170.307404957645,125.771259749928,181.120234527296,153.883031156965,94.1953279105949,106.739997473052,142.196834667317,128.793211585367,141.331395117597,89.8690910553626,115.393432076884,185.016514176098,164.68264844282,181.9890372038,140.029752554562,175.061437009231,163.820091573201,111.061670084792,157.332297742074,172.471604390299,164.257015256541,150.846665921022,204.473649862594,134.847444859938,158.635381645159,144.79507510321,112.793510077598,134.84912642333,149.547906038088,99.382440072054,146.95591140908,115.823149059972,125.333134949879,139.172000151778,166.843004302006,103.277038157464,186.742588808703,149.120831511759,199.283414797692,145.227434543058,123.601775403756,134.848645976646,134.41364408004,174.200561703004,148.685349168469,159.929337061259,141.335238691065,142.200197794101,126.200496286333,146.95735274913,160.796458174371,167.281129102055,138.301756135223,128.362533708911,143.060832876986,208.366326161271,162.094977833963,169.876246634506,99.8212855421284,133.550606763737,133.546522966928,98.5203636491186,103.704833353818,121.445263118038,130.517124207896,134.416526760141,169.4412447379,155.604301322735,202.304645963105,104.140315697108,220.043153940591,137.010443175883,119.277940781941,92.9013724944956,167.715410328636,90.3031320586021,154.735258422889,220.905470586868,119.274337431815,126.623967462536,153.443224793524,125.333134949879,151.27494156406,189.766462430875,168.142965301649,141.334277797698,147.387069732219,128.359170582127,174.635803822953,112.788465387422,146.959274535864,147.826635872318,143.495114103567,174.632440696168,145.220948512831,163.389653920088,156.895133835392,120.140017204877,121.004736084571,196.688777711925,107.174759146317,146.524272639258,130.954288114578,82.9486975610463,147.38730995556,172.903723606804,149.113624811507,189.768864664292,92.4642085878135,114.524148953696,116.685465706249,218.305788810924,108.905638245757,182.849672286685,174.194315896119,145.654028622703,211.821598329923,156.466858192354,157.332778188757,169.441965407925,141.763754557444,111.931914101346,168.576045411522,95.9247656699842,198.413891451163,150.842341900871,103.281121954274,138.301035465198,153.006781556867,120.572616868066,115.383342696531,173.765319583056,107.166591552698,144.357190526503,140.459229314308,103.283524187691,82.5218632580591,124.471538973627,175.501243372673,181.118552963903,182.851113626735,168.148730661851,108.039718249354,148.255872408723,147.823272745534,124.030531493477,156.898256738834,111.05734606464,135.716247536442,105.871915466573,178.090595544922,161.221370690624,89.0086961958193,143.926993096731,124.902697296766,194.956457272435,120.137614971459,175.06672192275,171.606645287262,128.353885668609,156.469740872455,144.358391643212,146.088309849284,156.03497919919,90.7333294883737,124.472019420311,118.841497545284,166.845646758765,142.200918464126,150.846425697681,140.470039364687,150.411423801074,178.52583766487,134.843120839786,170.739043727467,159.497938514778,149.984829721429,168.57580518818,102.413520394478,110.192867408287,157.333498858783,152.575623233728,158.195575281718,185.882914619185,126.631654609472,119.708378435055,163.388452803379,197.553256368278,123.170617080617,209.226721020814,209.229363477573,110.195269641705,137.867715131984,169.87408462443,166.41496888231,219.178675284238,145.222389852881,162.525175263735,183.283232843241,151.718591500969,152.149509600766,174.637485386345,152.573461223652,192.793699179831,143.059631760278,204.905528855758,148.257553972115,91.1688118316637,178.093718448364,161.651568120396,165.546886875831,172.898919139969,126.193049362739,175.504366276115,117.113501125945,110.194308748338,159.927175051183,165.124616816337,172.036602493692,208.798445377776,140.895432327623,135.27908362976,175.497640022546,166.845646758765,171.174045624073,179.390076097881,132.253528444195,144.356229633136,168.581570548382,164.255093469807,155.600697972609,160.789011250777,71.7061510083074,194.959580175878,89.8676497153121,147.390673082345,137.438478595579,103.707956257261,77.7656691963969,120.143380331661,110.193588078313,177.230921355404,157.340945782377,180.688835980815,202.741089199762,207.499925718184,109.33055076201,171.606645287262,146.091192529385,127.92416868552,115.383342696531,216.147354738472,168.581810771723,124.034375066945,163.386531016645,191.496861083631,128.784563545064,143.063475333745,95.497931366997,170.744568864327,190.201944774165,94.1974899206706,115.817864146454,121.442380437937,178.95723621135,156.902820982327,169.868079040887,119.711981785181,116.678259005997,162.958976043633,174.631960249485,220.904269470159,180.25815810436,161.654210577155,87.2770964263544,192.365903983477,139.168637024993,103.71059871402,220.90571081021,124.897172159906,185.877869929008,178.096601128465,179.389595651198,149.978583914543,182.419955303597,135.709761506215,168.143205524991,191.929941193504,169.439563174507,211.822799446632,163.817689339784,96.3559239931228,150.849548601123,139.169357695019,138.735797138463,177.228038675303,106.301872673003,156.468779979088,177.224435325177,148.688472071912,142.629434330506,95.4931269001621,133.981765086876,154.741264006433,116.679460122706,187.606827241714,177.222753761784,142.630395223873,162.525175263735,169.013689764887,177.228519121986,212.254438216454,160.794296164295,130.087647448149,180.250230734083,159.498418961462,136.57592172596,126.634056842889,159.494815611336,151.7089825673,187.604665231638,189.768864664292,180.685713077372,133.548925200345,166.849730555575,157.331096625365,198.415092567872,125.335777406638,97.2230451062349,181.981830503547,166.418332009095,77.3270639496643,141.33043422423,117.111819562553,108.46366987224,169.878889091265,109.327427858568,160.358813821005,148.254671292014,108.902755565656,141.331635340939,178.534245481831,166.846847875474,175.928317899002,156.037621655949,158.632498965058,197.986096254809,117.97822000564,147.817747608674,146.954950515713,168.147769768484,170.740004620834,201.012852557082,148.684868721786,111.060468968083,161.657093257256,77.7661496430804,131.395776041411,105.436913569967,176.36356001895,146.953028728979,136.579765299428,195.387615595573,117.983024472475,107.597990099178,140.471000258054,101.983322964707,221.772351476638,125.336257853322,120.571175528015,149.982907934695,159.928856614575,127.490608128964,130.952366327844,164.686972462971,183.282031726532,198.421578598099,128.367097952405,137.874681608894,138.300074571831,188.471065674725,133.981765086876,121.435654184368,127.494451702432,158.631778295033,106.309079373256,146.95591140908,199.280772340933,200.145010773944,90.3043331753108,205.773130415554,120.567572177889,133.114403750422,185.008827029162,125.336498076664,95.0622088003653,215.709710385106,113.216981253801,140.030473224587,185.012910825972,110.197671875122,144.792913093135,152.575383010386,151.282628710996,108.898191322163,164.253411906415,167.275603965195,153.441062783448,138.306560602058,205.770968405478,116.252145373035,167.709164521751,111.490666397855,123.167253953833,136.141400276037,142.632076787265,137.438718818921,98.087283539246,164.25269123639,163.81024241619,156.901619865619,151.71042390735,147.386589285535,147.821831405483,202.310651546649,160.357372480955,145.224792086298,146.956151632422,113.658949627318,190.200743657456,145.219747396122,146.521149735815,144.788829296325,181.976785813371,140.904080367926,149.547906038088,130.526492918224,166.412806872234,129.227973258631,132.249444647386,94.6250448936831,179.390316321223,139.606041155017,197.982492904683,132.249204424044,86.4063719631162,158.199659078528,199.711930664072,117.54826279921,124.899334169981,190.202425220848,157.765618075288,110.628830198261,140.475084054863,118.845341118752,116.253106266402,181.55499620056,154.7422248998,114.521506496937,128.361813038886,126.629973046079,125.764293273018,157.761294055137,121.870656080975,129.660813145162,167.713488541903,152.577545020462,146.95182761227,94.1842776368747,161.231460070977,111.054223161198,108.462708978873,168.143926195016,119.708138211713,148.248665708471,210.953276100102,182.851353850077,138.308722612134,146.086628285892,145.656671079462,166.850691448942,123.173499760718,150.418630501326,127.925369802229,85.9788169901037,101.976356487796,155.603340429368,147.817507385332,148.678382691559,210.963365480455,164.686972462971,162.521331690267,166.846607652132,131.817565654222,140.471720928079,134.83951748966,188.468663441308,209.223598117371,123.600093840364,218.309151937708,130.084044098023,200.146932560678,161.226175157459,211.395724920303,196.693101732076,158.627454274882,148.246984145078,159.502983204955,93.7627282474059,128.362533708911,218.738868920796,169.870000827621,103.275356594072,138.307041048741,140.037439701498,92.0308882545992,107.601593449304,144.788829296325,181.110625593626,83.3851407977033,96.3549630997558,110.196470758414,141.332356010964,149.550788718189,155.600938195951,146.953268952321,67.8189998464912,145.658352642855,164.251490119681,99.8198442020779,141.329713554205,127.495893042483,180.688355534132,152.14999004745,97.6568458861326,121.872577867709,162.950087779988,102.844918940959,236.47569533489,182.417072623496,113.22514884742,125.330012046437,116.686666822958,199.714573120831,190.636466224088,199.719858034349,204.906729972467,119.708138211713,134.847444859938,158.630817401666,94.6243242236579,145.222630076223,161.660936830724,206.203087621983,170.312689871163,103.277758827489,108.031070209051,109.759547075073,132.252327327487,118.41514368898,133.111040623638,162.95585314019,214.417676755741,144.793393539818,137.867955355325,114.522947836988,172.038284057084,123.609462550692,126.194250479448,118.844620448727,115.384063366556,119.284426812168,112.351781927423,146.088550072626,169.012248424837,143.06827980058,197.984414691417,179.391757661273,179.821714867703,126.630453492763,169.013449541545,169.442926301292,171.169961827264,207.499925718184,149.551989834898,98.0875237625878,165.981168102412,143.064196003771,157.762254948504,133.114643973764,191.068585440593,182.41491061342,118.41514368898,128.363014155595,91.1637671414871,138.740121158614,116.257670509895,158.628174944907,92.0311284779409,205.774331532262,159.488810027792,94.6245644469996,153.874142893321,168.579888984989,111.929992314612,158.197977515135,180.689316427499,116.679219899364,136.578323959377,139.171519705094,107.169234009457,173.761476009588,134.848165529963,130.525051578173,157.336141315542,142.628473437139,149.118189055,96.3619295766664,85.1189025772439,140.468357801295,185.011709709263,102.842036260858,197.988498488226,139.171039258411,121.873778984417,132.687809670777,188.906548018015,191.498302423682,141.331395117597,128.363254378937,124.034855513629,175.064800136016,76.8944642864752,144.351905612985,142.192030200482,104.137192793666,152.571058990235,183.713670496354,168.58133032504,164.683369112845,153.014468703802,215.717877978725,210.525961350431,188.476831034927,189.336024777762,203.606528749482,130.522649344756,148.255391962039,210.092400793875,152.579466807196,147.386829508877,120.574058208116,160.363378064498,194.528181629397,156.031375849064,75.1679092071868,140.034797244738,148.682226265027,171.605924617237,178.524876771503,140.904320591268,165.977564752286,85.9809790001794,196.686375478508,94.1946072405697,172.468001040173,93.3279665741411,124.034375066945,175.071526389584,101.114760511544,145.65907331288,144.35815141987,102.846600504351,159.496256951386,155.170500542837,178.088913981529,160.361696501106,188.903184891231,172.034200260275,181.550431957067,183.281551279849,108.895789088745,207.933005828056,107.175960263026,149.113624811507,116.246860459517,139.6036389216,111.060709191425,161.655411693863,123.598412276972,101.982602294681,122.306138424265,149.976902351151,103.70915737397,101.542075261215,102.841796037516,159.492893824602,141.767117684229,184.146029936202,151.286232061122,204.044173102848,127.056086679041,135.714325749708,187.601542328196,111.055184054565,164.680005986061,108.898431545505,188.903665337914,172.043568970603,131.38184308759,195.819734812079,157.33734243225,118.847022682144,114.950022363317,95.4926464534786,203.17344863961,181.124798770789,171.171883613998,167.28593356889,117.116383806046,166.845166312082,174.63268091951,153.002217313374,133.116565760498,148.250347271863,185.441907139035,140.9004770178,189.766942877558,157.338063102276,207.926039351146,160.361216054422,188.906307794673,154.737420432965,157.333498858783,169.443646971317,131.384245321008,116.253106266402,111.05590472459,115.818104369796,133.113202633714,162.088732027078,156.904022099036,138.74228316869,206.637368848565,112.788945834105,133.545081626877,193.229902193147,144.357670973186,181.552113520459,122.741620767555,136.574960832593,159.063897511539,137.873240268844,148.6786229149,127.066416282736,97.2218439895261,123.166533283808,162.521091466925,184.151795296403,115.388147163366,123.169175740567,130.527453811591,108.897710875479,109.765792881958,148.255151738698,165.978525645653,105.003833460094,208.364644597878,192.793939403173,130.954288114578,132.679642077157,124.027889036718,189.334823661053,168.147769768484,143.501119687111,185.875948142274,192.792978509806,199.282694127667,187.611871931891,165.115248106009,228.256061510956,180.256476540968,194.093660179474,113.227551080838,125.762851932967,174.197438799562,175.06672192275,85.1116958769916,159.490011144501,158.196536175085,114.520785826912,130.087167001466,131.384725767691,131.816364537513,71.7066314549909,132.250165317411,115.815461913036,175.064800136016,117.980622239057,213.982434635793,114.096834204026,118.847983575511,148.248425485129,98.5225256591943,138.741802722006,157.334219528808,179.823396431095,147.820149842091,135.281245639835,161.659255267331,121.874499654442,165.987413909298,165.979246315678,174.19719857622,143.069000470605,119.710780668472,120.577661558242,157.343348015794,101.109956044709,111.925187847778,164.250048779631,145.2305574465,152.144705133931,98.9541644290163,148.254431068672,143.492471646808,144.794114209843,138.304879038666,219.173630594061,168.575324741496,171.610969307414,133.981284640193,139.608923835118,149.121071735101,180.250470957424,185.880752609109,113.224908624079,136.576402172643,148.681025148318,154.73814110299,134.418928993558,175.930960355761,97.2237657762601,178.089634651555,107.602554342671,143.928674660124,109.33463455882,137.872039152135,195.394101625801,133.981284640193,164.683849559529,99.3838814121045,100.679998838279,182.849432063343,249.446958976993,178.956755764667,147.381544595358,82.5175392379077,200.583856244019,149.987952624871,134.844081733153,92.4622868010795,122.741140320871,140.03503746808,181.115430060461,172.037803610401,175.065040359357,213.548874079236,125.331213163145,178.963962464919,165.550970672641,165.117890562768,114.951463703367,136.572558599176,107.60783925619,169.013449541545,207.499925718184,127.929693822381,172.466799923464,123.606579870591,91.5970874747014,162.52589593376,207.059398684717,187.175909141917,164.257495703225,161.223052254016,128.35965102881,168.573883401446,127.500457285976,115.813780349644,117.986147375918,146.953028728979,117.121188272881,198.850815134503,136.582167532845,164.251009672998,169.869760604279,166.845406535424,112.356346170916,171.607606180629,95.4902442200612,164.686492016288,164.682888666162,176.358275105431,162.953450906772,111.493068631272,185.878830822375,155.604301322735,81.222142481758,120.570935304673,168.578207421597,116.68282324949,161.228096944193,127.931615609115,182.416111730129,106.306196693155,202.300802389637,124.898853723298,188.042549808346,157.761294055137,82.5199414713252,168.142004408282,191.070747450669,144.796276219919,164.686011569604,156.038582549316,184.575506695948],"x":{},"y":{},"type":"mesh3d","name":"Fitted values","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"Yr3"},"yaxis":{"title":"Yr2"},"camera":{"eye":{"x":-1.5,"y":1.75,"z":0}},"zaxis":{"title":"Final"},"aspectmode":"cube"},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[52,126,148,115,75,119,125,78,119,67,61,150,110,142,134,116,107,143,106,135,111,72,129,148,112,152,166,136,160,63,109,98,111,96,101,141,124,154,138,165,120,93,91,136,176,152,52,144,124,111,140,146,54,80,94,92,91,43,106,161,107,157,72,116,116,72,106,134,135,105,165,97,131,111,82,104,98,49,107,95,87,101,101,63,147,120,157,111,79,102,91,132,107,118,107,108,98,113,128,125,78,99,91,171,134,129,76,98,81,60,44,101,69,103,118,112,135,57,184,102,78,67,133,50,94,174,63,61,115,87,88,136,113,103,102,85,144,61,121,132,99,130,84,124,86,67,67,155,90,110,89,32,103,133,90,146,47,87,85,151,96,140,106,87,156,103,108,121,91,95,116,54,137,87,80,75,98,68,64,120,56,88,60,90,55,100,147,133,146,137,91,119,118,64,99,54,114,66,127,97,61,97,95,143,57,138,133,63,115,93,95,105,41,102,61,112,111,104,105,93,139,79,121,122,117,115,68,55,111,103,100,168,93,70,119,154,84,153,164,65,71,120,119,185,90,125,145,135,129,151,94,139,86,163,126,54,140,88,105,113,67,160,67,61,109,147,123,170,76,94,132,112,132,137,98,84,139,127,97,97,27,156,37,117,84,57,54,81,58,148,142,144,152,164,65,133,107,74,64,165,140,80,111,140,56,102,77,144,149,63,73,89,135,118,95,85,55,131,128,169,151,99,52,158,87,68,175,72,147,152,135,91,151,87,114,143,111,161,106,49,117,90,85,136,56,111,121,120,95,57,93,119,60,145,114,99,125,138,138,158,119,81,118,124,93,103,109,95,136,146,131,91,129,101,142,98,59,127,133,28,87,60,56,140,52,106,114,84,92,174,117,125,116,119,156,67,95,103,133,125,157,105,67,111,56,127,55,137,95,109,138,87,52,109,77,183,100,62,109,116,69,81,125,140,169,118,100,71,143,93,61,85,116,86,107,146,144,55,175,47,82,129,101,63,143,45,75,146,75,102,102,120,65,120,102,106,98,166,81,107,58,70,84,106,85,57,117,75,113,101,100,112,160,100,100,108,85,144,79,97,85,106,112,98,108,110,102,81,43,138,108,141,80,27,117,141,77,81,151,110,70,126,77,85,150,123,76,96,86,82,92,72,104,125,111,90,8,139,41,52,117,69,89,141,147,107,88,98,133,96,123,79,47,48,108,94,78,183,125,109,116,83,112,64,133,140,72,165,66,152,117,183,173,98,82,143,53,99,154,103,56,100,104,43,67,85,100,49,45,70,95,110,98,96,44,105,112,70,84,91,142,131,65,80,94,64,196,139,79,74,90,152,158,174,168,69,97,112,40,91,127,165,146,66,55,51,93,86,68,118,164,104,72,82,130,111,72,74,67,105,43,96,132,122,149,144,134,88,137,125,115,164,115,58,113,105,96,83,157,130,86,101,33,103,104,101,44,180,84,41,109,132,87,110,146,59,103,99,67,104,100,102,122,91,109,74,67,98,141,52,166,97,85,106,156,146,91,102,82,130,27,66,74,44,84,137,138,110,130,177,162,167,144,155,92,117,157,119,101,74,125,160,90,39,93,94,130,135,113,98,56,145,51,119,43,80,158,61,108,92,71,115,106,120,118,142,113,131,138,55,167,95,90,59,98,68,104,65,74,85,84,62,40,51,101,105,137,135,177,60,106,123,45,96,66,144,152,69,137,127,84,60,55,152,159,123,145,79,110,131,79,91,96,132,97,138,130,138,116,155,103,111,128,79,85,48,74,77,108,123,112,173,63,75,155,90,138,98,89,115,94,79,103,54,67,108,161,84,78,112,63,77,116,102,52,164,140,89,72,53,139,133,124,139,136,154,166,108,176,144,151,89,76,119,138,37,89,104,73,79,81,78,29,84,63,130,77,152,108,88,88,69,110,114,141,105,103,120,88,139,105,118,125,80,89,152,41,67,106,124,109,66,113,88,107,91,164,113,151,91,120,121,119,159,78,95,89,106,113,136,62,123,71,104,82,89,165,91,112,55,51,139,198,133,79,37,171,130,83,39,96,94,120,128,131,147,79,163,122,119,66,79,93,137,164,97,114,99,37,128,130,151,137,104,87,107,110,56,100,95,99,156,119,110,102,111,62,137,45,123,108,115,108,68,151,112,44,61,125,74,125,105,135,74,119,79,159,92,47,109,166,116,121,120,125],"y":[50,104,36,75,77,122,100,92,98,127,134,53,123,84,65,150,76,81,87,111,97,92,83,72,180,41,90,102,62,73,84,125,55,127,97,109,52,92,65,107,87,110,71,118,124,97,95,106,104,27,97,22,68,45,99,72,103,80,13,64,125,65,138,131,105,71,110,89,68,97,101,76,63,71,55,62,108,90,84,36,74,78,142,71,96,63,105,72,86,66,87,97,88,92,71,71,54,72,74,95,122,57,107,98,65,93,37,71,105,66,110,37,122,63,114,94,156,85,99,71,78,39,80,67,128,121,108,129,83,74,132,125,121,79,95,85,74,97,56,36,92,102,126,88,149,102,104,103,26,77,83,86,93,92,123,105,78,49,58,161,18,101,149,121,136,114,106,108,104,27,116,72,143,133,37,128,116,101,97,120,94,116,163,17,39,46,70,111,89,73,26,63,64,117,123,107,44,71,116,137,42,97,57,123,122,87,89,129,90,106,106,109,86,42,111,120,65,99,73,120,93,112,111,83,71,118,59,103,100,105,124,52,65,95,98,107,75,136,114,83,135,111,105,95,114,84,92,39,52,60,123,126,117,106,49,61,90,156,131,132,116,44,95,91,110,46,110,101,132,83,100,120,90,99,68,124,70,84,124,136,70,97,92,65,108,84,30,74,97,72,38,88,123,110,81,89,82,106,97,128,68,85,114,121,144,85,25,65,100,50,80,61,102,85,161,65,118,73,106,131,73,134,56,87,106,62,119,103,94,66,103,123,78,98,119,116,128,126,125,83,73,100,109,96,92,98,126,62,98,65,82,78,108,102,140,90,84,73,92,133,92,97,139,79,88,45,109,119,120,105,114,85,86,120,133,52,65,125,77,81,111,109,97,71,107,117,73,42,101,23,110,115,87,87,104,97,110,92,81,103,109,92,81,110,26,8,92,92,108,56,134,57,103,65,40,105,48,113,87,96,115,99,89,102,79,19,77,136,108,82,117,83,93,32,84,127,133,57,84,143,102,128,46,52,171,130,132,94,63,89,107,68,80,98,141,101,82,102,65,132,100,103,105,76,106,71,104,187,95,107,99,76,106,129,94,82,51,110,136,103,123,167,60,108,44,123,53,102,91,97,65,134,104,104,90,138,76,85,96,103,74,31,79,57,78,70,71,63,79,85,139,96,50,96,89,118,160,53,133,105,113,97,123,164,87,64,120,99,78,51,60,96,63,98,102,112,146,80,89,116,112,97,59,142,128,162,100,133,127,117,97,81,67,129,137,41,69,57,156,145,85,78,74,85,73,123,177,53,91,73,95,84,122,106,27,85,114,49,117,71,92,48,54,80,147,68,113,102,62,100,48,116,83,72,96,97,76,101,97,112,78,105,60,65,98,110,78,60,130,99,126,85,133,59,96,22,106,85,91,24,132,104,85,45,118,85,106,75,75,100,124,110,74,69,116,79,131,100,86,120,60,53,103,73,19,123,83,74,159,95,96,84,43,104,84,110,68,82,72,152,70,56,78,106,85,33,21,87,104,92,84,86,70,53,83,109,103,51,81,103,82,160,139,111,143,109,72,119,52,103,121,60,108,119,76,67,130,73,97,89,79,88,139,54,96,114,95,101,58,146,45,123,74,119,88,85,47,70,79,108,54,97,105,130,93,111,130,116,106,100,105,16,123,109,85,79,124,114,46,71,137,74,113,94,125,76,110,38,76,132,60,146,125,147,78,107,52,124,137,68,65,104,69,124,59,108,55,71,124,100,154,84,109,123,90,121,62,163,97,85,110,100,94,104,57,119,78,112,117,75,55,90,93,117,95,112,102,46,96,96,89,144,46,75,109,118,62,57,87,36,84,58,69,138,97,112,124,83,121,139,118,81,42,110,132,111,60,124,134,87,105,42,97,123,87,81,149,116,77,101,100,107,66,96,100,103,77,149,6,57,125,48,59,94,92,90,65,92,64,64,132,125,39,75,59,18,110,83,126,46,92,55,75,114,79,96,137,122,53,86,41,61,137,70,64,84,124,104,43,93,59,124,65,83,47,99,80,86,115,78,89,103,139,106,141,75,80,45,104,94,50,94,137,100,101,158,90,46,97,102,92,116,21,75,110,60,129,46,96,78,177,89,64,123,81,134,33,114,31,108,31,106,36,118,147,122,94,81,89,93,123,136,119,80,86,94,58,115,98,80,81,44,110,56,188,89,75,139,55,129,68,61,97,79,135],"z":[93,207,175,125,114,159,153,84,147,80,154,154,175,182,155,198,161,229,100,179,164,130,194,152,216,123,232,168,151,88,184,134,115,167,170,229,132,245,180,213,211,151,121,180,232,214,123,166,111,90,187,160,125,145,168,114,152,125,143,161,98,183,152,203,158,65,133,122,138,132,174,173,124,139,119,171,160,117,178,122,144,124,244,96,248,194,198,160,124,136,127,193,93,182,114,147,122,172,108,158,168,101,163,197,162,174,137,154,154,126,105,102,92,131,189,101,184,108,211,172,144,88,153,86,171,263,158,146,144,111,81,201,126,150,136,132,176,168,141,153,143,170,110,142,123,136,111,248,97,100,106,88,113,186,146,204,79,116,59,289,70,183,189,83,171,169,110,113,124,129,174,165,247,159,41,127,187,100,150,189,68,134,100,70,104,125,168,166,210,137,79,105,135,140,154,118,144,163,144,139,109,142,116,206,164,171,183,93,123,118,164,148,125,156,73,180,89,188,110,185,157,128,172,147,171,97,105,105,153,157,174,133,143,81,155,196,129,229,286,106,115,230,167,237,107,154,172,171,115,146,162,198,123,224,132,135,201,102,152,203,102,144,101,86,179,74,138,222,124,97,181,141,154,185,135,161,185,182,127,104,170,226,128,177,152,140,170,116,118,181,135,141,221,206,90,167,111,160,135,206,214,99,182,216,127,78,128,124,196,121,166,127,225,164,164,119,42,126,146,237,190,120,95,238,134,126,231,93,234,201,159,167,137,154,157,271,179,271,154,162,162,151,164,160,66,182,195,141,132,109,114,97,109,206,178,105,160,170,155,228,152,191,201,146,91,122,226,158,184,210,227,109,200,152,209,152,66,230,138,77,185,57,132,167,123,167,152,124,144,153,110,161,148,138,159,111,123,190,182,179,239,139,114,132,129,82,111,105,129,132,191,105,94,175,85,229,107,59,162,114,62,163,174,145,173,96,108,167,187,150,134,128,172,149,94,238,279,85,219,81,143,180,147,128,220,48,109,146,118,171,167,126,79,147,178,110,134,220,137,188,128,91,151,123,100,83,212,108,151,184,106,193,177,172,153,106,173,164,145,132,126,177,106,174,130,226,129,156,86,154,169,195,106,53,135,256,110,123,198,159,124,110,116,126,220,171,110,89,182,169,183,157,129,171,198,125,8,163,76,107,154,114,207,295,192,159,149,156,163,135,153,76,106,115,135,142,153,189,157,165,147,122,111,96,167,269,134,267,85,265,157,194,154,105,146,151,109,120,232,148,173,124,164,88,102,164,151,130,90,172,151,125,202,95,80,113,176,139,144,135,188,165,86,162,120,159,193,114,63,140,175,235,230,202,201,94,126,145,24,150,210,245,208,151,155,79,170,142,124,155,287,67,114,137,154,110,116,108,96,152,113,151,191,110,235,193,140,150,141,176,182,247,144,102,116,112,146,137,192,161,104,147,128,133,79,174,108,193,179,59,108,138,158,147,133,65,149,156,89,199,144,146,137,134,170,147,142,199,230,87,186,171,114,136,155,172,167,117,123,189,64,155,110,35,178,184,140,178,146,248,222,178,224,204,151,118,276,132,73,113,135,170,141,127,143,140,232,183,154,137,102,180,103,224,114,113,137,127,106,170,90,145,153,186,111,202,158,183,202,117,206,138,198,68,155,134,193,102,121,192,215,162,89,166,156,110,218,118,205,140,152,186,96,98,124,155,161,110,211,184,147,104,131,208,174,178,149,106,150,170,115,116,151,192,110,171,130,235,101,201,149,187,120,110,111,82,99,118,191,145,136,243,152,120,190,206,198,139,113,114,108,163,145,109,121,114,173,186,134,143,141,99,141,167,122,243,213,162,124,86,174,151,111,179,242,218,165,148,287,182,164,128,111,212,208,119,154,131,89,138,126,129,83,108,100,150,106,190,67,140,155,119,173,189,203,170,151,169,129,94,205,165,132,88,142,98,98,69,112,162,154,153,159,116,118,138,257,170,135,128,128,192,175,159,134,142,136,225,148,178,87,161,101,141,141,125,136,148,147,119,85,220,286,192,127,56,176,139,141,50,169,115,183,174,238,193,81,130,167,150,59,130,130,203,182,111,183,178,86,158,244,172,125,134,98,169,118,83,122,181,124,215,109,175,136,183,150,213,28,207,176,199,166,142,125,171,92,97,154,136,99,110,223,72,260,123,154,165,150,166,176,155,148,178,172],"mode":"markers","type":"scatter3d","marker":{"color":"blue","size":5,"symbol":104,"line":{"color":"rgba(31,119,180,1)"}},"name":"Observations","error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"Final","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":true,"z":[84.6829397872682,172.037323163715,161.665981520901,149.984349274745,116.250704032985,173.765079359715,169.442926301292,125.330972939804,163.388452803379,130.94900320106,128.785764661772,170.746010204377,166.412806872234,177.229480015353,162.094977833963,183.27626636633,143.497035890301,176.797360798848,147.388030625586,182.848471169976,156.036420539241,120.141218321585,165.552652236033,177.230921355404,192.787213149604,167.287615132282,200.58265512731,179.822195314387,183.286836193367,104.141757037159,148.685829615153,156.898016515493,137.877324065653,156.032817189114,147.386829508877,187.1735069085,147.824714085584,191.067864770568,165.554814246109,207.067806501678,159.497458068095,146.087829402601,127.495893042483,186.739946351944,223.932467112483,191.499743763732,104.1391145804,188.471305898067,170.307404957645,125.771259749928,181.120234527296,153.883031156965,94.1953279105949,106.739997473052,142.196834667317,128.793211585367,141.331395117597,89.8690910553626,115.393432076884,185.016514176098,164.68264844282,181.9890372038,140.029752554562,175.061437009231,163.820091573201,111.061670084792,157.332297742074,172.471604390299,164.257015256541,150.846665921022,204.473649862594,134.847444859938,158.635381645159,144.79507510321,112.793510077598,134.84912642333,149.547906038088,99.382440072054,146.95591140908,115.823149059972,125.333134949879,139.172000151778,166.843004302006,103.277038157464,186.742588808703,149.120831511759,199.283414797692,145.227434543058,123.601775403756,134.848645976646,134.41364408004,174.200561703004,148.685349168469,159.929337061259,141.335238691065,142.200197794101,126.200496286333,146.95735274913,160.796458174371,167.281129102055,138.301756135223,128.362533708911,143.060832876986,208.366326161271,162.094977833963,169.876246634506,99.8212855421284,133.550606763737,133.546522966928,98.5203636491186,103.704833353818,121.445263118038,130.517124207896,134.416526760141,169.4412447379,155.604301322735,202.304645963105,104.140315697108,220.043153940591,137.010443175883,119.277940781941,92.9013724944956,167.715410328636,90.3031320586021,154.735258422889,220.905470586868,119.274337431815,126.623967462536,153.443224793524,125.333134949879,151.27494156406,189.766462430875,168.142965301649,141.334277797698,147.387069732219,128.359170582127,174.635803822953,112.788465387422,146.959274535864,147.826635872318,143.495114103567,174.632440696168,145.220948512831,163.389653920088,156.895133835392,120.140017204877,121.004736084571,196.688777711925,107.174759146317,146.524272639258,130.954288114578,82.9486975610463,147.38730995556,172.903723606804,149.113624811507,189.768864664292,92.4642085878135,114.524148953696,116.685465706249,218.305788810924,108.905638245757,182.849672286685,174.194315896119,145.654028622703,211.821598329923,156.466858192354,157.332778188757,169.441965407925,141.763754557444,111.931914101346,168.576045411522,95.9247656699842,198.413891451163,150.842341900871,103.281121954274,138.301035465198,153.006781556867,120.572616868066,115.383342696531,173.765319583056,107.166591552698,144.357190526503,140.459229314308,103.283524187691,82.5218632580591,124.471538973627,175.501243372673,181.118552963903,182.851113626735,168.148730661851,108.039718249354,148.255872408723,147.823272745534,124.030531493477,156.898256738834,111.05734606464,135.716247536442,105.871915466573,178.090595544922,161.221370690624,89.0086961958193,143.926993096731,124.902697296766,194.956457272435,120.137614971459,175.06672192275,171.606645287262,128.353885668609,156.469740872455,144.358391643212,146.088309849284,156.03497919919,90.7333294883737,124.472019420311,118.841497545284,166.845646758765,142.200918464126,150.846425697681,140.470039364687,150.411423801074,178.52583766487,134.843120839786,170.739043727467,159.497938514778,149.984829721429,168.57580518818,102.413520394478,110.192867408287,157.333498858783,152.575623233728,158.195575281718,185.882914619185,126.631654609472,119.708378435055,163.388452803379,197.553256368278,123.170617080617,209.226721020814,209.229363477573,110.195269641705,137.867715131984,169.87408462443,166.41496888231,219.178675284238,145.222389852881,162.525175263735,183.283232843241,151.718591500969,152.149509600766,174.637485386345,152.573461223652,192.793699179831,143.059631760278,204.905528855758,148.257553972115,91.1688118316637,178.093718448364,161.651568120396,165.546886875831,172.898919139969,126.193049362739,175.504366276115,117.113501125945,110.194308748338,159.927175051183,165.124616816337,172.036602493692,208.798445377776,140.895432327623,135.27908362976,175.497640022546,166.845646758765,171.174045624073,179.390076097881,132.253528444195,144.356229633136,168.581570548382,164.255093469807,155.600697972609,160.789011250777,71.7061510083074,194.959580175878,89.8676497153121,147.390673082345,137.438478595579,103.707956257261,77.7656691963969,120.143380331661,110.193588078313,177.230921355404,157.340945782377,180.688835980815,202.741089199762,207.499925718184,109.33055076201,171.606645287262,146.091192529385,127.92416868552,115.383342696531,216.147354738472,168.581810771723,124.034375066945,163.386531016645,191.496861083631,128.784563545064,143.063475333745,95.497931366997,170.744568864327,190.201944774165,94.1974899206706,115.817864146454,121.442380437937,178.95723621135,156.902820982327,169.868079040887,119.711981785181,116.678259005997,162.958976043633,174.631960249485,220.904269470159,180.25815810436,161.654210577155,87.2770964263544,192.365903983477,139.168637024993,103.71059871402,220.90571081021,124.897172159906,185.877869929008,178.096601128465,179.389595651198,149.978583914543,182.419955303597,135.709761506215,168.143205524991,191.929941193504,169.439563174507,211.822799446632,163.817689339784,96.3559239931228,150.849548601123,139.169357695019,138.735797138463,177.228038675303,106.301872673003,156.468779979088,177.224435325177,148.688472071912,142.629434330506,95.4931269001621,133.981765086876,154.741264006433,116.679460122706,187.606827241714,177.222753761784,142.630395223873,162.525175263735,169.013689764887,177.228519121986,212.254438216454,160.794296164295,130.087647448149,180.250230734083,159.498418961462,136.57592172596,126.634056842889,159.494815611336,151.7089825673,187.604665231638,189.768864664292,180.685713077372,133.548925200345,166.849730555575,157.331096625365,198.415092567872,125.335777406638,97.2230451062349,181.981830503547,166.418332009095,77.3270639496643,141.33043422423,117.111819562553,108.46366987224,169.878889091265,109.327427858568,160.358813821005,148.254671292014,108.902755565656,141.331635340939,178.534245481831,166.846847875474,175.928317899002,156.037621655949,158.632498965058,197.986096254809,117.97822000564,147.817747608674,146.954950515713,168.147769768484,170.740004620834,201.012852557082,148.684868721786,111.060468968083,161.657093257256,77.7661496430804,131.395776041411,105.436913569967,176.36356001895,146.953028728979,136.579765299428,195.387615595573,117.983024472475,107.597990099178,140.471000258054,101.983322964707,221.772351476638,125.336257853322,120.571175528015,149.982907934695,159.928856614575,127.490608128964,130.952366327844,164.686972462971,183.282031726532,198.421578598099,128.367097952405,137.874681608894,138.300074571831,188.471065674725,133.981765086876,121.435654184368,127.494451702432,158.631778295033,106.309079373256,146.95591140908,199.280772340933,200.145010773944,90.3043331753108,205.773130415554,120.567572177889,133.114403750422,185.008827029162,125.336498076664,95.0622088003653,215.709710385106,113.216981253801,140.030473224587,185.012910825972,110.197671875122,144.792913093135,152.575383010386,151.282628710996,108.898191322163,164.253411906415,167.275603965195,153.441062783448,138.306560602058,205.770968405478,116.252145373035,167.709164521751,111.490666397855,123.167253953833,136.141400276037,142.632076787265,137.438718818921,98.087283539246,164.25269123639,163.81024241619,156.901619865619,151.71042390735,147.386589285535,147.821831405483,202.310651546649,160.357372480955,145.224792086298,146.956151632422,113.658949627318,190.200743657456,145.219747396122,146.521149735815,144.788829296325,181.976785813371,140.904080367926,149.547906038088,130.526492918224,166.412806872234,129.227973258631,132.249444647386,94.6250448936831,179.390316321223,139.606041155017,197.982492904683,132.249204424044,86.4063719631162,158.199659078528,199.711930664072,117.54826279921,124.899334169981,190.202425220848,157.765618075288,110.628830198261,140.475084054863,118.845341118752,116.253106266402,181.55499620056,154.7422248998,114.521506496937,128.361813038886,126.629973046079,125.764293273018,157.761294055137,121.870656080975,129.660813145162,167.713488541903,152.577545020462,146.95182761227,94.1842776368747,161.231460070977,111.054223161198,108.462708978873,168.143926195016,119.708138211713,148.248665708471,210.953276100102,182.851353850077,138.308722612134,146.086628285892,145.656671079462,166.850691448942,123.173499760718,150.418630501326,127.925369802229,85.9788169901037,101.976356487796,155.603340429368,147.817507385332,148.678382691559,210.963365480455,164.686972462971,162.521331690267,166.846607652132,131.817565654222,140.471720928079,134.83951748966,188.468663441308,209.223598117371,123.600093840364,218.309151937708,130.084044098023,200.146932560678,161.226175157459,211.395724920303,196.693101732076,158.627454274882,148.246984145078,159.502983204955,93.7627282474059,128.362533708911,218.738868920796,169.870000827621,103.275356594072,138.307041048741,140.037439701498,92.0308882545992,107.601593449304,144.788829296325,181.110625593626,83.3851407977033,96.3549630997558,110.196470758414,141.332356010964,149.550788718189,155.600938195951,146.953268952321,67.8189998464912,145.658352642855,164.251490119681,99.8198442020779,141.329713554205,127.495893042483,180.688355534132,152.14999004745,97.6568458861326,121.872577867709,162.950087779988,102.844918940959,236.47569533489,182.417072623496,113.22514884742,125.330012046437,116.686666822958,199.714573120831,190.636466224088,199.719858034349,204.906729972467,119.708138211713,134.847444859938,158.630817401666,94.6243242236579,145.222630076223,161.660936830724,206.203087621983,170.312689871163,103.277758827489,108.031070209051,109.759547075073,132.252327327487,118.41514368898,133.111040623638,162.95585314019,214.417676755741,144.793393539818,137.867955355325,114.522947836988,172.038284057084,123.609462550692,126.194250479448,118.844620448727,115.384063366556,119.284426812168,112.351781927423,146.088550072626,169.012248424837,143.06827980058,197.984414691417,179.391757661273,179.821714867703,126.630453492763,169.013449541545,169.442926301292,171.169961827264,207.499925718184,149.551989834898,98.0875237625878,165.981168102412,143.064196003771,157.762254948504,133.114643973764,191.068585440593,182.41491061342,118.41514368898,128.363014155595,91.1637671414871,138.740121158614,116.257670509895,158.628174944907,92.0311284779409,205.774331532262,159.488810027792,94.6245644469996,153.874142893321,168.579888984989,111.929992314612,158.197977515135,180.689316427499,116.679219899364,136.578323959377,139.171519705094,107.169234009457,173.761476009588,134.848165529963,130.525051578173,157.336141315542,142.628473437139,149.118189055,96.3619295766664,85.1189025772439,140.468357801295,185.011709709263,102.842036260858,197.988498488226,139.171039258411,121.873778984417,132.687809670777,188.906548018015,191.498302423682,141.331395117597,128.363254378937,124.034855513629,175.064800136016,76.8944642864752,144.351905612985,142.192030200482,104.137192793666,152.571058990235,183.713670496354,168.58133032504,164.683369112845,153.014468703802,215.717877978725,210.525961350431,188.476831034927,189.336024777762,203.606528749482,130.522649344756,148.255391962039,210.092400793875,152.579466807196,147.386829508877,120.574058208116,160.363378064498,194.528181629397,156.031375849064,75.1679092071868,140.034797244738,148.682226265027,171.605924617237,178.524876771503,140.904320591268,165.977564752286,85.9809790001794,196.686375478508,94.1946072405697,172.468001040173,93.3279665741411,124.034375066945,175.071526389584,101.114760511544,145.65907331288,144.35815141987,102.846600504351,159.496256951386,155.170500542837,178.088913981529,160.361696501106,188.903184891231,172.034200260275,181.550431957067,183.281551279849,108.895789088745,207.933005828056,107.175960263026,149.113624811507,116.246860459517,139.6036389216,111.060709191425,161.655411693863,123.598412276972,101.982602294681,122.306138424265,149.976902351151,103.70915737397,101.542075261215,102.841796037516,159.492893824602,141.767117684229,184.146029936202,151.286232061122,204.044173102848,127.056086679041,135.714325749708,187.601542328196,111.055184054565,164.680005986061,108.898431545505,188.903665337914,172.043568970603,131.38184308759,195.819734812079,157.33734243225,118.847022682144,114.950022363317,95.4926464534786,203.17344863961,181.124798770789,171.171883613998,167.28593356889,117.116383806046,166.845166312082,174.63268091951,153.002217313374,133.116565760498,148.250347271863,185.441907139035,140.9004770178,189.766942877558,157.338063102276,207.926039351146,160.361216054422,188.906307794673,154.737420432965,157.333498858783,169.443646971317,131.384245321008,116.253106266402,111.05590472459,115.818104369796,133.113202633714,162.088732027078,156.904022099036,138.74228316869,206.637368848565,112.788945834105,133.545081626877,193.229902193147,144.357670973186,181.552113520459,122.741620767555,136.574960832593,159.063897511539,137.873240268844,148.6786229149,127.066416282736,97.2218439895261,123.166533283808,162.521091466925,184.151795296403,115.388147163366,123.169175740567,130.527453811591,108.897710875479,109.765792881958,148.255151738698,165.978525645653,105.003833460094,208.364644597878,192.793939403173,130.954288114578,132.679642077157,124.027889036718,189.334823661053,168.147769768484,143.501119687111,185.875948142274,192.792978509806,199.282694127667,187.611871931891,165.115248106009,228.256061510956,180.256476540968,194.093660179474,113.227551080838,125.762851932967,174.197438799562,175.06672192275,85.1116958769916,159.490011144501,158.196536175085,114.520785826912,130.087167001466,131.384725767691,131.816364537513,71.7066314549909,132.250165317411,115.815461913036,175.064800136016,117.980622239057,213.982434635793,114.096834204026,118.847983575511,148.248425485129,98.5225256591943,138.741802722006,157.334219528808,179.823396431095,147.820149842091,135.281245639835,161.659255267331,121.874499654442,165.987413909298,165.979246315678,174.19719857622,143.069000470605,119.710780668472,120.577661558242,157.343348015794,101.109956044709,111.925187847778,164.250048779631,145.2305574465,152.144705133931,98.9541644290163,148.254431068672,143.492471646808,144.794114209843,138.304879038666,219.173630594061,168.575324741496,171.610969307414,133.981284640193,139.608923835118,149.121071735101,180.250470957424,185.880752609109,113.224908624079,136.576402172643,148.681025148318,154.73814110299,134.418928993558,175.930960355761,97.2237657762601,178.089634651555,107.602554342671,143.928674660124,109.33463455882,137.872039152135,195.394101625801,133.981284640193,164.683849559529,99.3838814121045,100.679998838279,182.849432063343,249.446958976993,178.956755764667,147.381544595358,82.5175392379077,200.583856244019,149.987952624871,134.844081733153,92.4622868010795,122.741140320871,140.03503746808,181.115430060461,172.037803610401,175.065040359357,213.548874079236,125.331213163145,178.963962464919,165.550970672641,165.117890562768,114.951463703367,136.572558599176,107.60783925619,169.013449541545,207.499925718184,127.929693822381,172.466799923464,123.606579870591,91.5970874747014,162.52589593376,207.059398684717,187.175909141917,164.257495703225,161.223052254016,128.35965102881,168.573883401446,127.500457285976,115.813780349644,117.986147375918,146.953028728979,117.121188272881,198.850815134503,136.582167532845,164.251009672998,169.869760604279,166.845406535424,112.356346170916,171.607606180629,95.4902442200612,164.686492016288,164.682888666162,176.358275105431,162.953450906772,111.493068631272,185.878830822375,155.604301322735,81.222142481758,120.570935304673,168.578207421597,116.68282324949,161.228096944193,127.931615609115,182.416111730129,106.306196693155,202.300802389637,124.898853723298,188.042549808346,157.761294055137,82.5199414713252,168.142004408282,191.070747450669,144.796276219919,164.686011569604,156.038582549316,184.575506695948],"x":[52,126,148,115,75,119,125,78,119,67,61,150,110,142,134,116,107,143,106,135,111,72,129,148,112,152,166,136,160,63,109,98,111,96,101,141,124,154,138,165,120,93,91,136,176,152,52,144,124,111,140,146,54,80,94,92,91,43,106,161,107,157,72,116,116,72,106,134,135,105,165,97,131,111,82,104,98,49,107,95,87,101,101,63,147,120,157,111,79,102,91,132,107,118,107,108,98,113,128,125,78,99,91,171,134,129,76,98,81,60,44,101,69,103,118,112,135,57,184,102,78,67,133,50,94,174,63,61,115,87,88,136,113,103,102,85,144,61,121,132,99,130,84,124,86,67,67,155,90,110,89,32,103,133,90,146,47,87,85,151,96,140,106,87,156,103,108,121,91,95,116,54,137,87,80,75,98,68,64,120,56,88,60,90,55,100,147,133,146,137,91,119,118,64,99,54,114,66,127,97,61,97,95,143,57,138,133,63,115,93,95,105,41,102,61,112,111,104,105,93,139,79,121,122,117,115,68,55,111,103,100,168,93,70,119,154,84,153,164,65,71,120,119,185,90,125,145,135,129,151,94,139,86,163,126,54,140,88,105,113,67,160,67,61,109,147,123,170,76,94,132,112,132,137,98,84,139,127,97,97,27,156,37,117,84,57,54,81,58,148,142,144,152,164,65,133,107,74,64,165,140,80,111,140,56,102,77,144,149,63,73,89,135,118,95,85,55,131,128,169,151,99,52,158,87,68,175,72,147,152,135,91,151,87,114,143,111,161,106,49,117,90,85,136,56,111,121,120,95,57,93,119,60,145,114,99,125,138,138,158,119,81,118,124,93,103,109,95,136,146,131,91,129,101,142,98,59,127,133,28,87,60,56,140,52,106,114,84,92,174,117,125,116,119,156,67,95,103,133,125,157,105,67,111,56,127,55,137,95,109,138,87,52,109,77,183,100,62,109,116,69,81,125,140,169,118,100,71,143,93,61,85,116,86,107,146,144,55,175,47,82,129,101,63,143,45,75,146,75,102,102,120,65,120,102,106,98,166,81,107,58,70,84,106,85,57,117,75,113,101,100,112,160,100,100,108,85,144,79,97,85,106,112,98,108,110,102,81,43,138,108,141,80,27,117,141,77,81,151,110,70,126,77,85,150,123,76,96,86,82,92,72,104,125,111,90,8,139,41,52,117,69,89,141,147,107,88,98,133,96,123,79,47,48,108,94,78,183,125,109,116,83,112,64,133,140,72,165,66,152,117,183,173,98,82,143,53,99,154,103,56,100,104,43,67,85,100,49,45,70,95,110,98,96,44,105,112,70,84,91,142,131,65,80,94,64,196,139,79,74,90,152,158,174,168,69,97,112,40,91,127,165,146,66,55,51,93,86,68,118,164,104,72,82,130,111,72,74,67,105,43,96,132,122,149,144,134,88,137,125,115,164,115,58,113,105,96,83,157,130,86,101,33,103,104,101,44,180,84,41,109,132,87,110,146,59,103,99,67,104,100,102,122,91,109,74,67,98,141,52,166,97,85,106,156,146,91,102,82,130,27,66,74,44,84,137,138,110,130,177,162,167,144,155,92,117,157,119,101,74,125,160,90,39,93,94,130,135,113,98,56,145,51,119,43,80,158,61,108,92,71,115,106,120,118,142,113,131,138,55,167,95,90,59,98,68,104,65,74,85,84,62,40,51,101,105,137,135,177,60,106,123,45,96,66,144,152,69,137,127,84,60,55,152,159,123,145,79,110,131,79,91,96,132,97,138,130,138,116,155,103,111,128,79,85,48,74,77,108,123,112,173,63,75,155,90,138,98,89,115,94,79,103,54,67,108,161,84,78,112,63,77,116,102,52,164,140,89,72,53,139,133,124,139,136,154,166,108,176,144,151,89,76,119,138,37,89,104,73,79,81,78,29,84,63,130,77,152,108,88,88,69,110,114,141,105,103,120,88,139,105,118,125,80,89,152,41,67,106,124,109,66,113,88,107,91,164,113,151,91,120,121,119,159,78,95,89,106,113,136,62,123,71,104,82,89,165,91,112,55,51,139,198,133,79,37,171,130,83,39,96,94,120,128,131,147,79,163,122,119,66,79,93,137,164,97,114,99,37,128,130,151,137,104,87,107,110,56,100,95,99,156,119,110,102,111,62,137,45,123,108,115,108,68,151,112,44,61,125,74,125,105,135,74,119,79,159,92,47,109,166,116,121,120,125],"y":[50,104,36,75,77,122,100,92,98,127,134,53,123,84,65,150,76,81,87,111,97,92,83,72,180,41,90,102,62,73,84,125,55,127,97,109,52,92,65,107,87,110,71,118,124,97,95,106,104,27,97,22,68,45,99,72,103,80,13,64,125,65,138,131,105,71,110,89,68,97,101,76,63,71,55,62,108,90,84,36,74,78,142,71,96,63,105,72,86,66,87,97,88,92,71,71,54,72,74,95,122,57,107,98,65,93,37,71,105,66,110,37,122,63,114,94,156,85,99,71,78,39,80,67,128,121,108,129,83,74,132,125,121,79,95,85,74,97,56,36,92,102,126,88,149,102,104,103,26,77,83,86,93,92,123,105,78,49,58,161,18,101,149,121,136,114,106,108,104,27,116,72,143,133,37,128,116,101,97,120,94,116,163,17,39,46,70,111,89,73,26,63,64,117,123,107,44,71,116,137,42,97,57,123,122,87,89,129,90,106,106,109,86,42,111,120,65,99,73,120,93,112,111,83,71,118,59,103,100,105,124,52,65,95,98,107,75,136,114,83,135,111,105,95,114,84,92,39,52,60,123,126,117,106,49,61,90,156,131,132,116,44,95,91,110,46,110,101,132,83,100,120,90,99,68,124,70,84,124,136,70,97,92,65,108,84,30,74,97,72,38,88,123,110,81,89,82,106,97,128,68,85,114,121,144,85,25,65,100,50,80,61,102,85,161,65,118,73,106,131,73,134,56,87,106,62,119,103,94,66,103,123,78,98,119,116,128,126,125,83,73,100,109,96,92,98,126,62,98,65,82,78,108,102,140,90,84,73,92,133,92,97,139,79,88,45,109,119,120,105,114,85,86,120,133,52,65,125,77,81,111,109,97,71,107,117,73,42,101,23,110,115,87,87,104,97,110,92,81,103,109,92,81,110,26,8,92,92,108,56,134,57,103,65,40,105,48,113,87,96,115,99,89,102,79,19,77,136,108,82,117,83,93,32,84,127,133,57,84,143,102,128,46,52,171,130,132,94,63,89,107,68,80,98,141,101,82,102,65,132,100,103,105,76,106,71,104,187,95,107,99,76,106,129,94,82,51,110,136,103,123,167,60,108,44,123,53,102,91,97,65,134,104,104,90,138,76,85,96,103,74,31,79,57,78,70,71,63,79,85,139,96,50,96,89,118,160,53,133,105,113,97,123,164,87,64,120,99,78,51,60,96,63,98,102,112,146,80,89,116,112,97,59,142,128,162,100,133,127,117,97,81,67,129,137,41,69,57,156,145,85,78,74,85,73,123,177,53,91,73,95,84,122,106,27,85,114,49,117,71,92,48,54,80,147,68,113,102,62,100,48,116,83,72,96,97,76,101,97,112,78,105,60,65,98,110,78,60,130,99,126,85,133,59,96,22,106,85,91,24,132,104,85,45,118,85,106,75,75,100,124,110,74,69,116,79,131,100,86,120,60,53,103,73,19,123,83,74,159,95,96,84,43,104,84,110,68,82,72,152,70,56,78,106,85,33,21,87,104,92,84,86,70,53,83,109,103,51,81,103,82,160,139,111,143,109,72,119,52,103,121,60,108,119,76,67,130,73,97,89,79,88,139,54,96,114,95,101,58,146,45,123,74,119,88,85,47,70,79,108,54,97,105,130,93,111,130,116,106,100,105,16,123,109,85,79,124,114,46,71,137,74,113,94,125,76,110,38,76,132,60,146,125,147,78,107,52,124,137,68,65,104,69,124,59,108,55,71,124,100,154,84,109,123,90,121,62,163,97,85,110,100,94,104,57,119,78,112,117,75,55,90,93,117,95,112,102,46,96,96,89,144,46,75,109,118,62,57,87,36,84,58,69,138,97,112,124,83,121,139,118,81,42,110,132,111,60,124,134,87,105,42,97,123,87,81,149,116,77,101,100,107,66,96,100,103,77,149,6,57,125,48,59,94,92,90,65,92,64,64,132,125,39,75,59,18,110,83,126,46,92,55,75,114,79,96,137,122,53,86,41,61,137,70,64,84,124,104,43,93,59,124,65,83,47,99,80,86,115,78,89,103,139,106,141,75,80,45,104,94,50,94,137,100,101,158,90,46,97,102,92,116,21,75,110,60,129,46,96,78,177,89,64,123,81,134,33,114,31,108,31,106,36,118,147,122,94,81,89,93,123,136,119,80,86,94,58,115,98,80,81,44,110,56,188,89,75,139,55,129,68,61,97,79,135],"type":"mesh3d","name":"Fitted values","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 4.8: 3D visualization of the fitted <code>newmodel</code> against the <code>ugtests</code> data
</p>
</div>
</div>
<div id="lin-good-fit" class="section level3" number="4.3.3">
<h3><span class="header-section-number">4.3.3</span> Model ‘goodness-of-fit’</h3>
<p>At this point we can further explore the overall summary of our model. As you saw in the previous section, our model summary contains numerous objects of interest, including statistics on the coefficients of our model. We can see what is inside our summary by looking at the names of its contents, and we can then dive in and explore specific objects of interest.</p>
<div class="sourceCode" id="cb257"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb257-1"><a href="linear-reg-ols.html#cb257-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get summary of model</span></span>
<span id="cb257-2"><a href="linear-reg-ols.html#cb257-2" aria-hidden="true" tabindex="-1"></a>newmodel_summary <span class="ot">&lt;-</span> <span class="fu">summary</span>(newmodel)</span>
<span id="cb257-3"><a href="linear-reg-ols.html#cb257-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb257-4"><a href="linear-reg-ols.html#cb257-4" aria-hidden="true" tabindex="-1"></a><span class="co"># see summary contents</span></span>
<span id="cb257-5"><a href="linear-reg-ols.html#cb257-5" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(newmodel_summary)</span></code></pre></div>
<pre><code>##  [1] &quot;call&quot;          &quot;terms&quot;         &quot;residuals&quot;     &quot;coefficients&quot;  &quot;aliased&quot;       &quot;sigma&quot;         &quot;df&quot;           
##  [8] &quot;r.squared&quot;     &quot;adj.r.squared&quot; &quot;fstatistic&quot;    &quot;cov.unscaled&quot;</code></pre>
<div class="sourceCode" id="cb259"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb259-1"><a href="linear-reg-ols.html#cb259-1" aria-hidden="true" tabindex="-1"></a><span class="co"># view r-squared</span></span>
<span id="cb259-2"><a href="linear-reg-ols.html#cb259-2" aria-hidden="true" tabindex="-1"></a>newmodel_summary<span class="sc">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 0.5296734</code></pre>
<p>We can see that our model explains more than half of the variance in the Final examination score. Alternatively, we can view the entire summary to receive a formatted report on our model.</p>
<div class="sourceCode" id="cb261"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb261-1"><a href="linear-reg-ols.html#cb261-1" aria-hidden="true" tabindex="-1"></a><span class="co"># see full model summary</span></span>
<span id="cb261-2"><a href="linear-reg-ols.html#cb261-2" aria-hidden="true" tabindex="-1"></a>newmodel_summary</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Final ~ Yr3 + Yr2, data = ugtests)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -91.12 -20.36  -0.22  18.94  98.29 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 18.08709    4.30701   4.199 2.92e-05 ***
## Yr3          0.86496    0.02914  29.687  &lt; 2e-16 ***
## Yr2          0.43236    0.03250  13.303  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 30.44 on 972 degrees of freedom
## Multiple R-squared:  0.5297, Adjusted R-squared:  0.5287 
## F-statistic: 547.3 on 2 and 972 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>This provides us with some of the most important metrics from our model. In particular, the last line gives us a report on our overall model confidence or ‘goodness-of-fit’—this is a hypothesis test on the null hypothesis that our model does not fit the data any better than a random model. A high F-statistic indicates a strong likelihood that the model fits the data better than a random model. More intuitively, perhaps, we also have the p-value for the F-statistic. In this case it is extremely small, so we can reject the null hypothesis and conclude that our model has significant explanatory power over and above a random model.</p>
<p>Be careful not to confuse model goodness-of-fit with <span class="math inline">\(R^2\)</span>. Depending on your sample, it is entirely possible for a model with a low <span class="math inline">\(R^2\)</span> to have high certainty for goodness-of-fit and vice versa.</p>
</div>
<div id="making-predictions-from-your-model" class="section level3" number="4.3.4">
<h3><span class="header-section-number">4.3.4</span> Making predictions from your model</h3>
<p>While this book focuses on inferential rather than predictive analytics, we briefly touch here on the mechanics of generating predictions from models. As you might imagine, once the model has been fitted, prediction is a relatively straightforward process. We feed the <code>Yr2</code> and <code>Yr3</code> examination scores into our fitted model, and it applies the coefficients to calculate the predicted outcome. Let’s look at three fictitious students and create a dataframe with their scores to input into the model.</p>
<div class="sourceCode" id="cb263"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb263-1"><a href="linear-reg-ols.html#cb263-1" aria-hidden="true" tabindex="-1"></a>(new_students <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb263-2"><a href="linear-reg-ols.html#cb263-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">Yr2 =</span> <span class="fu">c</span>(<span class="dv">67</span>, <span class="dv">23</span>, <span class="dv">88</span>), </span>
<span id="cb263-3"><a href="linear-reg-ols.html#cb263-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">Yr3 =</span> <span class="fu">c</span>(<span class="dv">144</span>, <span class="dv">100</span>, <span class="dv">166</span>)</span>
<span id="cb263-4"><a href="linear-reg-ols.html#cb263-4" aria-hidden="true" tabindex="-1"></a>))</span></code></pre></div>
<pre><code>##   Yr2 Yr3
## 1  67 144
## 2  23 100
## 3  88 166</code></pre>
<p>Now we can feed these values into our model to get predictions of the Final examination result for our three new students.</p>
<div class="sourceCode" id="cb265"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb265-1"><a href="linear-reg-ols.html#cb265-1" aria-hidden="true" tabindex="-1"></a><span class="co"># use newmodel to predict for new_students</span></span>
<span id="cb265-2"><a href="linear-reg-ols.html#cb265-2" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(newmodel, new_students)</span></code></pre></div>
<pre><code>##        1        2        3 
## 171.6093 114.5273 199.7179</code></pre>
<p>We know from our earlier work in this chapter that there is a confidence interval around the coefficients of our model, which means that there is a range of values for our prediction according to those confidence intervals. This can be determined by specifying that you require a confidence interval for your predictions.</p>
<div class="sourceCode" id="cb267"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb267-1"><a href="linear-reg-ols.html#cb267-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get a confidence interval </span></span>
<span id="cb267-2"><a href="linear-reg-ols.html#cb267-2" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(newmodel, new_students, <span class="at">interval =</span> <span class="st">&quot;confidence&quot;</span>)</span></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 171.6093 168.2125 175.0061
## 2 114.5273 109.7081 119.3464
## 3 199.7179 195.7255 203.7104</code></pre>
<p>You may also recall from Chapter <a href="inf-model.html#inf-model">1</a> that any observation in our outcome is subject to uncontrollable error, so that there is a further margin of ‘prediction error’‍, even after we take into consideration the confidence interval of our fitted model. Therefore to generate a more reliable prediction range to use in real life, which takes this random, uncontrollable error into consideration, you should calculate a ‘prediction interval’‍.</p>
<div class="sourceCode" id="cb269"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb269-1"><a href="linear-reg-ols.html#cb269-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get a prediction interval </span></span>
<span id="cb269-2"><a href="linear-reg-ols.html#cb269-2" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(newmodel, new_students, <span class="at">interval =</span> <span class="st">&quot;prediction&quot;</span>)</span></code></pre></div>
<pre><code>##        fit       lwr      upr
## 1 171.6093 111.77795 231.4406
## 2 114.5273  54.59835 174.4562
## 3 199.7179 139.84982 259.5860</code></pre>
<p>As discussed in Chapter <a href="inf-model.html#inf-model">1</a>, the process of developing a model to <em>predict</em> an outcome can be quite different from developing a model to <em>explain</em> an outcome. For a start, it is unlikely that you would use your entire sample to fit a predictive model, as you would want to reserve a portion of your data to test for its fit on new data. Since the focus of this book is inferential modeling, much of this topic will be out of our scope.</p>
</div>
</div>
<div id="managing-inputs-in-linear-regression" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Managing inputs in linear regression</h2>
<p>Our walkthrough example for this chapter, while useful for illustrating the key concepts, is a very straightforward data set to run a model on. There is no missing data, and all the data inputs have the same numeric data type (in the exercises at the end of this chapter we will present a more varied data set for analysis). Commonly, an analyst will have a list of possible input variables that they can consider in their model, and rarely will they run a model using all of these variables. In this section we will cover some common elements of decision making and design of input variables in regression models.</p>
<div id="relevance-of-input-variables" class="section level3" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> Relevance of input variables</h3>
<p>The first step in managing your input variables is to make a judgment about their relevance to the outcome being modeled. Analysts should not blindly run a model on a set of variables before considering their relevance. There are two common reasons for rejecting the inclusion of an input variable:</p>
<ol style="list-style-type: decimal">
<li><p>There is no reasonable possibility of a direct or indirect causal relationship between the input and the outcome. For example, if you were provided with the height of each individual taking the Final examination in our walkthrough example, it would be difficult to see how that could reasonably relate to the outcome that you are modeling.</p></li>
<li><p>If there is a possibility that the model will be used to predict based on new data in the future, there may be variables that you explicitly do not wish to be used in any prediction. For example, if our walkthrough model contained student gender data, we would not want to include that in a model that predicted future student scores because we would not want gender to be taken into consideration when determining student performance.</p></li>
</ol>
</div>
<div id="sparseness-missingness-of-data" class="section level3" number="4.4.2">
<h3><span class="header-section-number">4.4.2</span> Sparseness (‘missingness’) of data</h3>
<p>Missing data is a very common problem in modeling. If an observation has missing data in a variable that is being included in the model, that observation will be ignored, or an error will be thrown. This forces a model trained on a smaller set of data, which can compromise its powers of inference. Running summary functions on your data (such as <code>summary()</code> in R) will reveal variables that contain missing data if they exist.</p>
<p>There are three main options for how missing data is handled:</p>
<ol style="list-style-type: decimal">
<li><p>If the data for a given variable is relatively complete and only a small number of observations are missing, it’s usually best and simplest to remove the observations that are missing from the data set. Note that many modeling functions (though not all) will take care of this automatically.</p></li>
<li><p>As data becomes more sparse, removing observations becomes less of an option. If the sparseness is massive (for example, more than half of the data is missing), then there is no choice but to remove that variable from the model. While this may be unsatisfactory for a given variable (because it is thought to have an important explanatory role), the fact remains that data that is mostly missing is not a good measure of a construct in the first place.</p></li>
<li><p>Moderate sparse data could be considered for imputation. Imputation methods involve using the overall statistical properties of the entire data set or of specific other variables to ‘suggest’ what the missing value might be, ranging from simple mean and median values to more complex imputation methods. Imputation methods are more commonly used in predictive settings, and we will not cover imputation methods in depth here.</p></li>
</ol>
</div>
<div id="transforming-categorical-inputs-to-dummy-variables" class="section level3" number="4.4.3">
<h3><span class="header-section-number">4.4.3</span> Transforming categorical inputs to dummy variables</h3>
<p>Many models will have categorical inputs rather than numerical inputs. Categorical inputs usually take forms such as:</p>
<ul>
<li>Binary values—for example, Yes/No, True/False</li>
<li>Unordered categories—for example Car, Train, Bicycle</li>
<li>Ordered categories—for example Low, Medium, High</li>
</ul>
<p>Categorical variables do not behave like numerical variables. There is no sense of quantity in a categorical variable. We do not know how a Car relates to a Train quantitatively, we only know that they are different. Even for an ordered category, although we know that ‘Medium’ is higher than ‘Low’‍, we do not know how much higher or indeed whether the difference is the same as that between ‘High’ and ‘Medium’‍.</p>
<p>In general, all model input variables should take a numeric form. The most reliable way to do this is to convert categorical values to dummy variables. While some packages and functions have a built-in ability to convert categorical data to dummy variables, not all do, so it is important to know how to do this yourself. Consider the following data set:</p>
<div class="sourceCode" id="cb271"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb271-1"><a href="linear-reg-ols.html#cb271-1" aria-hidden="true" tabindex="-1"></a>(vehicle_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb271-2"><a href="linear-reg-ols.html#cb271-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">make =</span> <span class="fu">c</span>(<span class="st">&quot;Ford&quot;</span>, <span class="st">&quot;Toyota&quot;</span>, <span class="st">&quot;Audi&quot;</span>), </span>
<span id="cb271-3"><a href="linear-reg-ols.html#cb271-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">manufacturing_cost =</span> <span class="fu">c</span>(<span class="dv">15000</span>, <span class="dv">19000</span>, <span class="dv">28000</span>)</span>
<span id="cb271-4"><a href="linear-reg-ols.html#cb271-4" aria-hidden="true" tabindex="-1"></a>))</span></code></pre></div>
<pre><code>##     make manufacturing_cost
## 1   Ford              15000
## 2 Toyota              19000
## 3   Audi              28000</code></pre>
<p>The <code>make</code> data is categorical, so it will be converted to several columns for each possible value of <code>make</code>, and binary labeling will be used to identify whether that value is present in that specific observation. Many packages and functions are available to conveniently do this, for example:</p>
<div class="sourceCode" id="cb273"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb273-1"><a href="linear-reg-ols.html#cb273-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dummies)</span>
<span id="cb273-2"><a href="linear-reg-ols.html#cb273-2" aria-hidden="true" tabindex="-1"></a>(dummy_vehicle <span class="ot">&lt;-</span> dummies<span class="sc">::</span><span class="fu">dummy</span>(<span class="st">&quot;make&quot;</span>, <span class="at">data =</span> vehicle_data))</span></code></pre></div>
<pre><code>##   makeAudi makeFord makeToyota
## 1        0        1          0
## 2        0        0          1
## 3        1        0          0</code></pre>
<p>Dummy variables can then replace the original <code>make</code> column to get your data set ready for modeling.</p>
<div class="sourceCode" id="cb275"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb275-1"><a href="linear-reg-ols.html#cb275-1" aria-hidden="true" tabindex="-1"></a>(vehicle_data_dummies <span class="ot">&lt;-</span> <span class="fu">cbind</span>(</span>
<span id="cb275-2"><a href="linear-reg-ols.html#cb275-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">manufacturing_cost =</span> vehicle_data<span class="sc">$</span>manufacturing_cost,</span>
<span id="cb275-3"><a href="linear-reg-ols.html#cb275-3" aria-hidden="true" tabindex="-1"></a>  dummy_vehicle</span>
<span id="cb275-4"><a href="linear-reg-ols.html#cb275-4" aria-hidden="true" tabindex="-1"></a>))</span></code></pre></div>
<pre><code>##   manufacturing_cost makeAudi makeFord makeToyota
## 1              15000        0        1          0
## 2              19000        0        0          1
## 3              28000        1        0          0</code></pre>
<p>It is worth a moment to consider how to interpret coefficients of dummy variables in a linear regression model. Note that all observations will have one of the dummy variable values (all cars must have a make). Therefore the model will assume a ‘reference value’ for the categorical variable—often this is the first value in alphabetical or numerical order. In this case, <code>Audi</code> would be the reference dummy variable. The model then calculates the effect on the outcome variable of a ‘switch’ from <code>Audi</code> to one of the other dummies<a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a>. If we were to try to use the data in our <code>vehicle_data_dummies</code> data set to explain the retail price of a vehicle, we would interpret coefficients like this:</p>
<ul>
<li>Comparing two cars of the same make, we would expect each <em>extra dollar</em> spent on manufacturing to change the retail price by …</li>
<li>Comparing a Ford with an Audi <em>of the same manufacturing cost</em>, we would expect a difference in retail price of …</li>
<li>Comparing a Toyota with an Audi <em>of the same manufacturing cost</em>, we would expect a difference in retail price of …</li>
</ul>
<p>This highlights the importance of appropriate interpretation of coefficients, and in particular the proper understanding of units. It will be common to see much larger coefficients for dummy variables in regression models because they represent a binary ‘all’ or ‘nothing’ variable in the model. The coefficient for manufacturing cost would be much smaller because a unit in this case is a dollar of manufacturing spend, on a scale of many thousands of potential dollars in spend. Care should be taken not to ‘rank’ coefficients by their value. Higher coefficients in and of themselves do not imply greater importance<a href="#fn20" class="footnote-ref" id="fnref20"><sup>20</sup></a>.</p>
</div>
</div>
<div id="testing-your-model-assumptions" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Testing your model assumptions</h2>
<p>All modeling techniques have underlying assumptions about the data that they model and can generate inaccurate results when those assumptions do not hold true. Conscientious analysts will verify that these assumptions are satisfied before finalizing their modeling efforts. In this section we will outline some common checks of model assumptions when running linear regression models.</p>
<div id="assumption-of-linearity-and-additivity" class="section level3" number="4.5.1">
<h3><span class="header-section-number">4.5.1</span> Assumption of linearity and additivity</h3>
<p>Linear regression assumes that the relationship we are trying to model is linear and additive in nature. Therefore you can expect problems if you are using this approach to model a pattern that is not linear or additive.</p>
<p>You can check whether your linearity assumption was reasonable in a couple of ways. You can plot the true versus the predicted (fitted) values to see if they look correlated. You can see such a plot on our student examination model in Figure <a href="linear-reg-ols.html#fig:predplot">4.9</a>.</p>
<div class="sourceCode" id="cb277"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb277-1"><a href="linear-reg-ols.html#cb277-1" aria-hidden="true" tabindex="-1"></a>predicted_values <span class="ot">&lt;-</span> newmodel<span class="sc">$</span>fitted.values</span>
<span id="cb277-2"><a href="linear-reg-ols.html#cb277-2" aria-hidden="true" tabindex="-1"></a>true_values <span class="ot">&lt;-</span> ugtests<span class="sc">$</span>Final</span>
<span id="cb277-3"><a href="linear-reg-ols.html#cb277-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb277-4"><a href="linear-reg-ols.html#cb277-4" aria-hidden="true" tabindex="-1"></a><span class="co"># plot true values against predicted values</span></span>
<span id="cb277-5"><a href="linear-reg-ols.html#cb277-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(predicted_values, true_values)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:predplot"></span>
<img src="_main_files/figure-html/predplot-1.png" alt="Plot of true versus fitted/predicted student scores" width="672" />
<p class="caption">
Figure 4.9: Plot of true versus fitted/predicted student scores
</p>
</div>
<p>Alternatively, you can plot the residuals of your model against the predicted values and look for the pattern of a random distribution (that is, no major discernible pattern) such as in Figure <a href="linear-reg-ols.html#fig:resplot">4.10</a>.</p>
<div class="sourceCode" id="cb278"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb278-1"><a href="linear-reg-ols.html#cb278-1" aria-hidden="true" tabindex="-1"></a>residuals <span class="ot">&lt;-</span> newmodel<span class="sc">$</span>residuals</span>
<span id="cb278-2"><a href="linear-reg-ols.html#cb278-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb278-3"><a href="linear-reg-ols.html#cb278-3" aria-hidden="true" tabindex="-1"></a><span class="co"># plot residuals against predicted values</span></span>
<span id="cb278-4"><a href="linear-reg-ols.html#cb278-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(predicted_values, residuals)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:resplot"></span>
<img src="_main_files/figure-html/resplot-1.png" alt="Plot of residuals against fitted/predicted scores" width="672" />
<p class="caption">
Figure 4.10: Plot of residuals against fitted/predicted scores
</p>
</div>
<p>You can also plot the residuals against each input variable as an extra check of independent randomness, looking for a reasonably random distribution in all cases. If you find that your residuals are following a clear pattern and are not random in nature, this is an indication that a linear model is not a good choice for your data.</p>
</div>
<div id="lin-reg-const-var" class="section level3" number="4.5.2">
<h3><span class="header-section-number">4.5.2</span> Assumption of constant error variance</h3>
<p>It is assumed in a linear model that the errors or residuals are <em>homoscedastic</em>—this means that their variance is constant across the values of the input variables. If the errors of your model are <em>heteroscedastic</em>—that is, if they increase or decrease according to the value of the model inputs—this can lead to poor estimations and inaccurate inferences.</p>
<p>While a simple plot of residuals against predicted values (such as in Figure <a href="linear-reg-ols.html#fig:resplot">4.10</a>) can give a quick indication on homoscedacity, to be thorough the residuals should be plotted against each input variable, and it should be verified that the range of the residuals remains broadly stable. In our student examination model, we can first plot the residuals against the values of <code>Yr2</code> in Figure <a href="linear-reg-ols.html#fig:resyr2">4.11</a>.</p>
<div class="sourceCode" id="cb279"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb279-1"><a href="linear-reg-ols.html#cb279-1" aria-hidden="true" tabindex="-1"></a>Yr2 <span class="ot">&lt;-</span> ugtests<span class="sc">$</span>Yr2</span>
<span id="cb279-2"><a href="linear-reg-ols.html#cb279-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb279-3"><a href="linear-reg-ols.html#cb279-3" aria-hidden="true" tabindex="-1"></a><span class="co"># plot residuals against Yr2 values</span></span>
<span id="cb279-4"><a href="linear-reg-ols.html#cb279-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Yr2, residuals)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:resyr2"></span>
<img src="_main_files/figure-html/resyr2-1.png" alt="Plot of residuals against `Yr2` values" width="672" />
<p class="caption">
Figure 4.11: Plot of residuals against <code>Yr2</code> values
</p>
</div>
<p>We see a pretty consistent range of values for the residuals in <a href="linear-reg-ols.html#fig:resyr2">4.11</a>. Similarly we can plot the residuals against the values of <code>Yr3</code>, as in Figure <a href="linear-reg-ols.html#fig:resyr3">4.12</a>.</p>
<div class="sourceCode" id="cb280"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb280-1"><a href="linear-reg-ols.html#cb280-1" aria-hidden="true" tabindex="-1"></a>Yr3 <span class="ot">&lt;-</span> ugtests<span class="sc">$</span>Yr3</span>
<span id="cb280-2"><a href="linear-reg-ols.html#cb280-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb280-3"><a href="linear-reg-ols.html#cb280-3" aria-hidden="true" tabindex="-1"></a><span class="co"># plot residuals against Yr3 values</span></span>
<span id="cb280-4"><a href="linear-reg-ols.html#cb280-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Yr3, residuals)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:resyr3"></span>
<img src="_main_files/figure-html/resyr3-1.png" alt="Plot of residuals against `Yr3` values" width="672" />
<p class="caption">
Figure 4.12: Plot of residuals against <code>Yr3</code> values
</p>
</div>
<p>Figure <a href="linear-reg-ols.html#fig:resyr3">4.12</a> also shows a consistent range of values for the residuals, which reassures us that we have homoscedacity.</p>
</div>
<div id="norm-dist-assum" class="section level3" number="4.5.3">
<h3><span class="header-section-number">4.5.3</span> Assumption of normally distributed errors</h3>
<p>In an appropriate model we expect our errors to be random, so we would therefore expect our residuals to be normally distributed over sufficient numbers of observations. If our residuals are distributed differently, this is again an indicator of an inappropriate model and can result in inaccurate estimates of confidence intervals and the statistical significance of coefficients.</p>
<p>The quickest way to determine if residuals in your sample are consistent with a normal distribution is to run a quantile-quantile plot (or Q-Q plot) on the residuals. This will plot the observed quantiles of your sample against the theoretical quantiles of a normal distribution. The closer this plot looks like a perfect correlation, the more certain you can be that this normality assumption holds. An example for our student examination model is in Figure <a href="linear-reg-ols.html#fig:qqplottests">4.13</a>.</p>
<div class="sourceCode" id="cb281"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb281-1"><a href="linear-reg-ols.html#cb281-1" aria-hidden="true" tabindex="-1"></a><span class="co"># normal distribution qqplot of residuals</span></span>
<span id="cb281-2"><a href="linear-reg-ols.html#cb281-2" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(newmodel<span class="sc">$</span>residuals)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:qqplottests"></span>
<img src="_main_files/figure-html/qqplottests-1.png" alt="Quantile-quantile plot of residuals" width="672" />
<p class="caption">
Figure 4.13: Quantile-quantile plot of residuals
</p>
</div>
</div>
<div id="collinearity" class="section level3" number="4.5.4">
<h3><span class="header-section-number">4.5.4</span> Avoiding high collinearity and multicollinearity between input variables</h3>
<p>In multiple linear regression, the various input variables used can be considered ‘dimensions’ of the problem or model. In theory, we ideally expect dimensions to be independent and uncorrelated. Practically speaking, however, it’s very challenging in large data sets to ensure that every input variable is completely uncorrelated from another. For example, even in our limited <code>ugtests</code> data set we saw in Figure <a href="linear-reg-ols.html#fig:pairplot-ugtests">4.2</a> that <code>Yr2</code> and <code>Yr3</code> examination scores are correlated to some degree.</p>
<p>While some correlation between input variables can be expected and tolerated in linear regression models, high levels of correlation can result in significant inflation of coefficients and inaccurate estimates of p-values of coefficients.</p>
<p><em>Collinearity</em> means that two input variables are highly correlated. The definition of ‘high correlation’ is a matter of judgment, but as a rule of thumb correlations greater than 0.5 might be considered high and greater than 0.7 might be considered extreme. Creating a simple correlation matrix or a pairplot (such as Figure <a href="linear-reg-ols.html#fig:pairplot-ugtests">4.2</a>) can immediately surface high or extreme collinearity.</p>
<p><em>Multicollinearity</em> means that there is a linear relationship between more than two of the input variables. This may not always present itself in the form of high correlations between pairs of input variables, but may be seen by identifying ‘clusters’ of moderately correlated variables, or by calculating a Variance Inflation Factor (VIF) for each input variable—where VIFs greater than 5 indicate high multicollinearity. Easy-to-use tests also exist in statistical software for identifying multicollinearity (for example the <code>mctest</code> package in R). Here is how we would test for multicollinearity in our student examination model.</p>
<div class="sourceCode" id="cb282"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb282-1"><a href="linear-reg-ols.html#cb282-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mctest)</span>
<span id="cb282-2"><a href="linear-reg-ols.html#cb282-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb282-3"><a href="linear-reg-ols.html#cb282-3" aria-hidden="true" tabindex="-1"></a><span class="co"># diagnose possible overall presence of multicollinearity</span></span>
<span id="cb282-4"><a href="linear-reg-ols.html#cb282-4" aria-hidden="true" tabindex="-1"></a>mctest<span class="sc">::</span><span class="fu">omcdiag</span>(newmodel)</span></code></pre></div>
<pre><code>## 
## Call:
## mctest::omcdiag(mod = newmodel)
## 
## 
## Overall Multicollinearity Diagnostics
## 
##                        MC Results detection
## Determinant |X&#39;X|:         0.9981         0
## Farrar Chi-Square:         1.8365         0
## Red Indicator:             0.0434         0
## Sum of Lambda Inverse:     2.0038         0
## Theil&#39;s Method:           -0.5259         0
## Condition Number:          9.1952         0
## 
## 1 --&gt; COLLINEARITY is detected by the test 
## 0 --&gt; COLLINEARITY is not detected by the test</code></pre>
<div class="sourceCode" id="cb284"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb284-1"><a href="linear-reg-ols.html#cb284-1" aria-hidden="true" tabindex="-1"></a><span class="co"># if necessary, diagnose specific multicollinear variables using VIF </span></span>
<span id="cb284-2"><a href="linear-reg-ols.html#cb284-2" aria-hidden="true" tabindex="-1"></a>mctest<span class="sc">::</span><span class="fu">imcdiag</span>(newmodel, <span class="at">method =</span> <span class="st">&quot;VIF&quot;</span>)</span></code></pre></div>
<pre><code>## 
## Call:
## mctest::imcdiag(mod = newmodel, method = &quot;VIF&quot;)
## 
## 
##  VIF Multicollinearity Diagnostics
## 
##        VIF detection
## Yr3 1.0019         0
## Yr2 1.0019         0
## 
## NOTE:  VIF Method Failed to detect multicollinearity
## 
## 
## 0 --&gt; COLLINEARITY is not detected by the test
## 
## ===================================</code></pre>
<p>Note that collinearity and multicollinearity only affect the coefficients of the variables impacted, and do not affect other variables or the overall statistics and fit of a model. Therefore, if a model is being developed primarily to make predictions and there is little interest in using the model to explain a phenomenon, there may not be any need to address this issue at all. However, in inferential modeling the accuracy of the coefficients is very important, and so testing of multicollinearity is essential. In general, the best way to deal with collinear variables is to remove one of them from the model (usually the one that has the least significance in explaining the outcome).</p>
</div>
</div>
<div id="extending-multiple-linear-regression" class="section level2" number="4.6">
<h2><span class="header-section-number">4.6</span> Extending multiple linear regression</h2>
<p>We wrap up this chapter by introducing some simple extensions of linear regression, with a particular aim of trying to improve the overall fit of a model by relaxing the linear or additive assumptions. It is rare for practitioners to extend linear regression models too greatly due to the negative impact this can have on interpretation, but simple extensions such as experimenting with interaction terms or quadratics are not uncommon. If you have an appetite to explore this topic more fully, I recommend <span class="citation"><a href="#ref-rao" role="doc-biblioref">Rao et al.</a> (<a href="#ref-rao" role="doc-biblioref">2008</a>)</span>.</p>
<div id="interactions-between-input-variables" class="section level3" number="4.6.1">
<h3><span class="header-section-number">4.6.1</span> Interactions between input variables</h3>
<p>Recall that our model of student examination scores took each year’s score as an independent input variable, and therefore we are making the assumption that the score obtained in each year acts independently and additively in predicting the Final score. However, it is very possible that several input variables act together in relation to the outcome. One way of modeling this is to include interaction terms in your model, which are new input variables formed as products of the original input variables.</p>
<p>In our student examination data in <code>ugtests</code>, we could consider extending our model to not only include the individual year examinations, but also to include the impact of combined changes across multiple years. For example, we could combine the impact of <code>Yr2</code> and <code>Yr3</code> examinations by multiplying them together in our model.</p>
<div class="sourceCode" id="cb286"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb286-1"><a href="linear-reg-ols.html#cb286-1" aria-hidden="true" tabindex="-1"></a>interaction_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">data =</span> ugtests, </span>
<span id="cb286-2"><a href="linear-reg-ols.html#cb286-2" aria-hidden="true" tabindex="-1"></a>                        <span class="at">formula =</span> Final <span class="sc">~</span> Yr2 <span class="sc">+</span> Yr3 <span class="sc">+</span> Yr2<span class="sc">*</span>Yr3)</span>
<span id="cb286-3"><a href="linear-reg-ols.html#cb286-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(interaction_model)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Final ~ Yr2 + Yr3 + Yr2 * Yr3, data = ugtests)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -78.084 -18.284  -0.546  18.395  79.824 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.320e+02  1.021e+01  12.928  &lt; 2e-16 ***
## Yr2         -7.947e-01  1.056e-01  -7.528 1.18e-13 ***
## Yr3         -2.267e-01  9.397e-02  -2.412   0.0161 *  
## Yr2:Yr3      1.171e-02  9.651e-04  12.134  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 28.38 on 971 degrees of freedom
## Multiple R-squared:  0.5916, Adjusted R-squared:  0.5903 
## F-statistic: 468.9 on 3 and 971 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We see that introducing this interaction term has improved the fit of our model from 0.53 to 0.59, and that the interaction term is significant, so we conclude that in addition to a significant effect of the <code>Yr2</code> and <code>Yr3</code> scores, there is an additional significant effect from their interaction <code>Yr2*Yr3</code>. Let’s take a moment to understand how to interpret this, since we note that some of the coefficients are now negative.</p>
<p>Our model now includes two input variables and their interaction, so it can be written as</p>
<p><span class="math display">\[
\begin{aligned}
\mathrm{Final} &amp;= \beta_0 + \beta_1\mathrm{Yr3} + \beta_2\mathrm{Yr2} + \beta_3\mathrm{Yr3}\mathrm{Yr2} \\
&amp;= \beta_0 + (\beta_1 + \beta_3\mathrm{Yr2})\mathrm{Yr3} + \beta_2\mathrm{Yr2} \\
&amp;= \beta_0 + \gamma\mathrm{Yr3} + \beta_2\mathrm{Yr2}
\end{aligned}
\]</span>
where <span class="math inline">\(\gamma = \beta_1 + \beta_3\mathrm{Yr2}\)</span>. Therefore our model has coefficients which are not constant but change with the values of the input variables. We can conclude that the effect of an extra point in the examination in Year 3 will be different depending on how the student performed in Year 2. Visualizing this, we can see in Interactive Figure <a href="linear-reg-ols.html#fig:non-lin-reg-3d">4.14</a> that this non-constant term introduces a curvature to our fitted surface that aligns it a little more closely with the observations in our data set.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:non-lin-reg-3d"></span>
<div id="htmlwidget-5e3dc287febbda5a3058" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-5e3dc287febbda5a3058">{"x":{"visdat":{"761031aa61d0":["function () ","plotlyVisDat"]},"cur_data":"761031aa61d0","attrs":{"761031aa61d0":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"mode":"markers","type":"scatter3d","marker":{"size":5,"color":"blue","symbol":104},"name":"Observations","inherit":true},"761031aa61d0.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":[110.883620462442,174.210227972972,132.198604385871,147.296020259034,121.396235926407,178.051565182717,170.542221523676,125.202920609548,163.677688809768,115.492356065544,107.366163303924,148.941703397618,167.726229456888,172.70565654595,151.932093504083,190.231078633566,142.540992056498,170.822803182668,146.790954826973,188.635905193597,155.804229046644,120.098340252428,162.1476575195,165.985731779488,199.619096320761,137.905264905464,197.771439553371,182.527187245654,162.593609386008,113.522684308068,147.722175873198,153.86652633948,134.584850673952,152.05118749153,146.711043737164,193.362434452964,138.039370535423,189.860938466393,154.070356092648,216.283617577953,157.881870393591,143.264017856386,130.571787130256,195.295171006643,249.103815983409,193.086288815513,102.525890390572,193.83692542205,172.227636524409,120.438598425491,182.174466444136,118.997183035969,108.679925893694,120.221919790444,140.958085670659,131.459332712939,139.243874880936,98.919720861699,113.737511246482,165.274591301364,165.00157207799,164.227103388335,122.329028255062,179.519288412595,164.861049162845,119.079982686009,157.064322426314,170.521939819857,154.825523023553,150.348317860956,209.457904861883,135.907122546999,148.849508868725,142.668423387358,122.47864491635,134.625385786076,147.865867435276,100.972503052379,146.208025529395,121.866474537716,128.824935774034,139.336896387595,164.176129565088,113.636496370312,187.612875871902,143.226929357786,205.984325990732,143.173646681946,125.270861537713,135.225816841576,134.907831005596,174.899918196552,148.041542265843,159.233456323677,140.249096135938,140.853927948793,128.804950916037,144.406732362895,155.063496498226,167.19633311741,128.765702104417,130.304768148851,140.327885849771,211.57122585525,151.932093504083,169.307875436792,118.257840788,134.805609820242,129.755812846357,112.282196501642,91.2474852466573,123.424262633264,117.946854111619,134.537488295428,172.15186705793,155.162744610971,224.018863278526,108.22709473748,224.904738427393,137.224937071662,123.540289245275,116.377805082412,162.840516390016,106.61028036387,149.836024597562,242.923729326342,111.530973218796,107.767803735724,151.712559499544,128.824935774034,143.145340912155,200.881163902076,170.310354504359,141.122049473607,146.820714176077,129.753108324055,165.302934829618,110.338302499243,139.381737954248,129.079470846806,143.068951859466,176.719945231397,136.734369628453,161.708170066259,144.118541186869,115.7439040817,115.723780240407,201.937295467289,118.29882816086,145.024665176317,132.333091748196,88.5874918621574,146.883540504515,171.994907216476,143.45012447607,194.951251719836,102.249210053453,123.220969980711,124.33322075869,254.491491842721,116.129378538444,185.553838826287,174.484380529546,139.360391465482,236.980124899503,155.525777050876,157.307085359379,171.743083634676,139.514877623145,119.005852863555,171.062611922355,108.030704774924,216.693171434609,142.050295046277,119.084468283242,125.661177241984,150.689706919607,116.71012864377,113.066258092087,178.032414317822,106.208359517867,139.371426362525,103.354402445686,115.965202729552,113.61685793383,126.605223676468,163.515548361273,186.48935935837,180.309503539813,160.013470546953,118.376663730862,142.715785765883,142.7918426619,112.162358022025,154.374371717438,102.35001998568,129.893296634915,115.450991808878,183.512720535145,156.724525930271,114.756347249041,143.073769613372,128.541258444092,207.781802675239,103.521723454556,172.141618979243,169.706309509861,110.335946565233,155.57703133499,142.0863186666,144.115754225637,155.567803407446,95.6119669064128,125.630039737161,109.213709290203,168.60303698835,139.637083619831,150.285491532518,139.909346767976,146.208265830853,177.932619641514,128.663092218026,173.610084347009,156.930108509522,146.297414264489,171.034918676774,116.640866150915,103.978825801112,157.319898930408,151.82196138815,155.964578957259,154.8615512116,130.014901971286,118.472026953354,163.677688809768,204.992900699439,127.094066635774,232.881979075241,223.138543079564,114.444352285413,120.829211022248,172.536811429395,167.870069418545,220.350121660016,141.116499044762,159.835378623623,182.204067930714,132.023172257709,139.950981975897,156.151387508755,148.305345472234,205.425977882831,137.320217855223,213.116563232904,136.761348984996,109.816062851543,176.260564775373,148.8062127366,165.136860242677,176.125453352443,115.603037192653,143.170463723506,115.814337526223,110.820271017403,160.249008096298,141.271861428384,175.11087455622,214.238718069003,127.314886940817,136.059912469609,177.15338282031,168.60303698835,169.64183407445,181.065930876654,133.746670013617,136.356318530701,158.770582079383,161.349528967426,152.288030127279,156.383257022349,92.3410541329323,196.723562939305,90.3216215025651,142.844477502679,133.331909748684,108.354266934316,114.847526522016,124.985146399762,107.610346906399,165.985731779488,132.765459610273,177.786555713807,218.706049916607,218.634912045993,114.511320237181,169.706309509861,145.291267161171,122.806680855745,113.066258092087,240.173612084197,157.674016673544,125.909177326453,164.393025054638,202.45070073704,99.2641982951291,142.822473715904,117.17991739529,157.277749975497,193.209060254994,114.831523023876,120.226652704471,126.886316634837,181.559313576611,155.123052908233,161.597331123289,125.738376794155,101.719912020007,156.243947338395,177.601440949752,248.816442322202,168.809024700256,158.385972312203,109.769256452865,187.985784074412,135.998011989488,116.645813471833,241.604654395743,120.631765644362,185.759235294162,162.539650579593,182.345601534054,144.663929725111,173.677346696987,134.204742935624,170.420024943273,201.621969426477,171.466151178869,232.902621184582,163.764344773711,102.518525976891,147.448393185093,137.486415040505,134.570786159935,177.739193335283,106.486125966777,156.309452341232,182.945087908671,142.616306814628,141.57297940416,110.770538674196,135.02012352788,151.699458498976,108.416553714527,191.238050267039,181.767508869613,142.339569933145,159.835378623623,160.641724415046,176.248724180742,236.545297337641,160.08421971653,128.524673118203,186.831879255945,155.79097018355,136.78667231256,127.129856969975,159.767206856948,148.247763310536,196.891168976767,194.951251719836,186.561145064044,134.365825521178,164.295722894688,155.637643160325,215.250648933953,128.098991044954,111.839669968479,189.746118163566,160.551918683401,87.7994629425273,137.118805148152,108.324514600548,105.791709844503,160.208545960157,100.297162371405,160.190999521766,145.563631580813,120.856223522865,139.658032037942,121.107961937285,168.741503216253,180.579886742476,154.713037374557,157.089662138832,203.949099701392,115.794213684931,145.387141636375,146.472005430879,163.603382292221,172.549754567436,210.160048250972,148.173532216585,115.955204415271,162.372131876287,115.652418780791,108.711948994094,105.635362573923,175.397960787888,144.751447931006,134.231741171398,210.748407873331,125.014239034575,101.040071711127,138.567952325548,118.78564272844,232.062501540124,127.358027658027,110.149748090335,149.167579591248,159.787043268701,117.853333169477,128.832458050242,163.18126702989,186.398681921825,187.224371704855,116.367820705473,138.273685390628,120.865993212078,194.58215999932,135.02012352788,108.731740772043,129.351635171065,158.095707970653,119.262171505536,146.208025529395,215.083655467367,217.912479984414,110.906161396503,197.689137218702,86.3708493068033,130.262009117342,194.368638147311,126.917279798849,114.717710961631,250.020659238176,86.9549565713859,125.995682443205,184.88504984607,120.22546772213,144.421769899973,151.618602728285,146.280042073579,114.544804213065,164.598718368335,165.212620292872,153.044309017876,138.688389111198,211.563397316938,123.60011420559,168.210226366774,107.263963398748,118.672596251039,132.764833102056,141.877605105548,133.96857643045,110.007505493182,165.288566454444,130.595128960004,156.565575408888,150.592173921147,146.554529187774,145.857926811248,210.072409894348,157.846588911155,144.672519233878,146.024920277834,122.928064723225,197.403674246104,131.794382076776,145.121383060907,137.381098230866,182.524407346422,137.586977655938,147.865867435276,128.161492232055,167.726229456888,130.028104243351,129.294135448299,95.7173284431857,180.35582938224,138.033386678407,214.77628753188,128.610624656057,76.0719512691785,157.23171401022,218.202504024506,122.639383528001,126.677963525973,191.203305885218,157.85598411751,117.94553254693,124.505352225295,122.960528594631,124.132484182195,172.990537690799,149.280649515605,121.499309937429,130.959483152103,129.247208898892,127.446749725494,150.401155291395,120.292313122223,129.546432571013,167.865510798663,151.762442689941,142.153665903121,17.9806296986207,144.607336924765,80.8281808856741,100.668617041266,170.467971597158,117.612850746827,142.236319227029,240.472911226578,179.271493272069,137.040441847153,140.314904999933,144.689048015403,161.314784585606,127.004788588461,142.823093255451,126.575565645526,105.913447148833,98.2845901515122,155.426724512455,144.937851396512,131.615927300312,198.352069461303,163.18126702989,163.139815532398,168.807498191624,130.343310180099,137.070043333731,111.032482934447,199.458079695854,237.089267654085,120.486285992017,245.861706014255,114.231706706027,212.793797354816,161.260140232332,199.700486744456,175.242767756523,155.278446081646,136.058131394677,135.623756046886,107.936210856384,130.304768148851,254.42397732739,168.283364333599,107.45830853796,138.650087381408,139.704339001138,97.464087944193,116.03569978044,137.381098230866,175.913884468565,109.144338510515,97.3954331736543,117.92046138472,140.619438846106,148.4792510451,152.807586532856,145.130472005155,114.438976384884,145.128832314466,165.501431055108,117.318753491664,135.033139688568,130.571787130256,179.651777752155,137.757851164219,115.415387586053,125.198270134452,155.652605273809,114.376913193677,257.106874452414,185.430808252782,122.139571678963,122.375189219627,124.003245881836,211.808421927851,183.763217703696,182.015845631814,206.462633441813,117.612850746827,135.907122546999,158.781284866419,91.2426133493344,141.682899560815,157.193930548479,214.00838000593,153.771334963522,115.581629498469,104.731797061481,98.6784184766186,133.842424338093,125.210704420727,116.757952745979,163.343859739121,236.649436180276,144.360046114945,122.086562167819,123.141058890902,172.353548315308,117.912481952551,120.777245296708,121.296460129329,115.854585208808,118.596447453142,83.7811385196363,144.471356244548,165.88605970152,132.836413770458,210.313422448943,175.111494095767,183.689747626864,129.700770329098,161.632890572315,170.542221523676,174.347323107157,218.634912045993,146.74395285397,110.84325964447,167.66712775523,142.519089541221,153.369419012744,130.875253743871,186.149645254593,189.819135979662,125.210704420727,129.634070927637,82.4279624745536,138.65283903179,116.425803432102,156.80198221552,98.791705255478,188.34167107525,142.972212741362,92.7810291577607,153.503790745398,165.134904826934,121.876018190314,158.349496384478,175.733957233556,107.170913596574,136.595163663609,139.422042227861,116.045761701087,172.717534899044,135.638871455173,131.227576381403,153.75988551754,140.056883107562,148.203977112548,117.556865949633,116.558919654044,140.453288788905,189.079663837181,103.083072395364,190.875460671588,139.319811626224,126.74205967663,131.604237506207,182.272489415131,198.611688764842,139.243874880936,129.228456151317,126.784335750941,177.447678050745,86.599017828078,113.513199413276,125.179884854399,90.968069690775,139.947803959346,189.163031003462,159.820303374747,165.752180389013,140.333304263992,223.488158793848,228.639131000706,163.767555653501,195.620299834077,218.265431624441,132.59018779225,143.995456423283,232.082590117231,148.704900921279,146.711043737164,121.584121220075,156.489490217357,190.649264231846,147.598791909507,104.866034659399,139.142070692133,145.550123046643,171.62581549596,180.773025619168,137.005697465333,161.279104985851,113.01363751615,210.20941317332,105.786199528402,176.254830636098,96.5907081936893,125.909177326453,145.760120367256,112.507160830962,144.614649642641,141.637028426736,117.84985364605,159.441503170436,154.830981643849,184.138639749407,159.820656811598,196.148815616891,175.068162652792,188.040032757978,187.748618744938,104.430608557334,216.014393509729,115.509537484024,143.45012447607,107.274663738172,139.747328917822,116.67384829037,160.866644063898,113.406349033005,118.491764494557,126.942796253125,138.813650666089,112.825813381234,86.0224586876052,101.837432277411,157.578208252317,141.214218154598,189.972741016143,131.236884300267,188.978078622992,106.207614979032,134.730914601659,198.358077092774,88.2934792127024,158.642345097601,115.298581171021,194.728612628064,148.744394602081,117.973574380803,211.834911358524,150.267933183567,125.203811147014,108.784710170442,109.099030371619,219.691425343572,158.89211896083,173.819363304189,148.778333286314,123.313805375994,168.219741723857,176.208931206505,134.142849470839,134.09482277897,146.119145646066,194.429944935788,140.684887257914,200.069934349434,147.610632457473,234.569618042024,160.350821701384,183.568142290492,153.879636756331,157.319898930408,169.14971178043,127.619328931776,124.132484182195,93.4005897979774,120.793053220524,126.493124327561,162.47807768842,152.509427645682,135.002306044904,203.562828916678,112.384563685627,124.741287938624,193.773227388713,140.597915615582,184.462934583739,125.981111431706,135.551640678817,158.889435765372,137.896727420003,132.838145363027,127.541392043611,107.543788935846,115.673470637176,162.948167900151,163.093041950293,123.691606756005,124.609123693736,125.180553922974,112.896717965726,120.712513128221,144.565025586269,164.01314815482,102.154435720711,220.886727562779,204.985230023653,132.333091748196,121.504643558437,95.7550693445702,198.760921339481,163.603382292221,131.464703999079,192.095864796132,206.467156797509,209.028090628252,163.291545144454,165.768709170538,261.76821688161,176.894868507794,199.966285479334,122.182283582391,123.978080463463,178.650476698257,172.141618979243,94.2969417439296,148.673417088271,157.480675253857,120.046039969269,127.227917699432,128.986350516261,126.984311356982,95.3476195840711,131.063603162171,111.986221467773,177.447678050745,122.746431883544,244.325811017701,110.298064186276,125.455116460764,141.494253296692,116.958204151834,136.141444370876,156.911115507154,178.801014359301,147.303617958837,135.3605584427,160.934983109383,127.106204076227,153.771789671871,165.571817371551,178.611072425057,129.722382967225,124.48736294245,126.391155260896,115.241630085265,88.0627995766739,115.935080573978,164.211012930204,134.094570613616,151.576585787998,115.799358981121,145.992668412372,138.899687043821,143.916129608835,137.346855685475,249.034421522595,170.838999854185,149.335736713332,134.636828263387,129.793233408299,142.493405808135,187.03523791581,170.632997784107,121.877657881003,137.123123466576,142.483899914,154.384313487356,129.076017217946,175.345196380097,113.855069262349,184.151453320436,118.254457734176,143.513553912435,121.153816967245,136.294382739729,185.56791035564,134.636828263387,166.018365377315,107.743682102955,102.824624090159,186.263940320701,298.926414531484,182.675029847345,132.446734130683,96.4652982392193,189.829296724194,135.239174528556,131.584511828899,91.3472069003175,126.675230708157,139.427406545331,188.412997551517,173.375576365091,176.948375053472,245.075733781865,125.792743180838,146.264759386483,165.806732887072,166.073334871926,114.993759895309,129.184973861151,117.060210883633,161.632890572315,218.634912045993,130.446820020239,175.823588717721,126.293167554087,88.8760505057053,157.880739554667,231.299906682504,184.387655089795,152.726080432826,160.443397962643,130.394046196165,169.126984734998,123.310125429684,103.430695028772,120.959193814777,144.751447931006,120.822803106681,206.013538776274,126.545174846315,165.258668122044,167.611564568976,168.434811411341,111.453472206414,166.4911506484,97.9308422301809,164.133028913959,165.298618958807,180.972131967921,163.418258111882,116.675497397343,181.466661891757,155.162744610971,105.777094152534,108.892396944763,169.203866161169,120.936883765897,157.827845579863,127.295590030626,187.849617236154,119.210917221422,217.579725208328,125.662272770057,175.969760885596,150.401155291395,107.867706933036,169.403231643947,172.486183653499,140.054798124807,164.897414356124,152.996890048323,193.963440367542],"x":{},"y":{},"type":"mesh3d","name":"Fitted values","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"Yr3"},"yaxis":{"title":"Yr2"},"camera":{"eye":{"x":-0.5,"y":2,"z":0}},"zaxis":{"title":"Final"},"aspectmode":"cube"},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[52,126,148,115,75,119,125,78,119,67,61,150,110,142,134,116,107,143,106,135,111,72,129,148,112,152,166,136,160,63,109,98,111,96,101,141,124,154,138,165,120,93,91,136,176,152,52,144,124,111,140,146,54,80,94,92,91,43,106,161,107,157,72,116,116,72,106,134,135,105,165,97,131,111,82,104,98,49,107,95,87,101,101,63,147,120,157,111,79,102,91,132,107,118,107,108,98,113,128,125,78,99,91,171,134,129,76,98,81,60,44,101,69,103,118,112,135,57,184,102,78,67,133,50,94,174,63,61,115,87,88,136,113,103,102,85,144,61,121,132,99,130,84,124,86,67,67,155,90,110,89,32,103,133,90,146,47,87,85,151,96,140,106,87,156,103,108,121,91,95,116,54,137,87,80,75,98,68,64,120,56,88,60,90,55,100,147,133,146,137,91,119,118,64,99,54,114,66,127,97,61,97,95,143,57,138,133,63,115,93,95,105,41,102,61,112,111,104,105,93,139,79,121,122,117,115,68,55,111,103,100,168,93,70,119,154,84,153,164,65,71,120,119,185,90,125,145,135,129,151,94,139,86,163,126,54,140,88,105,113,67,160,67,61,109,147,123,170,76,94,132,112,132,137,98,84,139,127,97,97,27,156,37,117,84,57,54,81,58,148,142,144,152,164,65,133,107,74,64,165,140,80,111,140,56,102,77,144,149,63,73,89,135,118,95,85,55,131,128,169,151,99,52,158,87,68,175,72,147,152,135,91,151,87,114,143,111,161,106,49,117,90,85,136,56,111,121,120,95,57,93,119,60,145,114,99,125,138,138,158,119,81,118,124,93,103,109,95,136,146,131,91,129,101,142,98,59,127,133,28,87,60,56,140,52,106,114,84,92,174,117,125,116,119,156,67,95,103,133,125,157,105,67,111,56,127,55,137,95,109,138,87,52,109,77,183,100,62,109,116,69,81,125,140,169,118,100,71,143,93,61,85,116,86,107,146,144,55,175,47,82,129,101,63,143,45,75,146,75,102,102,120,65,120,102,106,98,166,81,107,58,70,84,106,85,57,117,75,113,101,100,112,160,100,100,108,85,144,79,97,85,106,112,98,108,110,102,81,43,138,108,141,80,27,117,141,77,81,151,110,70,126,77,85,150,123,76,96,86,82,92,72,104,125,111,90,8,139,41,52,117,69,89,141,147,107,88,98,133,96,123,79,47,48,108,94,78,183,125,109,116,83,112,64,133,140,72,165,66,152,117,183,173,98,82,143,53,99,154,103,56,100,104,43,67,85,100,49,45,70,95,110,98,96,44,105,112,70,84,91,142,131,65,80,94,64,196,139,79,74,90,152,158,174,168,69,97,112,40,91,127,165,146,66,55,51,93,86,68,118,164,104,72,82,130,111,72,74,67,105,43,96,132,122,149,144,134,88,137,125,115,164,115,58,113,105,96,83,157,130,86,101,33,103,104,101,44,180,84,41,109,132,87,110,146,59,103,99,67,104,100,102,122,91,109,74,67,98,141,52,166,97,85,106,156,146,91,102,82,130,27,66,74,44,84,137,138,110,130,177,162,167,144,155,92,117,157,119,101,74,125,160,90,39,93,94,130,135,113,98,56,145,51,119,43,80,158,61,108,92,71,115,106,120,118,142,113,131,138,55,167,95,90,59,98,68,104,65,74,85,84,62,40,51,101,105,137,135,177,60,106,123,45,96,66,144,152,69,137,127,84,60,55,152,159,123,145,79,110,131,79,91,96,132,97,138,130,138,116,155,103,111,128,79,85,48,74,77,108,123,112,173,63,75,155,90,138,98,89,115,94,79,103,54,67,108,161,84,78,112,63,77,116,102,52,164,140,89,72,53,139,133,124,139,136,154,166,108,176,144,151,89,76,119,138,37,89,104,73,79,81,78,29,84,63,130,77,152,108,88,88,69,110,114,141,105,103,120,88,139,105,118,125,80,89,152,41,67,106,124,109,66,113,88,107,91,164,113,151,91,120,121,119,159,78,95,89,106,113,136,62,123,71,104,82,89,165,91,112,55,51,139,198,133,79,37,171,130,83,39,96,94,120,128,131,147,79,163,122,119,66,79,93,137,164,97,114,99,37,128,130,151,137,104,87,107,110,56,100,95,99,156,119,110,102,111,62,137,45,123,108,115,108,68,151,112,44,61,125,74,125,105,135,74,119,79,159,92,47,109,166,116,121,120,125],"y":[50,104,36,75,77,122,100,92,98,127,134,53,123,84,65,150,76,81,87,111,97,92,83,72,180,41,90,102,62,73,84,125,55,127,97,109,52,92,65,107,87,110,71,118,124,97,95,106,104,27,97,22,68,45,99,72,103,80,13,64,125,65,138,131,105,71,110,89,68,97,101,76,63,71,55,62,108,90,84,36,74,78,142,71,96,63,105,72,86,66,87,97,88,92,71,71,54,72,74,95,122,57,107,98,65,93,37,71,105,66,110,37,122,63,114,94,156,85,99,71,78,39,80,67,128,121,108,129,83,74,132,125,121,79,95,85,74,97,56,36,92,102,126,88,149,102,104,103,26,77,83,86,93,92,123,105,78,49,58,161,18,101,149,121,136,114,106,108,104,27,116,72,143,133,37,128,116,101,97,120,94,116,163,17,39,46,70,111,89,73,26,63,64,117,123,107,44,71,116,137,42,97,57,123,122,87,89,129,90,106,106,109,86,42,111,120,65,99,73,120,93,112,111,83,71,118,59,103,100,105,124,52,65,95,98,107,75,136,114,83,135,111,105,95,114,84,92,39,52,60,123,126,117,106,49,61,90,156,131,132,116,44,95,91,110,46,110,101,132,83,100,120,90,99,68,124,70,84,124,136,70,97,92,65,108,84,30,74,97,72,38,88,123,110,81,89,82,106,97,128,68,85,114,121,144,85,25,65,100,50,80,61,102,85,161,65,118,73,106,131,73,134,56,87,106,62,119,103,94,66,103,123,78,98,119,116,128,126,125,83,73,100,109,96,92,98,126,62,98,65,82,78,108,102,140,90,84,73,92,133,92,97,139,79,88,45,109,119,120,105,114,85,86,120,133,52,65,125,77,81,111,109,97,71,107,117,73,42,101,23,110,115,87,87,104,97,110,92,81,103,109,92,81,110,26,8,92,92,108,56,134,57,103,65,40,105,48,113,87,96,115,99,89,102,79,19,77,136,108,82,117,83,93,32,84,127,133,57,84,143,102,128,46,52,171,130,132,94,63,89,107,68,80,98,141,101,82,102,65,132,100,103,105,76,106,71,104,187,95,107,99,76,106,129,94,82,51,110,136,103,123,167,60,108,44,123,53,102,91,97,65,134,104,104,90,138,76,85,96,103,74,31,79,57,78,70,71,63,79,85,139,96,50,96,89,118,160,53,133,105,113,97,123,164,87,64,120,99,78,51,60,96,63,98,102,112,146,80,89,116,112,97,59,142,128,162,100,133,127,117,97,81,67,129,137,41,69,57,156,145,85,78,74,85,73,123,177,53,91,73,95,84,122,106,27,85,114,49,117,71,92,48,54,80,147,68,113,102,62,100,48,116,83,72,96,97,76,101,97,112,78,105,60,65,98,110,78,60,130,99,126,85,133,59,96,22,106,85,91,24,132,104,85,45,118,85,106,75,75,100,124,110,74,69,116,79,131,100,86,120,60,53,103,73,19,123,83,74,159,95,96,84,43,104,84,110,68,82,72,152,70,56,78,106,85,33,21,87,104,92,84,86,70,53,83,109,103,51,81,103,82,160,139,111,143,109,72,119,52,103,121,60,108,119,76,67,130,73,97,89,79,88,139,54,96,114,95,101,58,146,45,123,74,119,88,85,47,70,79,108,54,97,105,130,93,111,130,116,106,100,105,16,123,109,85,79,124,114,46,71,137,74,113,94,125,76,110,38,76,132,60,146,125,147,78,107,52,124,137,68,65,104,69,124,59,108,55,71,124,100,154,84,109,123,90,121,62,163,97,85,110,100,94,104,57,119,78,112,117,75,55,90,93,117,95,112,102,46,96,96,89,144,46,75,109,118,62,57,87,36,84,58,69,138,97,112,124,83,121,139,118,81,42,110,132,111,60,124,134,87,105,42,97,123,87,81,149,116,77,101,100,107,66,96,100,103,77,149,6,57,125,48,59,94,92,90,65,92,64,64,132,125,39,75,59,18,110,83,126,46,92,55,75,114,79,96,137,122,53,86,41,61,137,70,64,84,124,104,43,93,59,124,65,83,47,99,80,86,115,78,89,103,139,106,141,75,80,45,104,94,50,94,137,100,101,158,90,46,97,102,92,116,21,75,110,60,129,46,96,78,177,89,64,123,81,134,33,114,31,108,31,106,36,118,147,122,94,81,89,93,123,136,119,80,86,94,58,115,98,80,81,44,110,56,188,89,75,139,55,129,68,61,97,79,135],"z":[93,207,175,125,114,159,153,84,147,80,154,154,175,182,155,198,161,229,100,179,164,130,194,152,216,123,232,168,151,88,184,134,115,167,170,229,132,245,180,213,211,151,121,180,232,214,123,166,111,90,187,160,125,145,168,114,152,125,143,161,98,183,152,203,158,65,133,122,138,132,174,173,124,139,119,171,160,117,178,122,144,124,244,96,248,194,198,160,124,136,127,193,93,182,114,147,122,172,108,158,168,101,163,197,162,174,137,154,154,126,105,102,92,131,189,101,184,108,211,172,144,88,153,86,171,263,158,146,144,111,81,201,126,150,136,132,176,168,141,153,143,170,110,142,123,136,111,248,97,100,106,88,113,186,146,204,79,116,59,289,70,183,189,83,171,169,110,113,124,129,174,165,247,159,41,127,187,100,150,189,68,134,100,70,104,125,168,166,210,137,79,105,135,140,154,118,144,163,144,139,109,142,116,206,164,171,183,93,123,118,164,148,125,156,73,180,89,188,110,185,157,128,172,147,171,97,105,105,153,157,174,133,143,81,155,196,129,229,286,106,115,230,167,237,107,154,172,171,115,146,162,198,123,224,132,135,201,102,152,203,102,144,101,86,179,74,138,222,124,97,181,141,154,185,135,161,185,182,127,104,170,226,128,177,152,140,170,116,118,181,135,141,221,206,90,167,111,160,135,206,214,99,182,216,127,78,128,124,196,121,166,127,225,164,164,119,42,126,146,237,190,120,95,238,134,126,231,93,234,201,159,167,137,154,157,271,179,271,154,162,162,151,164,160,66,182,195,141,132,109,114,97,109,206,178,105,160,170,155,228,152,191,201,146,91,122,226,158,184,210,227,109,200,152,209,152,66,230,138,77,185,57,132,167,123,167,152,124,144,153,110,161,148,138,159,111,123,190,182,179,239,139,114,132,129,82,111,105,129,132,191,105,94,175,85,229,107,59,162,114,62,163,174,145,173,96,108,167,187,150,134,128,172,149,94,238,279,85,219,81,143,180,147,128,220,48,109,146,118,171,167,126,79,147,178,110,134,220,137,188,128,91,151,123,100,83,212,108,151,184,106,193,177,172,153,106,173,164,145,132,126,177,106,174,130,226,129,156,86,154,169,195,106,53,135,256,110,123,198,159,124,110,116,126,220,171,110,89,182,169,183,157,129,171,198,125,8,163,76,107,154,114,207,295,192,159,149,156,163,135,153,76,106,115,135,142,153,189,157,165,147,122,111,96,167,269,134,267,85,265,157,194,154,105,146,151,109,120,232,148,173,124,164,88,102,164,151,130,90,172,151,125,202,95,80,113,176,139,144,135,188,165,86,162,120,159,193,114,63,140,175,235,230,202,201,94,126,145,24,150,210,245,208,151,155,79,170,142,124,155,287,67,114,137,154,110,116,108,96,152,113,151,191,110,235,193,140,150,141,176,182,247,144,102,116,112,146,137,192,161,104,147,128,133,79,174,108,193,179,59,108,138,158,147,133,65,149,156,89,199,144,146,137,134,170,147,142,199,230,87,186,171,114,136,155,172,167,117,123,189,64,155,110,35,178,184,140,178,146,248,222,178,224,204,151,118,276,132,73,113,135,170,141,127,143,140,232,183,154,137,102,180,103,224,114,113,137,127,106,170,90,145,153,186,111,202,158,183,202,117,206,138,198,68,155,134,193,102,121,192,215,162,89,166,156,110,218,118,205,140,152,186,96,98,124,155,161,110,211,184,147,104,131,208,174,178,149,106,150,170,115,116,151,192,110,171,130,235,101,201,149,187,120,110,111,82,99,118,191,145,136,243,152,120,190,206,198,139,113,114,108,163,145,109,121,114,173,186,134,143,141,99,141,167,122,243,213,162,124,86,174,151,111,179,242,218,165,148,287,182,164,128,111,212,208,119,154,131,89,138,126,129,83,108,100,150,106,190,67,140,155,119,173,189,203,170,151,169,129,94,205,165,132,88,142,98,98,69,112,162,154,153,159,116,118,138,257,170,135,128,128,192,175,159,134,142,136,225,148,178,87,161,101,141,141,125,136,148,147,119,85,220,286,192,127,56,176,139,141,50,169,115,183,174,238,193,81,130,167,150,59,130,130,203,182,111,183,178,86,158,244,172,125,134,98,169,118,83,122,181,124,215,109,175,136,183,150,213,28,207,176,199,166,142,125,171,92,97,154,136,99,110,223,72,260,123,154,165,150,166,176,155,148,178,172],"mode":"markers","type":"scatter3d","marker":{"color":"blue","size":5,"symbol":104,"line":{"color":"rgba(31,119,180,1)"}},"name":"Observations","error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"Final","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":true,"z":[110.883620462442,174.210227972972,132.198604385871,147.296020259034,121.396235926407,178.051565182717,170.542221523676,125.202920609548,163.677688809768,115.492356065544,107.366163303924,148.941703397618,167.726229456888,172.70565654595,151.932093504083,190.231078633566,142.540992056498,170.822803182668,146.790954826973,188.635905193597,155.804229046644,120.098340252428,162.1476575195,165.985731779488,199.619096320761,137.905264905464,197.771439553371,182.527187245654,162.593609386008,113.522684308068,147.722175873198,153.86652633948,134.584850673952,152.05118749153,146.711043737164,193.362434452964,138.039370535423,189.860938466393,154.070356092648,216.283617577953,157.881870393591,143.264017856386,130.571787130256,195.295171006643,249.103815983409,193.086288815513,102.525890390572,193.83692542205,172.227636524409,120.438598425491,182.174466444136,118.997183035969,108.679925893694,120.221919790444,140.958085670659,131.459332712939,139.243874880936,98.919720861699,113.737511246482,165.274591301364,165.00157207799,164.227103388335,122.329028255062,179.519288412595,164.861049162845,119.079982686009,157.064322426314,170.521939819857,154.825523023553,150.348317860956,209.457904861883,135.907122546999,148.849508868725,142.668423387358,122.47864491635,134.625385786076,147.865867435276,100.972503052379,146.208025529395,121.866474537716,128.824935774034,139.336896387595,164.176129565088,113.636496370312,187.612875871902,143.226929357786,205.984325990732,143.173646681946,125.270861537713,135.225816841576,134.907831005596,174.899918196552,148.041542265843,159.233456323677,140.249096135938,140.853927948793,128.804950916037,144.406732362895,155.063496498226,167.19633311741,128.765702104417,130.304768148851,140.327885849771,211.57122585525,151.932093504083,169.307875436792,118.257840788,134.805609820242,129.755812846357,112.282196501642,91.2474852466573,123.424262633264,117.946854111619,134.537488295428,172.15186705793,155.162744610971,224.018863278526,108.22709473748,224.904738427393,137.224937071662,123.540289245275,116.377805082412,162.840516390016,106.61028036387,149.836024597562,242.923729326342,111.530973218796,107.767803735724,151.712559499544,128.824935774034,143.145340912155,200.881163902076,170.310354504359,141.122049473607,146.820714176077,129.753108324055,165.302934829618,110.338302499243,139.381737954248,129.079470846806,143.068951859466,176.719945231397,136.734369628453,161.708170066259,144.118541186869,115.7439040817,115.723780240407,201.937295467289,118.29882816086,145.024665176317,132.333091748196,88.5874918621574,146.883540504515,171.994907216476,143.45012447607,194.951251719836,102.249210053453,123.220969980711,124.33322075869,254.491491842721,116.129378538444,185.553838826287,174.484380529546,139.360391465482,236.980124899503,155.525777050876,157.307085359379,171.743083634676,139.514877623145,119.005852863555,171.062611922355,108.030704774924,216.693171434609,142.050295046277,119.084468283242,125.661177241984,150.689706919607,116.71012864377,113.066258092087,178.032414317822,106.208359517867,139.371426362525,103.354402445686,115.965202729552,113.61685793383,126.605223676468,163.515548361273,186.48935935837,180.309503539813,160.013470546953,118.376663730862,142.715785765883,142.7918426619,112.162358022025,154.374371717438,102.35001998568,129.893296634915,115.450991808878,183.512720535145,156.724525930271,114.756347249041,143.073769613372,128.541258444092,207.781802675239,103.521723454556,172.141618979243,169.706309509861,110.335946565233,155.57703133499,142.0863186666,144.115754225637,155.567803407446,95.6119669064128,125.630039737161,109.213709290203,168.60303698835,139.637083619831,150.285491532518,139.909346767976,146.208265830853,177.932619641514,128.663092218026,173.610084347009,156.930108509522,146.297414264489,171.034918676774,116.640866150915,103.978825801112,157.319898930408,151.82196138815,155.964578957259,154.8615512116,130.014901971286,118.472026953354,163.677688809768,204.992900699439,127.094066635774,232.881979075241,223.138543079564,114.444352285413,120.829211022248,172.536811429395,167.870069418545,220.350121660016,141.116499044762,159.835378623623,182.204067930714,132.023172257709,139.950981975897,156.151387508755,148.305345472234,205.425977882831,137.320217855223,213.116563232904,136.761348984996,109.816062851543,176.260564775373,148.8062127366,165.136860242677,176.125453352443,115.603037192653,143.170463723506,115.814337526223,110.820271017403,160.249008096298,141.271861428384,175.11087455622,214.238718069003,127.314886940817,136.059912469609,177.15338282031,168.60303698835,169.64183407445,181.065930876654,133.746670013617,136.356318530701,158.770582079383,161.349528967426,152.288030127279,156.383257022349,92.3410541329323,196.723562939305,90.3216215025651,142.844477502679,133.331909748684,108.354266934316,114.847526522016,124.985146399762,107.610346906399,165.985731779488,132.765459610273,177.786555713807,218.706049916607,218.634912045993,114.511320237181,169.706309509861,145.291267161171,122.806680855745,113.066258092087,240.173612084197,157.674016673544,125.909177326453,164.393025054638,202.45070073704,99.2641982951291,142.822473715904,117.17991739529,157.277749975497,193.209060254994,114.831523023876,120.226652704471,126.886316634837,181.559313576611,155.123052908233,161.597331123289,125.738376794155,101.719912020007,156.243947338395,177.601440949752,248.816442322202,168.809024700256,158.385972312203,109.769256452865,187.985784074412,135.998011989488,116.645813471833,241.604654395743,120.631765644362,185.759235294162,162.539650579593,182.345601534054,144.663929725111,173.677346696987,134.204742935624,170.420024943273,201.621969426477,171.466151178869,232.902621184582,163.764344773711,102.518525976891,147.448393185093,137.486415040505,134.570786159935,177.739193335283,106.486125966777,156.309452341232,182.945087908671,142.616306814628,141.57297940416,110.770538674196,135.02012352788,151.699458498976,108.416553714527,191.238050267039,181.767508869613,142.339569933145,159.835378623623,160.641724415046,176.248724180742,236.545297337641,160.08421971653,128.524673118203,186.831879255945,155.79097018355,136.78667231256,127.129856969975,159.767206856948,148.247763310536,196.891168976767,194.951251719836,186.561145064044,134.365825521178,164.295722894688,155.637643160325,215.250648933953,128.098991044954,111.839669968479,189.746118163566,160.551918683401,87.7994629425273,137.118805148152,108.324514600548,105.791709844503,160.208545960157,100.297162371405,160.190999521766,145.563631580813,120.856223522865,139.658032037942,121.107961937285,168.741503216253,180.579886742476,154.713037374557,157.089662138832,203.949099701392,115.794213684931,145.387141636375,146.472005430879,163.603382292221,172.549754567436,210.160048250972,148.173532216585,115.955204415271,162.372131876287,115.652418780791,108.711948994094,105.635362573923,175.397960787888,144.751447931006,134.231741171398,210.748407873331,125.014239034575,101.040071711127,138.567952325548,118.78564272844,232.062501540124,127.358027658027,110.149748090335,149.167579591248,159.787043268701,117.853333169477,128.832458050242,163.18126702989,186.398681921825,187.224371704855,116.367820705473,138.273685390628,120.865993212078,194.58215999932,135.02012352788,108.731740772043,129.351635171065,158.095707970653,119.262171505536,146.208025529395,215.083655467367,217.912479984414,110.906161396503,197.689137218702,86.3708493068033,130.262009117342,194.368638147311,126.917279798849,114.717710961631,250.020659238176,86.9549565713859,125.995682443205,184.88504984607,120.22546772213,144.421769899973,151.618602728285,146.280042073579,114.544804213065,164.598718368335,165.212620292872,153.044309017876,138.688389111198,211.563397316938,123.60011420559,168.210226366774,107.263963398748,118.672596251039,132.764833102056,141.877605105548,133.96857643045,110.007505493182,165.288566454444,130.595128960004,156.565575408888,150.592173921147,146.554529187774,145.857926811248,210.072409894348,157.846588911155,144.672519233878,146.024920277834,122.928064723225,197.403674246104,131.794382076776,145.121383060907,137.381098230866,182.524407346422,137.586977655938,147.865867435276,128.161492232055,167.726229456888,130.028104243351,129.294135448299,95.7173284431857,180.35582938224,138.033386678407,214.77628753188,128.610624656057,76.0719512691785,157.23171401022,218.202504024506,122.639383528001,126.677963525973,191.203305885218,157.85598411751,117.94553254693,124.505352225295,122.960528594631,124.132484182195,172.990537690799,149.280649515605,121.499309937429,130.959483152103,129.247208898892,127.446749725494,150.401155291395,120.292313122223,129.546432571013,167.865510798663,151.762442689941,142.153665903121,17.9806296986207,144.607336924765,80.8281808856741,100.668617041266,170.467971597158,117.612850746827,142.236319227029,240.472911226578,179.271493272069,137.040441847153,140.314904999933,144.689048015403,161.314784585606,127.004788588461,142.823093255451,126.575565645526,105.913447148833,98.2845901515122,155.426724512455,144.937851396512,131.615927300312,198.352069461303,163.18126702989,163.139815532398,168.807498191624,130.343310180099,137.070043333731,111.032482934447,199.458079695854,237.089267654085,120.486285992017,245.861706014255,114.231706706027,212.793797354816,161.260140232332,199.700486744456,175.242767756523,155.278446081646,136.058131394677,135.623756046886,107.936210856384,130.304768148851,254.42397732739,168.283364333599,107.45830853796,138.650087381408,139.704339001138,97.464087944193,116.03569978044,137.381098230866,175.913884468565,109.144338510515,97.3954331736543,117.92046138472,140.619438846106,148.4792510451,152.807586532856,145.130472005155,114.438976384884,145.128832314466,165.501431055108,117.318753491664,135.033139688568,130.571787130256,179.651777752155,137.757851164219,115.415387586053,125.198270134452,155.652605273809,114.376913193677,257.106874452414,185.430808252782,122.139571678963,122.375189219627,124.003245881836,211.808421927851,183.763217703696,182.015845631814,206.462633441813,117.612850746827,135.907122546999,158.781284866419,91.2426133493344,141.682899560815,157.193930548479,214.00838000593,153.771334963522,115.581629498469,104.731797061481,98.6784184766186,133.842424338093,125.210704420727,116.757952745979,163.343859739121,236.649436180276,144.360046114945,122.086562167819,123.141058890902,172.353548315308,117.912481952551,120.777245296708,121.296460129329,115.854585208808,118.596447453142,83.7811385196363,144.471356244548,165.88605970152,132.836413770458,210.313422448943,175.111494095767,183.689747626864,129.700770329098,161.632890572315,170.542221523676,174.347323107157,218.634912045993,146.74395285397,110.84325964447,167.66712775523,142.519089541221,153.369419012744,130.875253743871,186.149645254593,189.819135979662,125.210704420727,129.634070927637,82.4279624745536,138.65283903179,116.425803432102,156.80198221552,98.791705255478,188.34167107525,142.972212741362,92.7810291577607,153.503790745398,165.134904826934,121.876018190314,158.349496384478,175.733957233556,107.170913596574,136.595163663609,139.422042227861,116.045761701087,172.717534899044,135.638871455173,131.227576381403,153.75988551754,140.056883107562,148.203977112548,117.556865949633,116.558919654044,140.453288788905,189.079663837181,103.083072395364,190.875460671588,139.319811626224,126.74205967663,131.604237506207,182.272489415131,198.611688764842,139.243874880936,129.228456151317,126.784335750941,177.447678050745,86.599017828078,113.513199413276,125.179884854399,90.968069690775,139.947803959346,189.163031003462,159.820303374747,165.752180389013,140.333304263992,223.488158793848,228.639131000706,163.767555653501,195.620299834077,218.265431624441,132.59018779225,143.995456423283,232.082590117231,148.704900921279,146.711043737164,121.584121220075,156.489490217357,190.649264231846,147.598791909507,104.866034659399,139.142070692133,145.550123046643,171.62581549596,180.773025619168,137.005697465333,161.279104985851,113.01363751615,210.20941317332,105.786199528402,176.254830636098,96.5907081936893,125.909177326453,145.760120367256,112.507160830962,144.614649642641,141.637028426736,117.84985364605,159.441503170436,154.830981643849,184.138639749407,159.820656811598,196.148815616891,175.068162652792,188.040032757978,187.748618744938,104.430608557334,216.014393509729,115.509537484024,143.45012447607,107.274663738172,139.747328917822,116.67384829037,160.866644063898,113.406349033005,118.491764494557,126.942796253125,138.813650666089,112.825813381234,86.0224586876052,101.837432277411,157.578208252317,141.214218154598,189.972741016143,131.236884300267,188.978078622992,106.207614979032,134.730914601659,198.358077092774,88.2934792127024,158.642345097601,115.298581171021,194.728612628064,148.744394602081,117.973574380803,211.834911358524,150.267933183567,125.203811147014,108.784710170442,109.099030371619,219.691425343572,158.89211896083,173.819363304189,148.778333286314,123.313805375994,168.219741723857,176.208931206505,134.142849470839,134.09482277897,146.119145646066,194.429944935788,140.684887257914,200.069934349434,147.610632457473,234.569618042024,160.350821701384,183.568142290492,153.879636756331,157.319898930408,169.14971178043,127.619328931776,124.132484182195,93.4005897979774,120.793053220524,126.493124327561,162.47807768842,152.509427645682,135.002306044904,203.562828916678,112.384563685627,124.741287938624,193.773227388713,140.597915615582,184.462934583739,125.981111431706,135.551640678817,158.889435765372,137.896727420003,132.838145363027,127.541392043611,107.543788935846,115.673470637176,162.948167900151,163.093041950293,123.691606756005,124.609123693736,125.180553922974,112.896717965726,120.712513128221,144.565025586269,164.01314815482,102.154435720711,220.886727562779,204.985230023653,132.333091748196,121.504643558437,95.7550693445702,198.760921339481,163.603382292221,131.464703999079,192.095864796132,206.467156797509,209.028090628252,163.291545144454,165.768709170538,261.76821688161,176.894868507794,199.966285479334,122.182283582391,123.978080463463,178.650476698257,172.141618979243,94.2969417439296,148.673417088271,157.480675253857,120.046039969269,127.227917699432,128.986350516261,126.984311356982,95.3476195840711,131.063603162171,111.986221467773,177.447678050745,122.746431883544,244.325811017701,110.298064186276,125.455116460764,141.494253296692,116.958204151834,136.141444370876,156.911115507154,178.801014359301,147.303617958837,135.3605584427,160.934983109383,127.106204076227,153.771789671871,165.571817371551,178.611072425057,129.722382967225,124.48736294245,126.391155260896,115.241630085265,88.0627995766739,115.935080573978,164.211012930204,134.094570613616,151.576585787998,115.799358981121,145.992668412372,138.899687043821,143.916129608835,137.346855685475,249.034421522595,170.838999854185,149.335736713332,134.636828263387,129.793233408299,142.493405808135,187.03523791581,170.632997784107,121.877657881003,137.123123466576,142.483899914,154.384313487356,129.076017217946,175.345196380097,113.855069262349,184.151453320436,118.254457734176,143.513553912435,121.153816967245,136.294382739729,185.56791035564,134.636828263387,166.018365377315,107.743682102955,102.824624090159,186.263940320701,298.926414531484,182.675029847345,132.446734130683,96.4652982392193,189.829296724194,135.239174528556,131.584511828899,91.3472069003175,126.675230708157,139.427406545331,188.412997551517,173.375576365091,176.948375053472,245.075733781865,125.792743180838,146.264759386483,165.806732887072,166.073334871926,114.993759895309,129.184973861151,117.060210883633,161.632890572315,218.634912045993,130.446820020239,175.823588717721,126.293167554087,88.8760505057053,157.880739554667,231.299906682504,184.387655089795,152.726080432826,160.443397962643,130.394046196165,169.126984734998,123.310125429684,103.430695028772,120.959193814777,144.751447931006,120.822803106681,206.013538776274,126.545174846315,165.258668122044,167.611564568976,168.434811411341,111.453472206414,166.4911506484,97.9308422301809,164.133028913959,165.298618958807,180.972131967921,163.418258111882,116.675497397343,181.466661891757,155.162744610971,105.777094152534,108.892396944763,169.203866161169,120.936883765897,157.827845579863,127.295590030626,187.849617236154,119.210917221422,217.579725208328,125.662272770057,175.969760885596,150.401155291395,107.867706933036,169.403231643947,172.486183653499,140.054798124807,164.897414356124,152.996890048323,193.963440367542],"x":[52,126,148,115,75,119,125,78,119,67,61,150,110,142,134,116,107,143,106,135,111,72,129,148,112,152,166,136,160,63,109,98,111,96,101,141,124,154,138,165,120,93,91,136,176,152,52,144,124,111,140,146,54,80,94,92,91,43,106,161,107,157,72,116,116,72,106,134,135,105,165,97,131,111,82,104,98,49,107,95,87,101,101,63,147,120,157,111,79,102,91,132,107,118,107,108,98,113,128,125,78,99,91,171,134,129,76,98,81,60,44,101,69,103,118,112,135,57,184,102,78,67,133,50,94,174,63,61,115,87,88,136,113,103,102,85,144,61,121,132,99,130,84,124,86,67,67,155,90,110,89,32,103,133,90,146,47,87,85,151,96,140,106,87,156,103,108,121,91,95,116,54,137,87,80,75,98,68,64,120,56,88,60,90,55,100,147,133,146,137,91,119,118,64,99,54,114,66,127,97,61,97,95,143,57,138,133,63,115,93,95,105,41,102,61,112,111,104,105,93,139,79,121,122,117,115,68,55,111,103,100,168,93,70,119,154,84,153,164,65,71,120,119,185,90,125,145,135,129,151,94,139,86,163,126,54,140,88,105,113,67,160,67,61,109,147,123,170,76,94,132,112,132,137,98,84,139,127,97,97,27,156,37,117,84,57,54,81,58,148,142,144,152,164,65,133,107,74,64,165,140,80,111,140,56,102,77,144,149,63,73,89,135,118,95,85,55,131,128,169,151,99,52,158,87,68,175,72,147,152,135,91,151,87,114,143,111,161,106,49,117,90,85,136,56,111,121,120,95,57,93,119,60,145,114,99,125,138,138,158,119,81,118,124,93,103,109,95,136,146,131,91,129,101,142,98,59,127,133,28,87,60,56,140,52,106,114,84,92,174,117,125,116,119,156,67,95,103,133,125,157,105,67,111,56,127,55,137,95,109,138,87,52,109,77,183,100,62,109,116,69,81,125,140,169,118,100,71,143,93,61,85,116,86,107,146,144,55,175,47,82,129,101,63,143,45,75,146,75,102,102,120,65,120,102,106,98,166,81,107,58,70,84,106,85,57,117,75,113,101,100,112,160,100,100,108,85,144,79,97,85,106,112,98,108,110,102,81,43,138,108,141,80,27,117,141,77,81,151,110,70,126,77,85,150,123,76,96,86,82,92,72,104,125,111,90,8,139,41,52,117,69,89,141,147,107,88,98,133,96,123,79,47,48,108,94,78,183,125,109,116,83,112,64,133,140,72,165,66,152,117,183,173,98,82,143,53,99,154,103,56,100,104,43,67,85,100,49,45,70,95,110,98,96,44,105,112,70,84,91,142,131,65,80,94,64,196,139,79,74,90,152,158,174,168,69,97,112,40,91,127,165,146,66,55,51,93,86,68,118,164,104,72,82,130,111,72,74,67,105,43,96,132,122,149,144,134,88,137,125,115,164,115,58,113,105,96,83,157,130,86,101,33,103,104,101,44,180,84,41,109,132,87,110,146,59,103,99,67,104,100,102,122,91,109,74,67,98,141,52,166,97,85,106,156,146,91,102,82,130,27,66,74,44,84,137,138,110,130,177,162,167,144,155,92,117,157,119,101,74,125,160,90,39,93,94,130,135,113,98,56,145,51,119,43,80,158,61,108,92,71,115,106,120,118,142,113,131,138,55,167,95,90,59,98,68,104,65,74,85,84,62,40,51,101,105,137,135,177,60,106,123,45,96,66,144,152,69,137,127,84,60,55,152,159,123,145,79,110,131,79,91,96,132,97,138,130,138,116,155,103,111,128,79,85,48,74,77,108,123,112,173,63,75,155,90,138,98,89,115,94,79,103,54,67,108,161,84,78,112,63,77,116,102,52,164,140,89,72,53,139,133,124,139,136,154,166,108,176,144,151,89,76,119,138,37,89,104,73,79,81,78,29,84,63,130,77,152,108,88,88,69,110,114,141,105,103,120,88,139,105,118,125,80,89,152,41,67,106,124,109,66,113,88,107,91,164,113,151,91,120,121,119,159,78,95,89,106,113,136,62,123,71,104,82,89,165,91,112,55,51,139,198,133,79,37,171,130,83,39,96,94,120,128,131,147,79,163,122,119,66,79,93,137,164,97,114,99,37,128,130,151,137,104,87,107,110,56,100,95,99,156,119,110,102,111,62,137,45,123,108,115,108,68,151,112,44,61,125,74,125,105,135,74,119,79,159,92,47,109,166,116,121,120,125],"y":[50,104,36,75,77,122,100,92,98,127,134,53,123,84,65,150,76,81,87,111,97,92,83,72,180,41,90,102,62,73,84,125,55,127,97,109,52,92,65,107,87,110,71,118,124,97,95,106,104,27,97,22,68,45,99,72,103,80,13,64,125,65,138,131,105,71,110,89,68,97,101,76,63,71,55,62,108,90,84,36,74,78,142,71,96,63,105,72,86,66,87,97,88,92,71,71,54,72,74,95,122,57,107,98,65,93,37,71,105,66,110,37,122,63,114,94,156,85,99,71,78,39,80,67,128,121,108,129,83,74,132,125,121,79,95,85,74,97,56,36,92,102,126,88,149,102,104,103,26,77,83,86,93,92,123,105,78,49,58,161,18,101,149,121,136,114,106,108,104,27,116,72,143,133,37,128,116,101,97,120,94,116,163,17,39,46,70,111,89,73,26,63,64,117,123,107,44,71,116,137,42,97,57,123,122,87,89,129,90,106,106,109,86,42,111,120,65,99,73,120,93,112,111,83,71,118,59,103,100,105,124,52,65,95,98,107,75,136,114,83,135,111,105,95,114,84,92,39,52,60,123,126,117,106,49,61,90,156,131,132,116,44,95,91,110,46,110,101,132,83,100,120,90,99,68,124,70,84,124,136,70,97,92,65,108,84,30,74,97,72,38,88,123,110,81,89,82,106,97,128,68,85,114,121,144,85,25,65,100,50,80,61,102,85,161,65,118,73,106,131,73,134,56,87,106,62,119,103,94,66,103,123,78,98,119,116,128,126,125,83,73,100,109,96,92,98,126,62,98,65,82,78,108,102,140,90,84,73,92,133,92,97,139,79,88,45,109,119,120,105,114,85,86,120,133,52,65,125,77,81,111,109,97,71,107,117,73,42,101,23,110,115,87,87,104,97,110,92,81,103,109,92,81,110,26,8,92,92,108,56,134,57,103,65,40,105,48,113,87,96,115,99,89,102,79,19,77,136,108,82,117,83,93,32,84,127,133,57,84,143,102,128,46,52,171,130,132,94,63,89,107,68,80,98,141,101,82,102,65,132,100,103,105,76,106,71,104,187,95,107,99,76,106,129,94,82,51,110,136,103,123,167,60,108,44,123,53,102,91,97,65,134,104,104,90,138,76,85,96,103,74,31,79,57,78,70,71,63,79,85,139,96,50,96,89,118,160,53,133,105,113,97,123,164,87,64,120,99,78,51,60,96,63,98,102,112,146,80,89,116,112,97,59,142,128,162,100,133,127,117,97,81,67,129,137,41,69,57,156,145,85,78,74,85,73,123,177,53,91,73,95,84,122,106,27,85,114,49,117,71,92,48,54,80,147,68,113,102,62,100,48,116,83,72,96,97,76,101,97,112,78,105,60,65,98,110,78,60,130,99,126,85,133,59,96,22,106,85,91,24,132,104,85,45,118,85,106,75,75,100,124,110,74,69,116,79,131,100,86,120,60,53,103,73,19,123,83,74,159,95,96,84,43,104,84,110,68,82,72,152,70,56,78,106,85,33,21,87,104,92,84,86,70,53,83,109,103,51,81,103,82,160,139,111,143,109,72,119,52,103,121,60,108,119,76,67,130,73,97,89,79,88,139,54,96,114,95,101,58,146,45,123,74,119,88,85,47,70,79,108,54,97,105,130,93,111,130,116,106,100,105,16,123,109,85,79,124,114,46,71,137,74,113,94,125,76,110,38,76,132,60,146,125,147,78,107,52,124,137,68,65,104,69,124,59,108,55,71,124,100,154,84,109,123,90,121,62,163,97,85,110,100,94,104,57,119,78,112,117,75,55,90,93,117,95,112,102,46,96,96,89,144,46,75,109,118,62,57,87,36,84,58,69,138,97,112,124,83,121,139,118,81,42,110,132,111,60,124,134,87,105,42,97,123,87,81,149,116,77,101,100,107,66,96,100,103,77,149,6,57,125,48,59,94,92,90,65,92,64,64,132,125,39,75,59,18,110,83,126,46,92,55,75,114,79,96,137,122,53,86,41,61,137,70,64,84,124,104,43,93,59,124,65,83,47,99,80,86,115,78,89,103,139,106,141,75,80,45,104,94,50,94,137,100,101,158,90,46,97,102,92,116,21,75,110,60,129,46,96,78,177,89,64,123,81,134,33,114,31,108,31,106,36,118,147,122,94,81,89,93,123,136,119,80,86,94,58,115,98,80,81,44,110,56,188,89,75,139,55,129,68,61,97,79,135],"type":"mesh3d","name":"Fitted values","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 4.14: 3D visualization of the fitted <code>interaction_model</code> against the <code>ugtests</code> data
</p>
</div>
<p>By examining the shape of this curved plane, we can observe that the model considers <em>trajectories</em> in the Year 2 and Year 3 examination scores. Those individuals who have improved from one year to the next will perform better in this model than those who declined. To demonstrate, let’s look at the predicted scores from our <code>interaction_model</code> for someone who declined and for someone who improved from Year 2 to Year 3.</p>
<div class="sourceCode" id="cb288"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb288-1"><a href="linear-reg-ols.html#cb288-1" aria-hidden="true" tabindex="-1"></a><span class="co"># data frame with a declining and an improving observation</span></span>
<span id="cb288-2"><a href="linear-reg-ols.html#cb288-2" aria-hidden="true" tabindex="-1"></a>obs <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb288-3"><a href="linear-reg-ols.html#cb288-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">Yr2 =</span> <span class="fu">c</span>(<span class="dv">150</span>, <span class="dv">75</span>),</span>
<span id="cb288-4"><a href="linear-reg-ols.html#cb288-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">Yr3 =</span> <span class="fu">c</span>(<span class="dv">75</span>, <span class="dv">150</span>)</span>
<span id="cb288-5"><a href="linear-reg-ols.html#cb288-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb288-6"><a href="linear-reg-ols.html#cb288-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb288-7"><a href="linear-reg-ols.html#cb288-7" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(interaction_model, obs)</span></code></pre></div>
<pre><code>##        1        2 
## 127.5010 170.1047</code></pre>
<p>Through including the interaction effect, the model interprets declining examination scores more negatively than improving examination scores. These kinds of additional inferential insights may be of great interest. However, consider the impact on interpretability of modeling too many combinations of interactions. As always, there is a trade-off between intepretability and accuracy<a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a>.</p>
<p>When running models with interaction terms, you can expect to see a hierarchy in the coefficients according to the level of the interaction. For example, single terms will usually generate higher coefficients than interactions of two terms, which will generate higher coefficients than interactions of three terms, and so on. Given this, whenever an interaction of terms is considered significant in a model, then the single terms contained in that interaction should automatically be regarded as significant.</p>
</div>
<div id="quadratic-and-higher-order-polynomial-terms" class="section level3" number="4.6.2">
<h3><span class="header-section-number">4.6.2</span> Quadratic and higher-order polynomial terms</h3>
<p>In many situations the real underlying relationship between the outcome and the inputs may be non-linear. For example, if the underlying relationship was thought to be quadratic on a given input variable <span class="math inline">\(x\)</span>, then the formula would take the form <span class="math inline">\(y = \beta_0 + \beta_1x + \beta_2x^2\)</span>. We can easily trial polynomial terms using our linear model technology.</p>
<p>For example, recall that we removed <code>Yr1</code> data from our model because it was not significant when modeled linearly. We could test if a quadratic model on <code>Yr1</code> helps improve our fit<a href="#fn22" class="footnote-ref" id="fnref22"><sup>22</sup></a>:</p>
<div class="sourceCode" id="cb290"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb290-1"><a href="linear-reg-ols.html#cb290-1" aria-hidden="true" tabindex="-1"></a><span class="co"># add a quadratic term in Yr1</span></span>
<span id="cb290-2"><a href="linear-reg-ols.html#cb290-2" aria-hidden="true" tabindex="-1"></a>quadratic_yr1_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">data =</span> ugtests, </span>
<span id="cb290-3"><a href="linear-reg-ols.html#cb290-3" aria-hidden="true" tabindex="-1"></a>                          <span class="at">formula =</span> Final <span class="sc">~</span> Yr3 <span class="sc">+</span> Yr2 <span class="sc">+</span> Yr1 <span class="sc">+</span> <span class="fu">I</span>(Yr1<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb290-4"><a href="linear-reg-ols.html#cb290-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb290-5"><a href="linear-reg-ols.html#cb290-5" aria-hidden="true" tabindex="-1"></a><span class="co"># test R-squared</span></span>
<span id="cb290-6"><a href="linear-reg-ols.html#cb290-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(quadratic_yr1_model)<span class="sc">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 0.5304198</code></pre>
<p>In this case we find that modeling <code>Yr1</code> as a quadratic makes no difference to the fit of the model.</p>
</div>
</div>
<div id="learning-exercises-2" class="section level2" number="4.7">
<h2><span class="header-section-number">4.7</span> Learning exercises</h2>
<div id="discussion-questions-2" class="section level3" number="4.7.1">
<h3><span class="header-section-number">4.7.1</span> Discussion questions</h3>
<ol style="list-style-type: decimal">
<li><p>What is the approximate meaning of the term ‘regression’‍? Why is the term particularly suited to the methodology described in this chapter?</p></li>
<li><p>What basic condition must the outcome variable satisfy for linear regression to be a potential modeling approach? Describe some ideas for problems that might be modeled using linear regression.</p></li>
<li><p>What is the difference between simple linear regression and multiple linear regression?</p></li>
<li><p>What is a residual, and how does it relate to the term ‘Ordinary Least Squares’‍?</p></li>
<li><p>How are the coefficients of a linear regression model interpreted? Explain why higher coefficients do not necessarily imply greater importance.</p></li>
<li><p>How is the <span class="math inline">\(R^2\)</span> of a linear regression model interpreted? What are the minimum and maximum possible values for <span class="math inline">\(R^2\)</span>, and what does each mean?</p></li>
<li><p>What are the key considerations when preparing input data for a linear regression model?</p></li>
<li><p>Describe your understanding of the term ‘dummy variable’‍. Why are dummy variable coefficients often larger than other coefficients in linear regression models?</p></li>
<li><p>Describe the term ‘collinearity’ and why it is an important consideration in regression models.</p></li>
<li><p>Describe some ways that linear regression models can be extended into non-linear models.</p></li>
</ol>
</div>
<div id="data-exercises-2" class="section level3" number="4.7.2">
<h3><span class="header-section-number">4.7.2</span> Data exercises</h3>
<p>Load the <code>sociological_data</code> data set via the <code>peopleanalyticsdata</code> package or download it from the internet<a href="#fn23" class="footnote-ref" id="fnref23"><sup>23</sup></a>. This data represents a sample of information obtained from individuals who participated in a global research study and contains the following fields:</p>
<ul>
<li><code>annual_income_ppp</code>: The annual income of the individual in PPP adjusted US dollars</li>
<li><code>average_wk_hrs</code>: The average number of hours per week worked by the individual</li>
<li><code>education_months</code>: The total number of months spent by the individual in formal primary, secondary and tertiary education</li>
<li><code>region</code>: The region of the world where the individual lives</li>
<li><code>job_type</code>: Whether the individual works in a skilled or unskilled profession</li>
<li><code>gender</code>: The gender of the individual</li>
<li><code>family_size</code>: The size of the individual’s family of dependents</li>
<li><code>work_distance</code>: The distance between the individual’s residence and workplace in kilometers</li>
<li><code>languages</code>: The number of languages spoken fluently by the individual</li>
</ul>
<p>Conduct some exploratory data analysis on this data set. Including:</p>
<ol style="list-style-type: decimal">
<li>Identify the extent to which missing data is an issue.</li>
<li>Determine if the data types are appropriate for analysis.</li>
<li>Using a correlation matrix, pairplot or alternative method, identify whether collinearity is present in the data.</li>
<li>Identify and discuss anything else interesting that you see in the data.</li>
</ol>
<p>Prepare to build a linear regression model to explain the variation in <code>annual_income_ppp</code> using the other data in the data set.</p>
<ol start="5" style="list-style-type: decimal">
<li>Are there any fields which you believe should not be included in the model? If so, why?</li>
<li>Would you consider imputing missing data for some or all fields where it is an issue? If so, what might be some simple ways to impute the missing data?</li>
<li>Which variables are categorical? Convert these variables to dummy variables using a convenient function or using your own approach.</li>
</ol>
<p>Run and interpret the model. For convenience, and to avoid long formula strings, you can use the formula notation <code>annual_income_ppp ~ .</code> which means ‘regress <code>annual_income</code> against everything else’‍. You can also remove fields this way, for example <code>annual_income_ppp ~ . - family_size</code>.</p>
<ol start="8" style="list-style-type: decimal">
<li>Determine what variables are significant predictors of annual income and what is the effect of each on the outcome.</li>
<li>Determine the overall fit of the model.</li>
<li>Do some simple analysis on the residuals of the model to determine if the model is safe to interpret.</li>
<li>Experiment with improving the model fit through possible interaction terms or non-linear extensions.</li>
<li>Comment on your results. Did anything in the results surprise you? If so, what might be possible explanations for this.</li>
<li>Explain why you would or would not be comfortable using a model like this in a predictive setting—for example to help employers determine the right pay for employees.</li>
</ol>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-linear" class="csl-entry">
Montgomery, Douglas C., Elizabeth A. Peck, and G. Geoffrey Vining. 2012. <em>Introduction to Linear Regression Analysis</em>.
</div>
<div id="ref-rao" class="csl-entry">
Rao, C. Radhakrishna, Shalabh, Helge Toutenburg, and Christian Heumann. 2008. <em>The Multiple Linear Regression Model and Its Extensions</em>.
</div>
<div id="ref-senn" class="csl-entry">
Senn, Stephen. 2011. <span>“Francis Galton and Regression to the Mean.”</span> <em>Significance</em>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="17">
<li id="fn17"><p>This chart includes a random ‘jitter’ to better illustrate observations that are identical and was first used to illustrate Galton’s data in <span class="citation"><a href="#ref-senn" role="doc-biblioref">Senn</a> (<a href="#ref-senn" role="doc-biblioref">2011</a>)</span>.<a href="linear-reg-ols.html#fnref17" class="footnote-back">↩︎</a></p></li>
<li id="fn18"><p>As a side note, in a simple regression model like this, where there is only one input variable, we have the simple identity <span class="math inline">\(R^2 = r^2\)</span>, where <span class="math inline">\(r\)</span> is the correlation between the input and outcome (for our small set of 10 observations here, the correlation is 0.864).<a href="linear-reg-ols.html#fnref18" class="footnote-back">↩︎</a></p></li>
<li id="fn19"><p>For more on how to control which categorical value is used as a reference, see Section <a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#def-ref">6.3.1</a>.<a href="linear-reg-ols.html#fnref19" class="footnote-back">↩︎</a></p></li>
<li id="fn20"><p>Rescaling numerical input variables onto common scales can help with understanding the ranked importance of these variables. In some techniques, for example structural modeling which we will review in Section <a href="modeling-explicit-and-latent-hierarchy-in-data.html#struc-eq-model">8.2</a>, scaled regression coefficients help determine the ranked importance of constructs to the outcome.<a href="linear-reg-ols.html#fnref20" class="footnote-back">↩︎</a></p></li>
<li id="fn21"><p>In a predictive context, there is also the issue of ‘overfitting’ the model, where the model is too ‘tightly’ aligned to the past data that was used in fitting it that it may be very inaccurate for new data. For example, in our interaction model, someone who scores very low in both Year 2 and Year 3 will be awarded an unreasonably high score (see the intercept coefficient in the interactive model summary). This reinforces the need to test and validate model fits in a predictive context.<a href="linear-reg-ols.html#fnref21" class="footnote-back">↩︎</a></p></li>
<li id="fn22"><p>Note the use of <code>I()</code> in the formula notation here. This is because the symbol <code>^</code> has a different meaning inside a formula, and we use <code>I()</code> to <em>isolate</em> what is inside the parentheses to ensure that it is interpreted literally as ‘the square of <code>Yr1</code>’‍.<a href="linear-reg-ols.html#fnref22" class="footnote-back">↩︎</a></p></li>
<li id="fn23"><p><a href="http://peopleanalytics-regression-book.org/data/sociological_data.csv" class="uri">http://peopleanalytics-regression-book.org/data/sociological_data.csv</a><a href="linear-reg-ols.html#fnref23" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="found-stats.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bin-log-reg.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
